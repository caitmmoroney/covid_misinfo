{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ICA embeddings\n",
    "embedded_tweets = np.load('tweet_embed_250.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, probability=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate classification algorithm\n",
    "\n",
    "# round 1 winner\n",
    "svc = SVC(C = 1, kernel = 'rbf', probability = True)\n",
    "\n",
    "class1_train_indices = list(range(100))\n",
    "class0_train_indices = list(range(280,380))\n",
    "\n",
    "train_X = embedded_tweets[[class1_train_indices + class0_train_indices],:][0]\n",
    "\n",
    "hundred_ones = [1]*100\n",
    "hundred_zeros = [0]*100\n",
    "train_Y = hundred_ones + hundred_zeros\n",
    "\n",
    "# fit SVC model on training subset of tweet embeddings\n",
    "svc.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disaggregate correct/incorrect predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test set\n",
    "test_indices = list(range(100,280))\n",
    "test_X = embedded_tweets[[test_indices],:][0]\n",
    "\n",
    "# model predictions for test set\n",
    "y_hat = svc.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct or not\n",
    "y_bool = y_hat==1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load list of explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in explanations list\n",
    "with open('explanation_list', 'rb') as f:\n",
    "    explanations = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cases', -0.20405363785689484),\n",
       " ('new', -0.155064717978664),\n",
       " ('deaths', -0.10207417820647408),\n",
       " ('coronavirus', -0.06698967143404387),\n",
       " ('60', 0.05479825610293619),\n",
       " ('buy', 0.037730810152627504),\n",
       " ('stocks', 0.030866492206280807),\n",
       " ('back', 0.02862044153289094),\n",
       " ('portfolio', 0.019228230730050148),\n",
       " ('virus', -0.01764508279082087),\n",
       " ('Reasons', 0.01762324495375404),\n",
       " ('markets', 0.017075460328202865),\n",
       " ('1500', 0.015832371859270025),\n",
       " ('recovered', 0.015536797813199194),\n",
       " ('2700', 0.015119529843808394),\n",
       " ('to', -0.002003116629506429),\n",
       " ('today', 0.0003091368069083442)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True, False,  True,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False,  True, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False,  True, False, False, False,\n",
       "       False, False,  True, False, False,  True, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_boolF = y_bool==False\n",
    "y_boolF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate by correctness of prediction\n",
    "from itertools import compress\n",
    "correctPred = list(compress(explanations, y_bool))\n",
    "wrongPred = list(compress(explanations, y_boolF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cases', -0.20405363785689484),\n",
       " ('new', -0.155064717978664),\n",
       " ('deaths', -0.10207417820647408),\n",
       " ('coronavirus', -0.06698967143404387),\n",
       " ('60', 0.05479825610293619),\n",
       " ('buy', 0.037730810152627504),\n",
       " ('stocks', 0.030866492206280807),\n",
       " ('back', 0.02862044153289094),\n",
       " ('portfolio', 0.019228230730050148),\n",
       " ('virus', -0.01764508279082087),\n",
       " ('Reasons', 0.01762324495375404),\n",
       " ('markets', 0.017075460328202865),\n",
       " ('1500', 0.015832371859270025),\n",
       " ('recovered', 0.015536797813199194),\n",
       " ('2700', 0.015119529843808394),\n",
       " ('to', -0.002003116629506429),\n",
       " ('today', 0.0003091368069083442)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrongPred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1659"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unique words from all explanations\n",
    "vocab_list = []\n",
    "for subList in explanations:\n",
    "    for el in subList:\n",
    "        if el[0] not in vocab_list:\n",
    "            vocab_list.append(el[0])\n",
    "len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1615"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique words from explanations for correctly predicted unreliable tweets\n",
    "vocab_correct = []\n",
    "for subList in correctPred:\n",
    "    for el in subList:\n",
    "        if el[0] not in vocab_correct:\n",
    "            vocab_correct.append(el[0])\n",
    "len(vocab_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique words from explanations for incorrectly predicted unreliable tweets\n",
    "vocab_wrong = []\n",
    "for subList in wrongPred:\n",
    "    for el in subList:\n",
    "        if el[0] not in vocab_wrong:\n",
    "            vocab_wrong.append(el[0])\n",
    "len(vocab_wrong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Table 1 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionary\n",
    "t1 = [\n",
    "    'blame',\n",
    "    'accuse',\n",
    "    'refuse',\n",
    "    'catastrophe',\n",
    "    'chaos',\n",
    "    'evil',\n",
    "    'fight',\n",
    "    'danger',\n",
    "    'hysteria',\n",
    "    'panic',\n",
    "    'paranoia',\n",
    "    'laugh',\n",
    "    'stupidity',\n",
    "    'hear',\n",
    "    'see',\n",
    "    'feel',\n",
    "    'suppose',\n",
    "    'perceive',\n",
    "    'look',\n",
    "    'appear',\n",
    "    'suggest',\n",
    "    'believe',\n",
    "    'pretend',\n",
    "    'martial',\n",
    "    'kill',\n",
    "    'die',\n",
    "    'weapon',\n",
    "    'weaponizing',\n",
    "    'ussr',\n",
    "    'japan',\n",
    "    'fukushima',\n",
    "    'chernobyl',\n",
    "    'wuhan',\n",
    "    'china',\n",
    "    'foreigners',\n",
    "    'cats',\n",
    "    'dogs',\n",
    "    'i',\n",
    "    'me',\n",
    "    'mine',\n",
    "    'my',\n",
    "    'you',\n",
    "    'your',\n",
    "    'we',\n",
    "    'our',\n",
    "    'propaganda',\n",
    "    'fake',\n",
    "    'conspiracy',\n",
    "    'claim',\n",
    "    'misleading',\n",
    "    'hoax',\n",
    "    'cure',\n",
    "    'breakthrough',\n",
    "    'bitch',\n",
    "    'wtf',\n",
    "    'dogbreath',\n",
    "    'zombie',\n",
    "    'junkies',\n",
    "    'hell',\n",
    "    'screwed',\n",
    "    'secular',\n",
    "    'bible',\n",
    "    'maga',\n",
    "    'magat',\n",
    "    'genetic',\n",
    "    'hillary',\n",
    "    'chinese',\n",
    "    'fundamentalist',\n",
    "    'market',\n",
    "    'communist',\n",
    "    'nazi',\n",
    "    'stock',\n",
    "    'economy',\n",
    "    'money',\n",
    "    'cost',\n",
    "    'costs',\n",
    "    'election',\n",
    "    'campaign',\n",
    "    'presidential',\n",
    "    'impeachment',\n",
    "    'rallies',\n",
    "    'base',\n",
    "    'trump',\n",
    "    'war',\n",
    "    'iran'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disaggregated by correctness of prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correctly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered vocab based on analysis words\n",
    "filtered_vocab = [el for el in vocab_correct if el.lower() in t1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary from filtered vocab\n",
    "myDict = dict()\n",
    "\n",
    "for i in range(len(filtered_vocab)):\n",
    "    myDict[filtered_vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of class associations of filtered vocab\n",
    "my_matrix = np.zeros((len(correctPred), len(vocab_correct)))\n",
    "\n",
    "for i in range(len(correctPred)):\n",
    "    expl = correctPred[i]\n",
    "    for j in range(len(expl)):\n",
    "        word = expl[j][0]\n",
    "        val = expl[j][1]\n",
    "        \n",
    "        if word in filtered_vocab:\n",
    "            if val > 0:\n",
    "                my_matrix[i, myDict[word]] = 1\n",
    "\n",
    "            if val < 0:\n",
    "                my_matrix[i, myDict[word]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no penalty\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        if el > 0:\n",
    "            rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalNonZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5341130604288499"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty for wrong class association of word\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3313840155945419"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incorrectly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered vocab based on analysis words\n",
    "filtered_vocab = [el for el in vocab_wrong if el.lower() in t1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary from filtered vocab\n",
    "myDict = dict()\n",
    "\n",
    "for i in range(len(filtered_vocab)):\n",
    "    myDict[filtered_vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of class associations of filtered vocab\n",
    "my_matrix = np.zeros((len(wrongPred), len(vocab_wrong)))\n",
    "\n",
    "for i in range(len(wrongPred)):\n",
    "    expl = wrongPred[i]\n",
    "    for j in range(len(expl)):\n",
    "        word = expl[j][0]\n",
    "        val = expl[j][1]\n",
    "        \n",
    "        if word in filtered_vocab:\n",
    "            if val > 0:\n",
    "                my_matrix[i, myDict[word]] = 1\n",
    "\n",
    "            if val < 0:\n",
    "                my_matrix[i, myDict[word]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no penalty\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        if el > 0:\n",
    "            rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2777777777777778"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty for wrong class association of word\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2222222222222222"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered vocab based on analysis words\n",
    "filtered_vocab = [el for el in vocab_list if el.lower() in t1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary from filtered vocab\n",
    "myDict = dict()\n",
    "\n",
    "for i in range(len(filtered_vocab)):\n",
    "    myDict[filtered_vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of class associations of filtered vocab\n",
    "my_matrix = np.zeros((len(explanations), len(vocab_list)))\n",
    "\n",
    "for i in range(len(explanations)):\n",
    "    expl = explanations[i]\n",
    "    for j in range(len(expl)):\n",
    "        word = expl[j][0]\n",
    "        val = expl[j][1]\n",
    "        \n",
    "        if word in filtered_vocab:\n",
    "            if val > 0:\n",
    "                my_matrix[i, myDict[word]] = 1\n",
    "\n",
    "            if val < 0:\n",
    "                my_matrix[i, myDict[word]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no penalty\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        if el > 0:\n",
    "            rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalNonZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5212962962962963"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty for wrong class association of word\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3259259259259259"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2:  Table 1 + manual additions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionary\n",
    "t1PlusManual = [\n",
    "    'blame',\n",
    "    'accuse',\n",
    "    'refuse',\n",
    "    'catastrophe',\n",
    "    'emergency',\n",
    "    'chaos',\n",
    "    'crisis',\n",
    "    'evil',\n",
    "    'fight',\n",
    "    'danger',\n",
    "    'hysteria',\n",
    "    'panic',\n",
    "    'paranoia',\n",
    "    'fear',\n",
    "    'fears',\n",
    "    'laugh',\n",
    "    'stupidity',\n",
    "    'hear',\n",
    "    'see',\n",
    "    'feel',\n",
    "    'suppose',\n",
    "    'perceive',\n",
    "    'look',\n",
    "    'appear',\n",
    "    'suggest',\n",
    "    'believe',\n",
    "    'believed',\n",
    "    'pretend',\n",
    "    'martial',\n",
    "    'kill',\n",
    "    'killing',\n",
    "    'kills',\n",
    "    'killed',\n",
    "    'die',\n",
    "    'death',\n",
    "    'dies',\n",
    "    'dying',\n",
    "    'dead',\n",
    "    'died',\n",
    "    'threat',\n",
    "    'weapon',\n",
    "    'weaponize',\n",
    "    'weaponizing',\n",
    "    'knife',\n",
    "    'ussr',\n",
    "    'japan',\n",
    "    'chernobyl',\n",
    "    'wuhan',\n",
    "    'china',\n",
    "    'foreigners',\n",
    "    'cat',\n",
    "    'cats',\n",
    "    'dog',\n",
    "    'dogs',\n",
    "    'i',\n",
    "    'me',\n",
    "    'mine',\n",
    "    'my',\n",
    "    'you',\n",
    "    'yours',\n",
    "    'your',\n",
    "    'we',\n",
    "    'our',\n",
    "    'propaganda',\n",
    "    'fake',\n",
    "    'conspiracy',\n",
    "    'claim',\n",
    "    'claims',\n",
    "    'claiming',\n",
    "    'claimed',\n",
    "    'misleading',\n",
    "    'hoax',\n",
    "    'cure',\n",
    "    'breakthrough',\n",
    "    'bitch',\n",
    "    'wtf',\n",
    "    'dogbreath',\n",
    "    'zombie',\n",
    "    'junkies',\n",
    "    'hell',\n",
    "    'screwed',\n",
    "    'fuck',\n",
    "    'fucking',\n",
    "    'fucked',\n",
    "    'fuckin',\n",
    "    'wth',\n",
    "    'secular',\n",
    "    'bible',\n",
    "    'maga',\n",
    "    'magat',\n",
    "    'genetic',\n",
    "    'hillary',\n",
    "    'clinton',\n",
    "    'fundamentalist',\n",
    "    'market',\n",
    "    'communist',\n",
    "    'nazi',\n",
    "    'stock',\n",
    "    'bank',\n",
    "    'economy',\n",
    "    'economic',\n",
    "    'money',\n",
    "    'cost',\n",
    "    'costs',\n",
    "    'election',\n",
    "    'campaign',\n",
    "    'presidential',\n",
    "    'impeachment',\n",
    "    'rally',\n",
    "    'rallies',\n",
    "    'base',\n",
    "    'president',\n",
    "    'trump',\n",
    "    'war',\n",
    "    'wwiii',\n",
    "    'asteroid',\n",
    "    'banknotes',\n",
    "    'dangerous',\n",
    "    'invent',\n",
    "    'invented',\n",
    "    'iran',\n",
    "    'lie',\n",
    "    'lies',\n",
    "    'lying',\n",
    "    'lied',\n",
    "    'liar',\n",
    "    'liars',\n",
    "    'lmfao',\n",
    "    'lmfaoooooo',\n",
    "    'misinformation',\n",
    "    'news',\n",
    "    'media',\n",
    "    'financial',\n",
    "    'propagandawars',\n",
    "    'antidote'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disaggregated by correctness of prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correctly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered vocab based on analysis words\n",
    "filtered_vocab = [el for el in vocab_correct if el.lower() in t1PlusManual]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary from filtered vocab\n",
    "myDict = dict()\n",
    "\n",
    "for i in range(len(filtered_vocab)):\n",
    "    myDict[filtered_vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of class associations of filtered vocab\n",
    "my_matrix = np.zeros((len(correctPred), len(vocab_correct)))\n",
    "\n",
    "for i in range(len(correctPred)):\n",
    "    expl = correctPred[i]\n",
    "    for j in range(len(expl)):\n",
    "        word = expl[j][0]\n",
    "        val = expl[j][1]\n",
    "        \n",
    "        if word in filtered_vocab:\n",
    "            if val > 0:\n",
    "                my_matrix[i, myDict[word]] = 1\n",
    "\n",
    "            if val < 0:\n",
    "                my_matrix[i, myDict[word]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no penalty\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        if el > 0:\n",
    "            rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "331"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalNonZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5933723196881092"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty for wrong class association of word\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35633528265107217"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incorrectly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered vocab based on analysis words\n",
    "filtered_vocab = [el for el in vocab_wrong if el.lower() in t1PlusManual]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary from filtered vocab\n",
    "myDict = dict()\n",
    "\n",
    "for i in range(len(filtered_vocab)):\n",
    "    myDict[filtered_vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of class associations of filtered vocab\n",
    "my_matrix = np.zeros((len(wrongPred), len(vocab_wrong)))\n",
    "\n",
    "for i in range(len(wrongPred)):\n",
    "    expl = wrongPred[i]\n",
    "    for j in range(len(expl)):\n",
    "        word = expl[j][0]\n",
    "        val = expl[j][1]\n",
    "        \n",
    "        if word in filtered_vocab:\n",
    "            if val > 0:\n",
    "                my_matrix[i, myDict[word]] = 1\n",
    "\n",
    "            if val < 0:\n",
    "                my_matrix[i, myDict[word]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no penalty\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        if el > 0:\n",
    "            rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31481481481481477"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty for wrong class association of word\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07407407407407408"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered vocab based on analysis words\n",
    "filtered_vocab = [el for el in vocab_list if el.lower() in t1PlusManual]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary from filtered vocab\n",
    "myDict = dict()\n",
    "\n",
    "for i in range(len(filtered_vocab)):\n",
    "    myDict[filtered_vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of class associations of filtered vocab\n",
    "my_matrix = np.zeros((len(explanations), len(vocab_list)))\n",
    "\n",
    "for i in range(len(explanations)):\n",
    "    expl = explanations[i]\n",
    "    for j in range(len(expl)):\n",
    "        word = expl[j][0]\n",
    "        val = expl[j][1]\n",
    "        \n",
    "        if word in filtered_vocab:\n",
    "            if val > 0:\n",
    "                my_matrix[i, myDict[word]] = 1\n",
    "\n",
    "            if val < 0:\n",
    "                my_matrix[i, myDict[word]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no penalty\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        if el > 0:\n",
    "            rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "341"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalNonZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5794444444444445"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty for wrong class association of word\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3422222222222222"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Table 1 words + stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionary\n",
    "stems = [stemmer.stem(word) for word in t1]\n",
    "stems = list(set(stems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fake',\n",
       " 'feel',\n",
       " 'base',\n",
       " 'elect',\n",
       " 'fundamentalist',\n",
       " 'me',\n",
       " 'look',\n",
       " 'we',\n",
       " 'economi',\n",
       " 'conspiraci',\n",
       " 'impeach',\n",
       " 'blame',\n",
       " 'hear',\n",
       " 'zombi',\n",
       " 'ralli',\n",
       " 'dogbreath',\n",
       " 'foreign',\n",
       " 'weapon',\n",
       " 'danger',\n",
       " 'hoax',\n",
       " 'hillari',\n",
       " 'my',\n",
       " 'genet',\n",
       " 'secular',\n",
       " 'wuhan',\n",
       " 'i',\n",
       " 'die',\n",
       " 'screw',\n",
       " 'appear',\n",
       " 'stupid',\n",
       " 'chernobyl',\n",
       " 'market',\n",
       " 'cat',\n",
       " 'wtf',\n",
       " 'presidenti',\n",
       " 'suppos',\n",
       " 'believ',\n",
       " 'nazi',\n",
       " 'iran',\n",
       " 'panic',\n",
       " 'trump',\n",
       " 'hysteria',\n",
       " 'maga',\n",
       " 'stock',\n",
       " 'our',\n",
       " 'bitch',\n",
       " 'japan',\n",
       " 'bibl',\n",
       " 'fukushima',\n",
       " 'mislead',\n",
       " 'cost',\n",
       " 'laugh',\n",
       " 'chao',\n",
       " 'breakthrough',\n",
       " 'accus',\n",
       " 'pretend',\n",
       " 'china',\n",
       " 'see',\n",
       " 'magat',\n",
       " 'catastroph',\n",
       " 'refus',\n",
       " 'perceiv',\n",
       " 'junki',\n",
       " 'kill',\n",
       " 'mine',\n",
       " 'war',\n",
       " 'communist',\n",
       " 'ussr',\n",
       " 'evil',\n",
       " 'propaganda',\n",
       " 'fight',\n",
       " 'claim',\n",
       " 'cure',\n",
       " 'suggest',\n",
       " 'dog',\n",
       " 'chines',\n",
       " 'paranoia',\n",
       " 'your',\n",
       " 'you',\n",
       " 'campaign',\n",
       " 'hell',\n",
       " 'martial',\n",
       " 'money']"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disaggregated by correctness of prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correctly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered vocab based on analysis words\n",
    "filtered_vocab = [el for el in vocab_correct if stemmer.stem(el).lower() in stems]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary from filtered vocab\n",
    "myDict = dict()\n",
    "\n",
    "for i in range(len(filtered_vocab)):\n",
    "    myDict[filtered_vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of class associations of filtered vocab\n",
    "my_matrix = np.zeros((len(correctPred), len(vocab_correct)))\n",
    "\n",
    "for i in range(len(correctPred)):\n",
    "    expl = correctPred[i]\n",
    "    for j in range(len(expl)):\n",
    "        word = expl[j][0]\n",
    "        val = expl[j][1]\n",
    "        \n",
    "        if word in filtered_vocab:\n",
    "            if val > 0:\n",
    "                my_matrix[i, myDict[word]] = 1\n",
    "\n",
    "            if val < 0:\n",
    "                my_matrix[i, myDict[word]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no penalty\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        if el > 0:\n",
    "            rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalNonZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5927875243664718"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty for wrong class association of word\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3961013645224172"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incorrectly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered vocab based on analysis words\n",
    "filtered_vocab = [el for el in vocab_wrong if stemmer.stem(el).lower() in stems]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary from filtered vocab\n",
    "myDict = dict()\n",
    "\n",
    "for i in range(len(filtered_vocab)):\n",
    "    myDict[filtered_vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of class associations of filtered vocab\n",
    "my_matrix = np.zeros((len(wrongPred), len(vocab_wrong)))\n",
    "\n",
    "for i in range(len(wrongPred)):\n",
    "    expl = wrongPred[i]\n",
    "    for j in range(len(expl)):\n",
    "        word = expl[j][0]\n",
    "        val = expl[j][1]\n",
    "        \n",
    "        if word in filtered_vocab:\n",
    "            if val > 0:\n",
    "                my_matrix[i, myDict[word]] = 1\n",
    "\n",
    "            if val < 0:\n",
    "                my_matrix[i, myDict[word]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no penalty\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        if el > 0:\n",
    "            rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty for wrong class association of word\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4444444444444444"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered vocab based on analysis words\n",
    "filtered_vocab = [el for el in vocab_list if stemmer.stem(el).lower() in stems]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary from filtered vocab\n",
    "myDict = dict()\n",
    "\n",
    "for i in range(len(filtered_vocab)):\n",
    "    myDict[filtered_vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of class associations of filtered vocab\n",
    "my_matrix = np.zeros((len(explanations), len(vocab_list)))\n",
    "\n",
    "for i in range(len(explanations)):\n",
    "    expl = explanations[i]\n",
    "    for j in range(len(expl)):\n",
    "        word = expl[j][0]\n",
    "        val = expl[j][1]\n",
    "        \n",
    "        if word in filtered_vocab:\n",
    "            if val > 0:\n",
    "                my_matrix[i, myDict[word]] = 1\n",
    "\n",
    "            if val < 0:\n",
    "                my_matrix[i, myDict[word]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no penalty\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        if el > 0:\n",
    "            rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalNonZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5881481481481482"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty for wrong class association of word\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39851851851851844"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Table 1 words + slang dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionary\n",
    "slang = [\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disaggregated by correctness of prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correctly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered vocab based on analysis words\n",
    "filtered_vocab = [el for el in vocab_correct if el.lower() in slang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary from filtered vocab\n",
    "myDict = dict()\n",
    "\n",
    "for i in range(len(filtered_vocab)):\n",
    "    myDict[filtered_vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of class associations of filtered vocab\n",
    "my_matrix = np.zeros((len(correctPred), len(vocab_correct)))\n",
    "\n",
    "for i in range(len(correctPred)):\n",
    "    expl = correctPred[i]\n",
    "    for j in range(len(expl)):\n",
    "        word = expl[j][0]\n",
    "        val = expl[j][1]\n",
    "        \n",
    "        if word in filtered_vocab:\n",
    "            if val > 0:\n",
    "                my_matrix[i, myDict[word]] = 1\n",
    "\n",
    "            if val < 0:\n",
    "                my_matrix[i, myDict[word]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no penalty\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        if el > 0:\n",
    "            rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalNonZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty for wrong class association of word\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incorrectly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered vocab based on analysis words\n",
    "filtered_vocab = [el for el in vocab_wrong if el.lower() in slang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary from filtered vocab\n",
    "myDict = dict()\n",
    "\n",
    "for i in range(len(filtered_vocab)):\n",
    "    myDict[filtered_vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of class associations of filtered vocab\n",
    "my_matrix = np.zeros((len(wrongPred), len(vocab_wrong)))\n",
    "\n",
    "for i in range(len(wrongPred)):\n",
    "    expl = wrongPred[i]\n",
    "    for j in range(len(expl)):\n",
    "        word = expl[j][0]\n",
    "        val = expl[j][1]\n",
    "        \n",
    "        if word in filtered_vocab:\n",
    "            if val > 0:\n",
    "                my_matrix[i, myDict[word]] = 1\n",
    "\n",
    "            if val < 0:\n",
    "                my_matrix[i, myDict[word]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no penalty\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        if el > 0:\n",
    "            rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty for wrong class association of word\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered vocab based on analysis words\n",
    "filtered_vocab = [el for el in vocab_list if el.lower() in slang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary from filtered vocab\n",
    "myDict = dict()\n",
    "\n",
    "for i in range(len(filtered_vocab)):\n",
    "    myDict[filtered_vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of class associations of filtered vocab\n",
    "my_matrix = np.zeros((len(explanations), len(vocab_list)))\n",
    "\n",
    "for i in range(len(explanations)):\n",
    "    expl = explanations[i]\n",
    "    for j in range(len(expl)):\n",
    "        word = expl[j][0]\n",
    "        val = expl[j][1]\n",
    "        \n",
    "        if word in filtered_vocab:\n",
    "            if val > 0:\n",
    "                my_matrix[i, myDict[word]] = 1\n",
    "\n",
    "            if val < 0:\n",
    "                my_matrix[i, myDict[word]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no penalty\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        if el > 0:\n",
    "            rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalNonZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty for wrong class association of word\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
