{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.decomposition import FastICA#, TruncatedSVD, PCA, NMF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.svm import SVC\n",
    "from lime import lime_text\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from lime import submodular_pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('COVID19_Dataset-text_labels_only.csv')\n",
    "\n",
    "# to convert contractions picked up by word_tokenize() into full words\n",
    "contractions = {\n",
    "    \"n't\": 'not',\n",
    "    \"'ve\": 'have',\n",
    "    \"'s\": 'is',  # note that this will include possessive nouns\n",
    "    'gonna': 'going to',\n",
    "    'gotta': 'got to',\n",
    "    \"'d\": 'would',\n",
    "    \"'ll\": 'will',\n",
    "    \"'re\": 'are',\n",
    "    \"'m\": 'am',\n",
    "    'wanna': 'want to'\n",
    "}\n",
    "\n",
    "\n",
    "# to convert nltk_pos tags to wordnet-compatible PoS tags\n",
    "def convert_pos_wordnet(tag):\n",
    "    tag_abbr = tag[0].upper()\n",
    "    tag_dict = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "\n",
    "    if tag_abbr in tag_dict:\n",
    "        return tag_dict[tag_abbr]\n",
    "\n",
    "\n",
    "class Text2Embed(TransformerMixin):\n",
    "    \"\"\" Description:\n",
    "        Transformer that takes in a list of strings, calculates the word-context matrix\n",
    "        (with any specified transformations), reduces the dimensionality of these word\n",
    "        embeddings, and then provides the text embeddings of a (new) list of texts\n",
    "        depending on which words in the \"vocab\" occur in the (new) strings.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize class & private variables\n",
    "    def __init__(self,\n",
    "                 window_size=4,\n",
    "                 remove_stopwords=True,\n",
    "                 add_start_end_tokens=True,\n",
    "                 lowercase=False,\n",
    "                 lemmatize=False,\n",
    "                 pmi=False,\n",
    "                 spmi_k=1,\n",
    "                 laplace_smoothing=0,\n",
    "                 pmi_positive=False,\n",
    "                 sppmi_k=1,\n",
    "                 n_components=250,\n",
    "                 random_state=4):\n",
    "\n",
    "        \"\"\" Params:\n",
    "                window_size: size of +/- context window (default = 4)\n",
    "                remove_stopwords: boolean, whether or not to remove NLTK English stopwords\n",
    "                add_start_end_tokens: boolean, whether or not to append <START> and <END> to the\n",
    "                beginning/end of each document in the corpus (default = True)\n",
    "                lowercase: boolean, whether or not to convert words to all lowercase\n",
    "                lemmatize: boolean, whether or not to lemmatize input text\n",
    "                pmi: boolean, whether or not to compute pointwise mutual information\n",
    "                spmi_k: numeric, shifted pmi value\n",
    "                pmi_positive: boolean, whether or not to compute positive PMI\n",
    "                sppmi_k: numeric, shifted ppmi value\n",
    "                n_components: number of components for PCA/FastICA\n",
    "                random_state: set seed for FastICA\n",
    "        \"\"\"\n",
    "        self.window_size = window_size\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.add_start_end_tokens = add_start_end_tokens\n",
    "        self.lowercase = lowercase\n",
    "        self.lemmatize = lemmatize\n",
    "        self.pmi = pmi\n",
    "        self.spmi_k = spmi_k\n",
    "        self.laplace_smoothing = laplace_smoothing\n",
    "        self.pmi_positive = pmi_positive\n",
    "        self.sppmi_k = sppmi_k\n",
    "        self.corpus = None\n",
    "        self.clean_corpus = None\n",
    "        self.vocabulary = None\n",
    "        self.X = None\n",
    "        self.doc_terms_lists = None\n",
    "        self.ica = FastICA(n_components=n_components, random_state=random_state)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.word_embeddings = None\n",
    "        self.text_embeddings = None\n",
    "\n",
    "    def fit(self, corpus, y=None):\n",
    "\n",
    "        \"\"\" Learn the dictionary of all unique tokens for given corpus. Compute the co-occurrence matrix\n",
    "            for input corpus and window_size, using term dictionary. Reduce dimensionality of embeddings.\n",
    "\n",
    "            Params:\n",
    "                corpus: list of strings\n",
    "\n",
    "            Returns: self\n",
    "        \"\"\"\n",
    "        self.corpus = corpus\n",
    "\n",
    "        term_dict = dict()\n",
    "        k = 0\n",
    "        corpus_words = []\n",
    "        clean_corpus = []\n",
    "        doc_terms_lists = []\n",
    "        detokenizer = TreebankWordDetokenizer()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        for text in corpus:\n",
    "            text = re.sub(r'[_~`@$%^&*[\\]+=\\|}{\\\"\\'<>/]+', '', text)\n",
    "\n",
    "            words = word_tokenize(text)\n",
    "\n",
    "            if self.remove_stopwords:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    if word.lower() not in set(stopwords.words('english')):\n",
    "                        clean_words.append(word)\n",
    "                words = clean_words\n",
    "\n",
    "            if self.lowercase:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    clean_words.append(word.lower())\n",
    "\n",
    "                words = clean_words\n",
    "\n",
    "            if self.lemmatize:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    PoS_tag = pos_tag([word])[0][1]\n",
    "\n",
    "                    # to change contractions to full word form\n",
    "                    if word in contractions:\n",
    "                        word = contractions[word]\n",
    "\n",
    "                    if PoS_tag[0].upper() in 'JNVR':\n",
    "                        word = lemmatizer.lemmatize(word, convert_pos_wordnet(PoS_tag))\n",
    "                    else:\n",
    "                        word = lemmatizer.lemmatize(word)\n",
    "\n",
    "                    clean_words.append(word)\n",
    "\n",
    "                words = clean_words\n",
    "\n",
    "            # detokenize trick taken from this StackOverflow post:\n",
    "            # https://stackoverflow.com/questions/21948019/python-untokenize-a-sentence\n",
    "            # and NLTK treebank documentation:\n",
    "            # https://www.nltk.org/_modules/nltk/tokenize/treebank.html\n",
    "            text = detokenizer.detokenize(words)\n",
    "            clean_corpus.append(text)\n",
    "\n",
    "            [corpus_words.append(word) for word in words]\n",
    "\n",
    "            if self.add_start_end_tokens:\n",
    "                words = ['<START>'] + words + ['<END>']\n",
    "\n",
    "            doc_terms_lists.append(words)\n",
    "\n",
    "        self.clean_corpus = clean_corpus\n",
    "\n",
    "        self.doc_terms_lists = doc_terms_lists\n",
    "\n",
    "        corpus_words = list(set(corpus_words))\n",
    "\n",
    "        if self.add_start_end_tokens:\n",
    "            corpus_words = ['<START>'] + corpus_words + ['<END>']\n",
    "\n",
    "        corpus_words = sorted(corpus_words)\n",
    "\n",
    "        for el in corpus_words:\n",
    "            term_dict[el] = k\n",
    "            k += 1\n",
    "\n",
    "        self.vocabulary = term_dict\n",
    "\n",
    "        num_terms = len(self.vocabulary)\n",
    "        window = self.window_size\n",
    "        X = np.full((num_terms, num_terms), self.laplace_smoothing)\n",
    "\n",
    "        for el in range(len(clean_corpus)):\n",
    "            words = doc_terms_lists[el]\n",
    "\n",
    "            # Construct word-context matrix\n",
    "            for i in range(len(words)):\n",
    "                target = words[i]\n",
    "\n",
    "                # grab index from dictionary\n",
    "                target_dict_index = self.vocabulary[target]\n",
    "\n",
    "                # find left-most and right-most window indices for each target word\n",
    "                left_end_index = max(i - window, 0)\n",
    "                right_end_index = min(i + window, len(words) - 1)\n",
    "\n",
    "                # loop over all words within window\n",
    "                # NOTE: this will include the target word; make sure to skip over it\n",
    "                for j in range(left_end_index, right_end_index + 1):\n",
    "\n",
    "                    # skip \"context word\" where the \"context word\" index is equal to the\n",
    "                    # target word index\n",
    "                    if j != i:\n",
    "                        context_word = words[j]\n",
    "\n",
    "                        # count co-occurrence of target and context words\n",
    "                        X[target_dict_index, self.vocabulary[context_word]] += 1\n",
    "\n",
    "        # if pmi = True, compute pmi matrix from word-context raw frequencies\n",
    "        # more concise code taken from this StackOverflow post:\n",
    "        # https://stackoverflow.com/questions/58701337/how-to-construct-ppmi-matrix-from-a-text-corpus\n",
    "        if self.pmi:\n",
    "            denom = X.sum()\n",
    "            col_sums = X.sum(axis=0)\n",
    "            row_sums = X.sum(axis=1)\n",
    "\n",
    "            expected = np.outer(row_sums, col_sums) / denom\n",
    "\n",
    "            X = X / expected\n",
    "\n",
    "            for i in range(X.shape[0]):\n",
    "                for j in range(X.shape[1]):\n",
    "\n",
    "                    if X[i, j] > 0:\n",
    "                        X[i, j] = np.log(X[i, j]) - np.log(self.spmi_k)\n",
    "\n",
    "                        if self.pmi_positive:\n",
    "                            X[i, j] = max(X[i, j] - np.log(self.sppmi_k), 0)\n",
    "\n",
    "        # note that X is a dense matrix\n",
    "        self.X = X\n",
    "\n",
    "        word_embeddings = self.scaler.fit_transform(self.X)\n",
    "        word_embeddings = self.ica.fit_transform(word_embeddings)\n",
    "\n",
    "        # transformed WordICA embeddings\n",
    "        self.word_embeddings = word_embeddings\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, new_corpus=None, y=None):\n",
    "\n",
    "        \"\"\" Get text embeddings for given corpus, using term dictionary and word embeddings obtained\n",
    "            with fit method.\n",
    "\n",
    "            Returns: text embeddings (shape: num texts by embedding dimensions)\n",
    "        \"\"\"\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        #if type(new_corpus) != list:\n",
    "            #exit()\n",
    "\n",
    "        for k in range(len(new_corpus)):\n",
    "            text = new_corpus[k]\n",
    "            text = re.sub(r'[_~`@$%^&*[\\]+=\\|}{\\\"\\'<>/]+', '', text)\n",
    "            text_vec = np.zeros(self.word_embeddings.shape[1])\n",
    "            words = word_tokenize(text)\n",
    "            tracker = 0  # to track whether we've encountered a word for which we have an embedding (in each tweet)\n",
    "\n",
    "            if self.remove_stopwords:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    if word.lower() not in set(stopwords.words('english')):\n",
    "                        clean_words.append(word)\n",
    "                words = clean_words\n",
    "\n",
    "            if self.lowercase:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    clean_words.append(word.lower())\n",
    "\n",
    "                words = clean_words\n",
    "\n",
    "            if self.lemmatize:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    PoS_tag = pos_tag([word])[0][1]\n",
    "\n",
    "                    # to change contractions to full word form\n",
    "                    if word in contractions:\n",
    "                        word = contractions[word]\n",
    "\n",
    "                    if PoS_tag[0].upper() in 'JNVR':\n",
    "                        word = lemmatizer.lemmatize(word, convert_pos_wordnet(PoS_tag))\n",
    "                    else:\n",
    "                        word = lemmatizer.lemmatize(word)\n",
    "\n",
    "                    clean_words.append(word)\n",
    "\n",
    "                words = clean_words\n",
    "\n",
    "            if self.add_start_end_tokens:\n",
    "                words = ['<START>'] + words + ['<END>']\n",
    "\n",
    "            for i in range(len(words)):\n",
    "                word = words[i]\n",
    "                if word in self.vocabulary:\n",
    "                    word_embed_vec = self.word_embeddings[self.vocabulary[word], :]\n",
    "                    if tracker == 0:\n",
    "                        text_matrix = word_embed_vec\n",
    "                    else:\n",
    "                        text_matrix = np.vstack((text_matrix, word_embed_vec))\n",
    "\n",
    "                    # only increment if we have come across a word in the embeddings dictionary\n",
    "                    tracker += 1\n",
    "\n",
    "            for j in range(len(text_vec)):\n",
    "                text_vec[j] = text_matrix[:, j].mean()\n",
    "\n",
    "            if k == 0:\n",
    "                full_matrix = text_vec\n",
    "            else:\n",
    "                full_matrix = np.vstack((full_matrix, text_vec))\n",
    "\n",
    "        self.text_embeddings = full_matrix\n",
    "\n",
    "        return self.text_embeddings.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Text2Embed at 0x7fa7d8b099d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate embedder\n",
    "embedder = Text2Embed(window_size = 15, lowercase = True, lemmatize = True, pmi = True)\n",
    "embedder.fit(tweets['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_tweets = embedder.transform(tweets['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(560, 250)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, probability=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate classification algorithm\n",
    "\n",
    "# round 1 winner\n",
    "svc = SVC(C = 1, kernel = 'rbf', probability = True)\n",
    "\n",
    "class1_train_indices = list(range(100))\n",
    "class0_train_indices = list(range(280,380))\n",
    "\n",
    "train_X = embedded_tweets[[class1_train_indices + class0_train_indices],:][0]\n",
    "\n",
    "hundred_ones = [1]*100\n",
    "hundred_zeros = [0]*100\n",
    "train_Y = hundred_ones + hundred_zeros\n",
    "\n",
    "# fit SVC model on training subset of tweet embeddings\n",
    "svc.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class1_test_indices = list(range(100,280))\n",
    "class0_test_indices = list(range(380,560))\n",
    "test_X = tweets['Tweet'][class1_test_indices + class0_test_indices]\n",
    "test_X = test_X.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = make_pipeline(embedder, svc)\n",
    "\n",
    "explainer = LimeTextExplainer(class_names = ['Reliable', 'Unreliable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "113\n",
      "136\n",
      "146\n",
      "193\n",
      "213\n",
      "219\n",
      "222\n",
      "238\n"
     ]
    }
   ],
   "source": [
    "# find unreliable tweets predicted as reliable\n",
    "for i in range(100,280):\n",
    "    if svc.predict(embedded_tweets[i].reshape(1,-1)) != tweets['Is_Unreliable'][i]:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.predict(embedded_tweets[100].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#coronavirus #virus Reasons to buy back stocks today 1500 new cases 60 deaths 2700 recovered #markets #portfolio'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['Tweet'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cases', -0.19393095841158178),\n",
       " ('new', -0.14484235628810083),\n",
       " ('deaths', -0.09198812664803652),\n",
       " ('60', 0.06941335119222543),\n",
       " ('coronavirus', -0.057026746565421935),\n",
       " ('buy', 0.04756727124584794)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrongPredict_exp = explainer.explain_instance(tweets['Tweet'][100], c.predict_proba, num_features = 6)\n",
    "wrongPredict_exp.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "fig = wrongPredict_exp.as_pyplot_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n",
      "150\n",
      "175\n",
      "200\n",
      "225\n",
      "250\n"
     ]
    }
   ],
   "source": [
    "# Establish list\n",
    "explanations = []\n",
    "\n",
    "# Loop through test set of unreliable tweets:\n",
    "for idx in range(100,280):\n",
    "    tweet = tweets['Tweet'][idx]\n",
    "    num_words = len(re.split(\"\\W+\", tweet))\n",
    "    exp = explainer.explain_instance(tweets['Tweet'][idx], c.predict_proba, num_features = num_words)\n",
    "    explanations.append(exp.as_list())\n",
    "    if idx in [125, 150, 175, 200, 225, 250]:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(explanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cases', -0.20405363785689484),\n",
       " ('new', -0.155064717978664),\n",
       " ('deaths', -0.10207417820647408),\n",
       " ('coronavirus', -0.06698967143404387),\n",
       " ('60', 0.05479825610293619),\n",
       " ('buy', 0.037730810152627504),\n",
       " ('stocks', 0.030866492206280807),\n",
       " ('back', 0.02862044153289094),\n",
       " ('portfolio', 0.019228230730050148),\n",
       " ('virus', -0.01764508279082087),\n",
       " ('Reasons', 0.01762324495375404),\n",
       " ('markets', 0.017075460328202865),\n",
       " ('1500', 0.015832371859270025),\n",
       " ('recovered', 0.015536797813199194),\n",
       " ('2700', 0.015119529843808394),\n",
       " ('to', -0.002003116629506429),\n",
       " ('today', 0.0003091368069083442)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1659"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = []\n",
    "for subList in explanations:\n",
    "    for el in subList:\n",
    "        if el[0] not in vocab_list:\n",
    "            vocab_list.append(el[0])\n",
    "len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cases',\n",
       " 'new',\n",
       " 'deaths',\n",
       " 'coronavirus',\n",
       " '60',\n",
       " 'buy',\n",
       " 'stocks',\n",
       " 'back',\n",
       " 'portfolio',\n",
       " 'virus',\n",
       " 'Reasons',\n",
       " 'markets',\n",
       " '1500',\n",
       " 'recovered',\n",
       " '2700',\n",
       " 'to',\n",
       " 'today',\n",
       " 'Coronavirus',\n",
       " 'inside',\n",
       " '1',\n",
       " '2',\n",
       " 'cotton',\n",
       " 'anus',\n",
       " 'solutions',\n",
       " 'water',\n",
       " 'joke',\n",
       " 'Dip',\n",
       " 'viruses',\n",
       " 'bottle',\n",
       " 'place',\n",
       " 'spit',\n",
       " 'cleric',\n",
       " 'Ayatollahs',\n",
       " 'sleep',\n",
       " 'blessings',\n",
       " 'drinking',\n",
       " 'have',\n",
       " 'a',\n",
       " 'him',\n",
       " 'all',\n",
       " 'and',\n",
       " 'other',\n",
       " 'of',\n",
       " 'in',\n",
       " 'before',\n",
       " 'it',\n",
       " 'for',\n",
       " 'Not',\n",
       " 'Take',\n",
       " 'the',\n",
       " 'oil',\n",
       " 'you',\n",
       " 'good',\n",
       " 'kitchen',\n",
       " 'cookies',\n",
       " 'longer',\n",
       " 'meeting',\n",
       " 'leftovers',\n",
       " 'Except',\n",
       " 'staff',\n",
       " 'set',\n",
       " 'we',\n",
       " 'out',\n",
       " 'Because',\n",
       " 'from',\n",
       " 'will',\n",
       " 'no',\n",
       " 'Trump',\n",
       " 'US',\n",
       " 'people',\n",
       " 'President',\n",
       " 'visit',\n",
       " 'amp',\n",
       " 'embrace',\n",
       " 'personal',\n",
       " 'responsibility',\n",
       " 'contract',\n",
       " 'has',\n",
       " 'who',\n",
       " 'vaccine',\n",
       " 'finding',\n",
       " 'Israelis',\n",
       " 'end',\n",
       " '100',\n",
       " 'unironic',\n",
       " 'roughly',\n",
       " 'NYT',\n",
       " 'write',\n",
       " 'likely',\n",
       " 'do',\n",
       " 'is',\n",
       " 'If',\n",
       " 'up',\n",
       " 'an',\n",
       " 'reason',\n",
       " 'reached',\n",
       " 'fallen',\n",
       " 'Sflecce',\n",
       " 'D19VP',\n",
       " 'behind',\n",
       " 'deal',\n",
       " 'be',\n",
       " 'Now',\n",
       " 've',\n",
       " 'about',\n",
       " 't',\n",
       " 'That',\n",
       " 'You',\n",
       " 'can',\n",
       " 's',\n",
       " 'test',\n",
       " 'Florida',\n",
       " 'police',\n",
       " 'meth',\n",
       " 'dept',\n",
       " 'contaminated',\n",
       " 'free',\n",
       " 'with',\n",
       " 'your',\n",
       " 'Is',\n",
       " 'This',\n",
       " 'TRUMP',\n",
       " 'Spread',\n",
       " 'trying',\n",
       " 'make',\n",
       " 'inevitable',\n",
       " 'control',\n",
       " 'worse',\n",
       " 'media',\n",
       " 'look',\n",
       " 'The',\n",
       " 'under',\n",
       " 'CDC',\n",
       " 'than',\n",
       " 'First',\n",
       " 'year',\n",
       " '2020',\n",
       " 'defs',\n",
       " 'asteroid',\n",
       " 'WWIII',\n",
       " 'dying',\n",
       " 'now',\n",
       " 'then',\n",
       " 're',\n",
       " 'What',\n",
       " 'by',\n",
       " 'We',\n",
       " 'CORONAVIRUS',\n",
       " 'Please',\n",
       " 'BANKNOTES',\n",
       " 'ALARMED',\n",
       " 'irreversibly',\n",
       " 'soon',\n",
       " 'spread',\n",
       " 'but',\n",
       " 'through',\n",
       " 'any',\n",
       " 'don',\n",
       " 'dangerous',\n",
       " 'go',\n",
       " 'work',\n",
       " 'infected',\n",
       " 'urged',\n",
       " 'guidance',\n",
       " 'contradicts',\n",
       " 'idiot',\n",
       " 'that',\n",
       " 'WATCH',\n",
       " 'everyone',\n",
       " 'suggests',\n",
       " 'Rick',\n",
       " 'Santelli',\n",
       " 'infecting',\n",
       " 'impact',\n",
       " 'economic',\n",
       " 'lessen',\n",
       " 'said',\n",
       " 'realDonaldTrump',\n",
       " 'hit',\n",
       " 'cancer',\n",
       " 'windmill',\n",
       " 'littledeekay',\n",
       " 'ears',\n",
       " 'My',\n",
       " 'are',\n",
       " 'not',\n",
       " 'yet',\n",
       " 'refused',\n",
       " 'testing',\n",
       " 'positive',\n",
       " 'zero',\n",
       " 'believed',\n",
       " 'Italy',\n",
       " 'patient',\n",
       " 'isolate',\n",
       " 'Pakistani',\n",
       " 'self',\n",
       " 'migrant',\n",
       " 'man',\n",
       " 'after',\n",
       " 'toilet',\n",
       " 'amid',\n",
       " 'knife',\n",
       " 'shopper',\n",
       " 'Australian',\n",
       " 'paper',\n",
       " 'draws',\n",
       " 'fight',\n",
       " 'stockpiling',\n",
       " 'over',\n",
       " 'LEVEL',\n",
       " 'EMERGENCY',\n",
       " 'HIGHEST',\n",
       " 'ENGLAND',\n",
       " 'NHS',\n",
       " 'INCIDENT',\n",
       " 'SKY',\n",
       " '4',\n",
       " 'DECLARED',\n",
       " 'NEWS',\n",
       " 'IS',\n",
       " 'OF',\n",
       " 'A',\n",
       " 'WHICH',\n",
       " 'HAS',\n",
       " 'THE',\n",
       " 'need',\n",
       " 'pathetic',\n",
       " 'States',\n",
       " 'VoteBlueNoMatterWho2020',\n",
       " 'evil',\n",
       " 'Perfect',\n",
       " 'VoteBlueToEndThisNightmare',\n",
       " 'funding',\n",
       " 'when',\n",
       " 'Just',\n",
       " 'more',\n",
       " 'invented',\n",
       " 'calories',\n",
       " 'standardized',\n",
       " 'Apparently',\n",
       " 'DM',\n",
       " 'Obama',\n",
       " 'also',\n",
       " 'cuts',\n",
       " 'only',\n",
       " 'he',\n",
       " 'Iran',\n",
       " '8',\n",
       " '000',\n",
       " 'parliament',\n",
       " '54',\n",
       " 'incapable',\n",
       " 'prisoners',\n",
       " 'chaos',\n",
       " 'gvmnt',\n",
       " 'released',\n",
       " 'descending',\n",
       " 'into',\n",
       " 'refuse',\n",
       " 'woman',\n",
       " 'tell',\n",
       " 'Doctors',\n",
       " 'her',\n",
       " 'she',\n",
       " 'Houses',\n",
       " 'Passes',\n",
       " 'Miracle',\n",
       " 'Sauce',\n",
       " 'Smeared',\n",
       " 'Fil',\n",
       " 'Door',\n",
       " 'Posts',\n",
       " 'Chick',\n",
       " 'On',\n",
       " 'Over',\n",
       " 'With',\n",
       " 'lying',\n",
       " 'infinitely',\n",
       " 'less',\n",
       " 'credible',\n",
       " 'crap',\n",
       " 'just',\n",
       " 'or',\n",
       " 'me',\n",
       " 'ply',\n",
       " 'roll',\n",
       " '24',\n",
       " 'Cashmere',\n",
       " 'WiseGuysLiquor',\n",
       " 'receive',\n",
       " 'Sale',\n",
       " 'Buy',\n",
       " 'Corona',\n",
       " 'at',\n",
       " 'on',\n",
       " 'Of',\n",
       " 'J',\n",
       " 'men',\n",
       " 'Hardick',\n",
       " 'virologist',\n",
       " 'Dr',\n",
       " 'explained',\n",
       " 'solely',\n",
       " 'Leading',\n",
       " 'B',\n",
       " 'confirms',\n",
       " 'BREAKING',\n",
       " 'Latest',\n",
       " 'Data',\n",
       " 'Numbers',\n",
       " 'Tell',\n",
       " 'Still',\n",
       " 'Misleading',\n",
       " 'Don',\n",
       " 'Here',\n",
       " 'going',\n",
       " 'Portland',\n",
       " 'SF',\n",
       " 'junkies',\n",
       " 'Zombie',\n",
       " 'happen',\n",
       " 'one',\n",
       " 'Seattle',\n",
       " 'LA',\n",
       " 'homeless',\n",
       " 'When',\n",
       " 'Health',\n",
       " 'limit',\n",
       " 'Could',\n",
       " 'Cash',\n",
       " 'advises',\n",
       " 'cease',\n",
       " 'interaction',\n",
       " 'World',\n",
       " 'Spreading',\n",
       " 'Organization',\n",
       " 'Be',\n",
       " 'market',\n",
       " 'Market',\n",
       " 'stock',\n",
       " 'rules',\n",
       " 'dictate',\n",
       " 'Fundamentalists',\n",
       " 'reality',\n",
       " 'convince',\n",
       " 'nature',\n",
       " 'Free',\n",
       " 'isn',\n",
       " 'doing',\n",
       " 'here',\n",
       " 'what',\n",
       " 'using',\n",
       " 'kits',\n",
       " 'previously',\n",
       " 'owned',\n",
       " 'company',\n",
       " 'been',\n",
       " 'there',\n",
       " 'call',\n",
       " 'racist',\n",
       " 'claims',\n",
       " 'Wuhan',\n",
       " 'RepGosar',\n",
       " 'integrity',\n",
       " 'lol',\n",
       " 'xenophobic',\n",
       " 'Virus',\n",
       " 'these',\n",
       " 'its',\n",
       " 'China',\n",
       " 'flu',\n",
       " 'governments',\n",
       " 'infection',\n",
       " 'says',\n",
       " 'compares',\n",
       " 'doctor',\n",
       " 'hysterical',\n",
       " 'Senior',\n",
       " 'German',\n",
       " 'So',\n",
       " 'run',\n",
       " 'get',\n",
       " 'job',\n",
       " 'theory',\n",
       " 'conspiracy',\n",
       " 'ought',\n",
       " 'Somebody',\n",
       " 'multinational',\n",
       " 'put',\n",
       " 'effect',\n",
       " 'killing',\n",
       " 'CoV',\n",
       " 'fat',\n",
       " 'surrounded',\n",
       " 'effective',\n",
       " 'dissolving',\n",
       " 'Soap',\n",
       " 'SARS',\n",
       " 'MORE',\n",
       " 'let',\n",
       " 'better',\n",
       " 'would',\n",
       " 'anchor',\n",
       " 'CNBC',\n",
       " 'so',\n",
       " 'Americans',\n",
       " 'Hold',\n",
       " 'Admits',\n",
       " 'Stranded',\n",
       " 'Flatly',\n",
       " 'Leave',\n",
       " 'Static',\n",
       " 'He',\n",
       " 'd',\n",
       " 'lied',\n",
       " 'symptoms',\n",
       " 'LMFAOOOOOO',\n",
       " 'Bro',\n",
       " 'street',\n",
       " 'wouldn',\n",
       " 'having',\n",
       " 'health',\n",
       " 'always',\n",
       " 'thing',\n",
       " 'threat',\n",
       " 'mass',\n",
       " 'It',\n",
       " 'because',\n",
       " 'was',\n",
       " 'never',\n",
       " 'crisis',\n",
       " 'probably',\n",
       " 'guide',\n",
       " 'phase',\n",
       " 'CarmichaelKevin',\n",
       " 'writes',\n",
       " 'prices',\n",
       " 'Bank',\n",
       " 'Oil',\n",
       " 'early',\n",
       " 'Canada',\n",
       " 'Let',\n",
       " 'Democrats',\n",
       " 'panic',\n",
       " 'mainstream',\n",
       " 'incite',\n",
       " 'straight',\n",
       " 'this',\n",
       " 'Supplies',\n",
       " 'Hell',\n",
       " 'Medical',\n",
       " 'Manufacturing',\n",
       " 'Stupidity',\n",
       " 'Pure',\n",
       " 'Medicine',\n",
       " 'Depending',\n",
       " 'OR',\n",
       " 'Death',\n",
       " 'People',\n",
       " 'like',\n",
       " 'Flu',\n",
       " 'Know',\n",
       " 'Apples',\n",
       " 'Ratio',\n",
       " 'Oranges',\n",
       " 'Intelligent',\n",
       " 'Between',\n",
       " 'collapsed',\n",
       " 'may',\n",
       " 'Recession',\n",
       " 'called',\n",
       " 'imminent',\n",
       " 'Thousands',\n",
       " 'And',\n",
       " 'died',\n",
       " 'tool',\n",
       " 'hyperbole',\n",
       " 'TrumpLies',\n",
       " 'MoronInChief',\n",
       " 'fix',\n",
       " 'toolbox',\n",
       " 'his',\n",
       " 'Want',\n",
       " 'Claims',\n",
       " 'Goes',\n",
       " 'Fox',\n",
       " 'Use',\n",
       " 'Host',\n",
       " 'BONKERS',\n",
       " 'Impeach',\n",
       " 'Dems',\n",
       " 'Fears',\n",
       " 'Ya',\n",
       " 'Destroy',\n",
       " 'Purpose',\n",
       " 'Paranoia',\n",
       " 'Melting',\n",
       " 'Someone',\n",
       " 'Give',\n",
       " 'Press',\n",
       " 'Down',\n",
       " 'Will',\n",
       " 'Him',\n",
       " 'hoax',\n",
       " 'claiming',\n",
       " 'weaponizing',\n",
       " 'Mulvaney',\n",
       " 'cuckoo',\n",
       " 'rattled',\n",
       " 'liberals',\n",
       " 'CPAC',\n",
       " 'Mick',\n",
       " 'At',\n",
       " 'off',\n",
       " 'Fear',\n",
       " 'via',\n",
       " 'epidemic',\n",
       " 'Hub',\n",
       " 'volatility',\n",
       " 'real',\n",
       " 'if',\n",
       " 'Donald',\n",
       " 'rather',\n",
       " 'something',\n",
       " 'treating',\n",
       " 'could',\n",
       " 'somethi',\n",
       " 'negatively',\n",
       " 'as',\n",
       " 'campaign',\n",
       " 'Kills',\n",
       " 'White',\n",
       " 'Melania',\n",
       " 'Pavillion',\n",
       " 'House',\n",
       " 'Tantrums',\n",
       " 'Tennis',\n",
       " 'As',\n",
       " 'goes',\n",
       " 'literally',\n",
       " 'dont',\n",
       " 'common',\n",
       " 'realize',\n",
       " 'berezin_goal',\n",
       " 'Newmarket',\n",
       " 'cold',\n",
       " 'smh',\n",
       " 'Same',\n",
       " 'takes',\n",
       " 'graphic',\n",
       " 'sanitiser',\n",
       " 'how',\n",
       " 'hand',\n",
       " 'death',\n",
       " 'confirmed',\n",
       " 'first',\n",
       " 'patients',\n",
       " 'none',\n",
       " 'talks',\n",
       " 'Netherlands',\n",
       " '56',\n",
       " 'business',\n",
       " 'discharge',\n",
       " 'But',\n",
       " 'my',\n",
       " 'RickSantelli',\n",
       " 'charge',\n",
       " 'exposed',\n",
       " 'blocking',\n",
       " 'MUST',\n",
       " 'much',\n",
       " 'GOP',\n",
       " 'drugmakers',\n",
       " 'limits',\n",
       " 'bill',\n",
       " 'NOW',\n",
       " 'stop',\n",
       " 'CoronaVirus',\n",
       " 'Wear',\n",
       " 'CoronaVirusChallenge',\n",
       " 'HIV',\n",
       " 'condoms',\n",
       " 'hands',\n",
       " 'fucked',\n",
       " 'ring',\n",
       " 'alarm',\n",
       " 'CoronaVirusSeattle',\n",
       " 'bells',\n",
       " 'might',\n",
       " 'm',\n",
       " 'I',\n",
       " 'countries',\n",
       " 'different',\n",
       " '10',\n",
       " 'WHO',\n",
       " 'produced',\n",
       " 'seems',\n",
       " 'say',\n",
       " 'refuses',\n",
       " 'tedlieu',\n",
       " 'Mike',\n",
       " 'Pompeo',\n",
       " 'Deny',\n",
       " 'even',\n",
       " 'POTUS',\n",
       " 'continue',\n",
       " 'manage',\n",
       " 'VP',\n",
       " 'professionals',\n",
       " 'nation',\n",
       " 'decisively',\n",
       " 'aside',\n",
       " 'our',\n",
       " 'Friday',\n",
       " 'Daily',\n",
       " 'published',\n",
       " 'treat',\n",
       " 'Politico',\n",
       " 'article',\n",
       " 'base',\n",
       " 'rallies',\n",
       " 'headlined',\n",
       " 'emergency',\n",
       " 'Pence',\n",
       " 'solution',\n",
       " 'team',\n",
       " 'praying',\n",
       " 'screwed',\n",
       " 'thousands',\n",
       " 'open',\n",
       " 'One',\n",
       " 'doors',\n",
       " 'Hamilton',\n",
       " 'Trudeau',\n",
       " 'welcome',\n",
       " 'throws',\n",
       " 'flying',\n",
       " 'really',\n",
       " 'defend',\n",
       " 'degree',\n",
       " 'brilliant',\n",
       " 'idea',\n",
       " 'dotard',\n",
       " 'against',\n",
       " 'Iranian',\n",
       " 'weapon',\n",
       " 'regime',\n",
       " 'attempts',\n",
       " 'care',\n",
       " 'taking',\n",
       " 'continues',\n",
       " 'nuclear',\n",
       " 'Instead',\n",
       " 'achieve',\n",
       " 'victims',\n",
       " 'Democrat',\n",
       " 'Tweets',\n",
       " 'Supporters',\n",
       " 'Support',\n",
       " 'Large',\n",
       " 'Infecting',\n",
       " 'Amounts',\n",
       " 'Rallies',\n",
       " 'Her',\n",
       " 'By',\n",
       " 'Gets',\n",
       " 'Attempts',\n",
       " 'Chelsea',\n",
       " 'BRUTAL',\n",
       " 'Clinton',\n",
       " 'Lecture',\n",
       " 'Destroyed',\n",
       " 'Facts',\n",
       " 'gonna',\n",
       " 'eat',\n",
       " 'sugar',\n",
       " 'brown',\n",
       " 'oatmeal',\n",
       " 'sprinkles',\n",
       " 'DS8',\n",
       " 'DD6',\n",
       " 'too',\n",
       " 'Patient',\n",
       " 'Zero',\n",
       " 'DEAD',\n",
       " '1st',\n",
       " 'NorthKorea',\n",
       " 'Problem',\n",
       " 'quote',\n",
       " 'Stalin',\n",
       " 'loosely',\n",
       " 'Shot',\n",
       " 'For',\n",
       " 'To',\n",
       " 'No',\n",
       " 'pictures',\n",
       " 'Soooo',\n",
       " 'Briefing',\n",
       " 'Boring',\n",
       " 'Instatute',\n",
       " 'National',\n",
       " 'show',\n",
       " 'they',\n",
       " 'had',\n",
       " 'did',\n",
       " 'Wash',\n",
       " 'laugh',\n",
       " 'ADVICE',\n",
       " 'Book',\n",
       " 'vacation',\n",
       " 'll',\n",
       " 'hold',\n",
       " 'brexit',\n",
       " 'beer',\n",
       " 'fuelling',\n",
       " 'sure',\n",
       " 'racism',\n",
       " 'great',\n",
       " 'xenophobia',\n",
       " 'cure',\n",
       " 'use',\n",
       " 'know',\n",
       " 'powers',\n",
       " 'keystroke',\n",
       " 'ability',\n",
       " 'ThePME',\n",
       " 'Help',\n",
       " 'Research',\n",
       " 'Services',\n",
       " 'Salary',\n",
       " 'Donates',\n",
       " 'Q4',\n",
       " 'Human',\n",
       " 'false',\n",
       " 'helping',\n",
       " 'pushing',\n",
       " 'spin',\n",
       " 'column',\n",
       " 'some',\n",
       " 'GOING',\n",
       " 'LIVE',\n",
       " 'DOESN',\n",
       " 'T',\n",
       " 'SPRAY',\n",
       " 'DIE',\n",
       " 'OUTSIDE',\n",
       " 'FOREVER',\n",
       " 'ANYWAY',\n",
       " 'Disinfection',\n",
       " 'HOST',\n",
       " 'OFF',\n",
       " 'WHY',\n",
       " 'THEN',\n",
       " 'IF',\n",
       " 'S',\n",
       " 'TO',\n",
       " 'ITS',\n",
       " 'IT',\n",
       " 'lie',\n",
       " 'administration',\n",
       " 'another',\n",
       " 'spins',\n",
       " 'DC',\n",
       " 'troubling',\n",
       " 'most',\n",
       " 'response',\n",
       " 'Hoax',\n",
       " 'U',\n",
       " 'mischaracterized',\n",
       " 'way',\n",
       " 'NOT',\n",
       " 'spreading',\n",
       " 'Trying',\n",
       " 'lies',\n",
       " 'points',\n",
       " 'political',\n",
       " 'hurt',\n",
       " 'score',\n",
       " 'cheap',\n",
       " 'Chinese',\n",
       " 'America',\n",
       " 'Thanks',\n",
       " 'chance',\n",
       " 'giving',\n",
       " 'reliance',\n",
       " 'supplies',\n",
       " 'HudsonInstitute',\n",
       " 'electronics',\n",
       " 'vital',\n",
       " 'medical',\n",
       " 'discuss',\n",
       " 'manufacturing',\n",
       " 'economy',\n",
       " 'dog',\n",
       " 'germs',\n",
       " 'fur',\n",
       " 'beards',\n",
       " 'hipsters',\n",
       " 'Swiss',\n",
       " 'Enjoy',\n",
       " 'Lumberjack',\n",
       " 'researchers',\n",
       " 'carry',\n",
       " 'according',\n",
       " 'entire',\n",
       " 'herd',\n",
       " 'thin',\n",
       " 'Maybe',\n",
       " 'mission',\n",
       " 'communications',\n",
       " 'information',\n",
       " 'interfere',\n",
       " 'attempting',\n",
       " 'interfering',\n",
       " 'CREW',\n",
       " 'requesting',\n",
       " 'indicate',\n",
       " 'release',\n",
       " 'began',\n",
       " 'see',\n",
       " 'since',\n",
       " 'diagnosed',\n",
       " 'news',\n",
       " '90',\n",
       " 'often',\n",
       " 'Idk',\n",
       " 'stat',\n",
       " 'why',\n",
       " 'wants',\n",
       " 'risk',\n",
       " 'putting',\n",
       " 'contained',\n",
       " 'vulnerable',\n",
       " 'pretend',\n",
       " 'millions',\n",
       " 'anyone',\n",
       " 'ChrisCuomo',\n",
       " 'goodness',\n",
       " 'cal',\n",
       " 'foreign',\n",
       " 'townhallcom',\n",
       " 'Oh',\n",
       " 'Can',\n",
       " 'Phase',\n",
       " 'Getting',\n",
       " 'Incredible',\n",
       " 'ever',\n",
       " 'occurred',\n",
       " 'fastest',\n",
       " 'Praises',\n",
       " 'Thing',\n",
       " 'Every',\n",
       " 'California',\n",
       " 'Single',\n",
       " 'Governor',\n",
       " 'Ship',\n",
       " 'Said',\n",
       " 'Response',\n",
       " 'Cruise',\n",
       " 'suggest',\n",
       " 'leaked',\n",
       " 'Norwegian',\n",
       " 'emails',\n",
       " 'wow',\n",
       " 'persuade',\n",
       " 'Lines',\n",
       " 'cancel',\n",
       " 'misinformation',\n",
       " 'customers',\n",
       " 'last',\n",
       " 'week',\n",
       " 'seriously',\n",
       " 'overreaction',\n",
       " 'blast',\n",
       " 'blasted',\n",
       " 'same',\n",
       " 'Like',\n",
       " 'Meet',\n",
       " 'Childish',\n",
       " 'Doesn',\n",
       " 'Pelosi',\n",
       " 'Won',\n",
       " 'shutting',\n",
       " 'Harvard',\n",
       " 'home',\n",
       " 'Corvid19',\n",
       " 'ruling',\n",
       " 'sit',\n",
       " 'University',\n",
       " 'class',\n",
       " 'down',\n",
       " 'British',\n",
       " 'pay',\n",
       " 'Any',\n",
       " 'offering',\n",
       " 'takers',\n",
       " '4600',\n",
       " 'means',\n",
       " 'getting',\n",
       " 'around',\n",
       " 'start',\n",
       " 'warmer',\n",
       " 'suck',\n",
       " 'mosquitos',\n",
       " 'blood',\n",
       " 'which',\n",
       " 'global',\n",
       " 'least',\n",
       " 'cost',\n",
       " 'development',\n",
       " 'world',\n",
       " 'creating',\n",
       " 'perform',\n",
       " 'ocean',\n",
       " 'frank',\n",
       " 'coachella',\n",
       " 'respiratory',\n",
       " 'MERS',\n",
       " 'viral',\n",
       " 'Middle',\n",
       " 'East',\n",
       " 'syndrome',\n",
       " 'camel',\n",
       " 'known',\n",
       " 'caused',\n",
       " 'Homeless',\n",
       " 'Dirty',\n",
       " 'Responible',\n",
       " 'District',\n",
       " 'Clean',\n",
       " 'She',\n",
       " 'holding',\n",
       " 'Government',\n",
       " 'accounts',\n",
       " 'looks',\n",
       " 'though',\n",
       " 'certain',\n",
       " 'worried',\n",
       " 'bad',\n",
       " 'noticed',\n",
       " 'realdonaldtrump',\n",
       " 'cool',\n",
       " 'Have',\n",
       " 'Owns',\n",
       " 'Institute',\n",
       " 'PIRBRIGHT',\n",
       " 'Bill',\n",
       " 'Gates',\n",
       " 'Funded',\n",
       " 'Patent',\n",
       " 'Which',\n",
       " 'beginning',\n",
       " 'Communist',\n",
       " 'help',\n",
       " 'journalists',\n",
       " 'arrested',\n",
       " 'penny',\n",
       " 'CoronavirusPandemic',\n",
       " 'CoronaVirusCanada',\n",
       " 'bring',\n",
       " 'time',\n",
       " 'Given',\n",
       " 'properties',\n",
       " 'copper',\n",
       " 'antiviral',\n",
       " 'CoronaVirusUpdate',\n",
       " 'covid19',\n",
       " 'ban',\n",
       " 'afraid',\n",
       " 'fuckin',\n",
       " 'offend',\n",
       " 'failed',\n",
       " 'hundred',\n",
       " 'arse',\n",
       " 'PM',\n",
       " 'globalnews',\n",
       " 'country',\n",
       " 'carrying',\n",
       " 'COVID19',\n",
       " 'kill',\n",
       " 'Friends',\n",
       " 'doesn',\n",
       " 'take',\n",
       " 'distancing',\n",
       " 'social',\n",
       " 'ask',\n",
       " 'age',\n",
       " 'etc',\n",
       " 'fear',\n",
       " 'hunter',\n",
       " 'gatherers',\n",
       " 'beginnings',\n",
       " 'profit',\n",
       " 'corporate',\n",
       " 'amplify',\n",
       " 'civilization',\n",
       " 'evolved',\n",
       " 'humble',\n",
       " 'fridaynightmusings',\n",
       " ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[el for el in vocab_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1Words = [\n",
    "    'blame',\n",
    "    'accuse',\n",
    "    'refuse',\n",
    "    'catastrophe',\n",
    "    'emergency',\n",
    "    'chaos',\n",
    "    'crisis',\n",
    "    'evil',\n",
    "    'fight',\n",
    "    'danger',\n",
    "    'hysteria',\n",
    "    'panic',\n",
    "    'paranoia',\n",
    "    'fear',\n",
    "    'fears',\n",
    "    'laugh',\n",
    "    'stupidity',\n",
    "    'hear',\n",
    "    'see',\n",
    "    'fee',\n",
    "    'perceive',\n",
    "    'look',\n",
    "    'appear',\n",
    "    'suggest'\n",
    "    'believe',\n",
    "    'believed',\n",
    "    'pretend',\n",
    "    'martial',\n",
    "    'kill',\n",
    "    'killing',\n",
    "    'kills',\n",
    "    'killed',\n",
    "    'die',\n",
    "    'threat',\n",
    "    'weapon',\n",
    "    'weaponize',\n",
    "    'weaponizing',\n",
    "    'knife',\n",
    "    'ussr',\n",
    "    'japan',\n",
    "    'chernobyl',\n",
    "    'wuhan',\n",
    "    'china',\n",
    "    'foreigners',\n",
    "    'cat',\n",
    "    'cats',\n",
    "    'dog',\n",
    "    'dogs',\n",
    "    'i',\n",
    "    'me',\n",
    "    'mine',\n",
    "    'my',\n",
    "    'you',\n",
    "    'yours',\n",
    "    'your',\n",
    "    'we',\n",
    "    'our',\n",
    "    'propaganda',\n",
    "    'fake',\n",
    "    'conspiracy',\n",
    "    'claim',\n",
    "    'claims',\n",
    "    'claiming',\n",
    "    'misleading',\n",
    "    'hoax',\n",
    "    'cure',\n",
    "    'breakthrough',\n",
    "    'bitch',\n",
    "    'wtf',\n",
    "    'dogbreath',\n",
    "    'zombie',\n",
    "    'junkies',\n",
    "    'hell',\n",
    "    'screwed',\n",
    "    'fuck',\n",
    "    'fucking',\n",
    "    'fucked',\n",
    "    'fuckin',\n",
    "    'wth',\n",
    "    'secular',\n",
    "    'bible',\n",
    "    'maga',\n",
    "    'magat',\n",
    "    'genetic',\n",
    "    'hillary',\n",
    "    'clinton',\n",
    "    'fundamentalist',\n",
    "    'market',\n",
    "    'communist',\n",
    "    'nazi',\n",
    "    'stock',\n",
    "    'bank',\n",
    "    'economy',\n",
    "    'economic',\n",
    "    'money',\n",
    "    'cost',\n",
    "    'costs',\n",
    "    'election',\n",
    "    'campaign',\n",
    "    'presidential',\n",
    "    'impeachment',\n",
    "    'rally',\n",
    "    'rallies',\n",
    "    'base',\n",
    "    'president',\n",
    "    'trump',\n",
    "    'war',\n",
    "    'wwiii',\n",
    "    'asteroid',\n",
    "    'banknotes',\n",
    "    'dangerous',\n",
    "    'invent',\n",
    "    'invented',\n",
    "    'iran',\n",
    "    'lie',\n",
    "    'lies',\n",
    "    'lying',\n",
    "    'lied',\n",
    "    'liar',\n",
    "    'liars',\n",
    "    'lmfao',\n",
    "    'lmfaoooooo',\n",
    "    'misinformation',\n",
    "    'news',\n",
    "    'media',\n",
    "    'financial',\n",
    "    'propagandawars',\n",
    "    'antidote'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_vocab = [el for el in vocab_list if el.lower() in table1Words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you',\n",
       " 'we',\n",
       " 'Trump',\n",
       " 'President',\n",
       " 'You',\n",
       " 'your',\n",
       " 'TRUMP',\n",
       " 'media',\n",
       " 'look',\n",
       " 'asteroid',\n",
       " 'WWIII',\n",
       " 'We',\n",
       " 'BANKNOTES',\n",
       " 'dangerous',\n",
       " 'economic',\n",
       " 'My',\n",
       " 'believed',\n",
       " 'knife',\n",
       " 'fight',\n",
       " 'EMERGENCY',\n",
       " 'NEWS',\n",
       " 'evil',\n",
       " 'invented',\n",
       " 'Iran',\n",
       " 'chaos',\n",
       " 'refuse',\n",
       " 'lying',\n",
       " 'me',\n",
       " 'Misleading',\n",
       " 'junkies',\n",
       " 'Zombie',\n",
       " 'market',\n",
       " 'Market',\n",
       " 'stock',\n",
       " 'claims',\n",
       " 'Wuhan',\n",
       " 'China',\n",
       " 'conspiracy',\n",
       " 'killing',\n",
       " 'lied',\n",
       " 'LMFAOOOOOO',\n",
       " 'threat',\n",
       " 'crisis',\n",
       " 'Bank',\n",
       " 'panic',\n",
       " 'Hell',\n",
       " 'Stupidity',\n",
       " 'Claims',\n",
       " 'Fears',\n",
       " 'Paranoia',\n",
       " 'hoax',\n",
       " 'claiming',\n",
       " 'weaponizing',\n",
       " 'Fear',\n",
       " 'campaign',\n",
       " 'Kills',\n",
       " 'my',\n",
       " 'fucked',\n",
       " 'I',\n",
       " 'our',\n",
       " 'base',\n",
       " 'rallies',\n",
       " 'emergency',\n",
       " 'screwed',\n",
       " 'weapon',\n",
       " 'Rallies',\n",
       " 'Clinton',\n",
       " 'laugh',\n",
       " 'cure',\n",
       " 'DIE',\n",
       " 'lie',\n",
       " 'Hoax',\n",
       " 'lies',\n",
       " 'economy',\n",
       " 'dog',\n",
       " 'see',\n",
       " 'news',\n",
       " 'pretend',\n",
       " 'misinformation',\n",
       " 'cost',\n",
       " 'Communist',\n",
       " 'fuckin',\n",
       " 'kill',\n",
       " 'fear',\n",
       " 'financial',\n",
       " 'hysteria',\n",
       " 'fucking',\n",
       " 'kills',\n",
       " 'killed',\n",
       " 'Fake',\n",
       " 'war',\n",
       " 'Killed',\n",
       " 'fears',\n",
       " 'impeachment',\n",
       " 'Cure',\n",
       " 'Conspiracy',\n",
       " 'FAKE',\n",
       " 'fake',\n",
       " 'News',\n",
       " 'STOCK',\n",
       " 'MARKET',\n",
       " 'ECONOMY',\n",
       " 'money',\n",
       " 'LIES',\n",
       " 'LYING',\n",
       " 'Japan',\n",
       " 'Chernobyl',\n",
       " 'USSR',\n",
       " 'KILL',\n",
       " 'MY',\n",
       " 'Lies',\n",
       " 'MAGA',\n",
       " 'Propaganda',\n",
       " 'Media',\n",
       " 'antidote',\n",
       " 'PropagandaWars']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cases': 0,\n",
       " 'new': 1,\n",
       " 'deaths': 2,\n",
       " 'coronavirus': 3,\n",
       " '60': 4,\n",
       " 'buy': 5,\n",
       " 'stocks': 6,\n",
       " 'back': 7,\n",
       " 'portfolio': 8,\n",
       " 'virus': 9,\n",
       " 'Reasons': 10,\n",
       " 'markets': 11,\n",
       " '1500': 12,\n",
       " 'recovered': 13,\n",
       " '2700': 14,\n",
       " 'to': 15,\n",
       " 'today': 16,\n",
       " 'Coronavirus': 17,\n",
       " 'inside': 18,\n",
       " '1': 19,\n",
       " '2': 20,\n",
       " 'cotton': 21,\n",
       " 'anus': 22,\n",
       " 'solutions': 23,\n",
       " 'water': 24,\n",
       " 'joke': 25,\n",
       " 'Dip': 26,\n",
       " 'viruses': 27,\n",
       " 'bottle': 28,\n",
       " 'place': 29,\n",
       " 'spit': 30,\n",
       " 'cleric': 31,\n",
       " 'Ayatollahs': 32,\n",
       " 'sleep': 33,\n",
       " 'blessings': 34,\n",
       " 'drinking': 35,\n",
       " 'have': 36,\n",
       " 'a': 37,\n",
       " 'him': 38,\n",
       " 'all': 39,\n",
       " 'and': 40,\n",
       " 'other': 41,\n",
       " 'of': 42,\n",
       " 'in': 43,\n",
       " 'before': 44,\n",
       " 'it': 45,\n",
       " 'for': 46,\n",
       " 'Not': 47,\n",
       " 'Take': 48,\n",
       " 'the': 49,\n",
       " 'oil': 50,\n",
       " 'you': 51,\n",
       " 'good': 52,\n",
       " 'kitchen': 53,\n",
       " 'cookies': 54,\n",
       " 'longer': 55,\n",
       " 'meeting': 56,\n",
       " 'leftovers': 57,\n",
       " 'Except': 58,\n",
       " 'staff': 59,\n",
       " 'set': 60,\n",
       " 'we': 61,\n",
       " 'out': 62,\n",
       " 'Because': 63,\n",
       " 'from': 64,\n",
       " 'will': 65,\n",
       " 'no': 66,\n",
       " 'Trump': 67,\n",
       " 'US': 68,\n",
       " 'people': 69,\n",
       " 'President': 70,\n",
       " 'visit': 71,\n",
       " 'amp': 72,\n",
       " 'embrace': 73,\n",
       " 'personal': 74,\n",
       " 'responsibility': 75,\n",
       " 'contract': 76,\n",
       " 'has': 77,\n",
       " 'who': 78,\n",
       " 'vaccine': 79,\n",
       " 'finding': 80,\n",
       " 'Israelis': 81,\n",
       " 'end': 82,\n",
       " '100': 83,\n",
       " 'unironic': 84,\n",
       " 'roughly': 85,\n",
       " 'NYT': 86,\n",
       " 'write': 87,\n",
       " 'likely': 88,\n",
       " 'do': 89,\n",
       " 'is': 90,\n",
       " 'If': 91,\n",
       " 'up': 92,\n",
       " 'an': 93,\n",
       " 'reason': 94,\n",
       " 'reached': 95,\n",
       " 'fallen': 96,\n",
       " 'Sflecce': 97,\n",
       " 'D19VP': 98,\n",
       " 'behind': 99,\n",
       " 'deal': 100,\n",
       " 'be': 101,\n",
       " 'Now': 102,\n",
       " 've': 103,\n",
       " 'about': 104,\n",
       " 't': 105,\n",
       " 'That': 106,\n",
       " 'You': 107,\n",
       " 'can': 108,\n",
       " 's': 109,\n",
       " 'test': 110,\n",
       " 'Florida': 111,\n",
       " 'police': 112,\n",
       " 'meth': 113,\n",
       " 'dept': 114,\n",
       " 'contaminated': 115,\n",
       " 'free': 116,\n",
       " 'with': 117,\n",
       " 'your': 118,\n",
       " 'Is': 119,\n",
       " 'This': 120,\n",
       " 'TRUMP': 121,\n",
       " 'Spread': 122,\n",
       " 'trying': 123,\n",
       " 'make': 124,\n",
       " 'inevitable': 125,\n",
       " 'control': 126,\n",
       " 'worse': 127,\n",
       " 'media': 128,\n",
       " 'look': 129,\n",
       " 'The': 130,\n",
       " 'under': 131,\n",
       " 'CDC': 132,\n",
       " 'than': 133,\n",
       " 'First': 134,\n",
       " 'year': 135,\n",
       " '2020': 136,\n",
       " 'defs': 137,\n",
       " 'asteroid': 138,\n",
       " 'WWIII': 139,\n",
       " 'dying': 140,\n",
       " 'now': 141,\n",
       " 'then': 142,\n",
       " 're': 143,\n",
       " 'What': 144,\n",
       " 'by': 145,\n",
       " 'We': 146,\n",
       " 'CORONAVIRUS': 147,\n",
       " 'Please': 148,\n",
       " 'BANKNOTES': 149,\n",
       " 'ALARMED': 150,\n",
       " 'irreversibly': 151,\n",
       " 'soon': 152,\n",
       " 'spread': 153,\n",
       " 'but': 154,\n",
       " 'through': 155,\n",
       " 'any': 156,\n",
       " 'don': 157,\n",
       " 'dangerous': 158,\n",
       " 'go': 159,\n",
       " 'work': 160,\n",
       " 'infected': 161,\n",
       " 'urged': 162,\n",
       " 'guidance': 163,\n",
       " 'contradicts': 164,\n",
       " 'idiot': 165,\n",
       " 'that': 166,\n",
       " 'WATCH': 167,\n",
       " 'everyone': 168,\n",
       " 'suggests': 169,\n",
       " 'Rick': 170,\n",
       " 'Santelli': 171,\n",
       " 'infecting': 172,\n",
       " 'impact': 173,\n",
       " 'economic': 174,\n",
       " 'lessen': 175,\n",
       " 'said': 176,\n",
       " 'realDonaldTrump': 177,\n",
       " 'hit': 178,\n",
       " 'cancer': 179,\n",
       " 'windmill': 180,\n",
       " 'littledeekay': 181,\n",
       " 'ears': 182,\n",
       " 'My': 183,\n",
       " 'are': 184,\n",
       " 'not': 185,\n",
       " 'yet': 186,\n",
       " 'refused': 187,\n",
       " 'testing': 188,\n",
       " 'positive': 189,\n",
       " 'zero': 190,\n",
       " 'believed': 191,\n",
       " 'Italy': 192,\n",
       " 'patient': 193,\n",
       " 'isolate': 194,\n",
       " 'Pakistani': 195,\n",
       " 'self': 196,\n",
       " 'migrant': 197,\n",
       " 'man': 198,\n",
       " 'after': 199,\n",
       " 'toilet': 200,\n",
       " 'amid': 201,\n",
       " 'knife': 202,\n",
       " 'shopper': 203,\n",
       " 'Australian': 204,\n",
       " 'paper': 205,\n",
       " 'draws': 206,\n",
       " 'fight': 207,\n",
       " 'stockpiling': 208,\n",
       " 'over': 209,\n",
       " 'LEVEL': 210,\n",
       " 'EMERGENCY': 211,\n",
       " 'HIGHEST': 212,\n",
       " 'ENGLAND': 213,\n",
       " 'NHS': 214,\n",
       " 'INCIDENT': 215,\n",
       " 'SKY': 216,\n",
       " '4': 217,\n",
       " 'DECLARED': 218,\n",
       " 'NEWS': 219,\n",
       " 'IS': 220,\n",
       " 'OF': 221,\n",
       " 'A': 222,\n",
       " 'WHICH': 223,\n",
       " 'HAS': 224,\n",
       " 'THE': 225,\n",
       " 'need': 226,\n",
       " 'pathetic': 227,\n",
       " 'States': 228,\n",
       " 'VoteBlueNoMatterWho2020': 229,\n",
       " 'evil': 230,\n",
       " 'Perfect': 231,\n",
       " 'VoteBlueToEndThisNightmare': 232,\n",
       " 'funding': 233,\n",
       " 'when': 234,\n",
       " 'Just': 235,\n",
       " 'more': 236,\n",
       " 'invented': 237,\n",
       " 'calories': 238,\n",
       " 'standardized': 239,\n",
       " 'Apparently': 240,\n",
       " 'DM': 241,\n",
       " 'Obama': 242,\n",
       " 'also': 243,\n",
       " 'cuts': 244,\n",
       " 'only': 245,\n",
       " 'he': 246,\n",
       " 'Iran': 247,\n",
       " '8': 248,\n",
       " '000': 249,\n",
       " 'parliament': 250,\n",
       " '54': 251,\n",
       " 'incapable': 252,\n",
       " 'prisoners': 253,\n",
       " 'chaos': 254,\n",
       " 'gvmnt': 255,\n",
       " 'released': 256,\n",
       " 'descending': 257,\n",
       " 'into': 258,\n",
       " 'refuse': 259,\n",
       " 'woman': 260,\n",
       " 'tell': 261,\n",
       " 'Doctors': 262,\n",
       " 'her': 263,\n",
       " 'she': 264,\n",
       " 'Houses': 265,\n",
       " 'Passes': 266,\n",
       " 'Miracle': 267,\n",
       " 'Sauce': 268,\n",
       " 'Smeared': 269,\n",
       " 'Fil': 270,\n",
       " 'Door': 271,\n",
       " 'Posts': 272,\n",
       " 'Chick': 273,\n",
       " 'On': 274,\n",
       " 'Over': 275,\n",
       " 'With': 276,\n",
       " 'lying': 277,\n",
       " 'infinitely': 278,\n",
       " 'less': 279,\n",
       " 'credible': 280,\n",
       " 'crap': 281,\n",
       " 'just': 282,\n",
       " 'or': 283,\n",
       " 'me': 284,\n",
       " 'ply': 285,\n",
       " 'roll': 286,\n",
       " '24': 287,\n",
       " 'Cashmere': 288,\n",
       " 'WiseGuysLiquor': 289,\n",
       " 'receive': 290,\n",
       " 'Sale': 291,\n",
       " 'Buy': 292,\n",
       " 'Corona': 293,\n",
       " 'at': 294,\n",
       " 'on': 295,\n",
       " 'Of': 296,\n",
       " 'J': 297,\n",
       " 'men': 298,\n",
       " 'Hardick': 299,\n",
       " 'virologist': 300,\n",
       " 'Dr': 301,\n",
       " 'explained': 302,\n",
       " 'solely': 303,\n",
       " 'Leading': 304,\n",
       " 'B': 305,\n",
       " 'confirms': 306,\n",
       " 'BREAKING': 307,\n",
       " 'Latest': 308,\n",
       " 'Data': 309,\n",
       " 'Numbers': 310,\n",
       " 'Tell': 311,\n",
       " 'Still': 312,\n",
       " 'Misleading': 313,\n",
       " 'Don': 314,\n",
       " 'Here': 315,\n",
       " 'going': 316,\n",
       " 'Portland': 317,\n",
       " 'SF': 318,\n",
       " 'junkies': 319,\n",
       " 'Zombie': 320,\n",
       " 'happen': 321,\n",
       " 'one': 322,\n",
       " 'Seattle': 323,\n",
       " 'LA': 324,\n",
       " 'homeless': 325,\n",
       " 'When': 326,\n",
       " 'Health': 327,\n",
       " 'limit': 328,\n",
       " 'Could': 329,\n",
       " 'Cash': 330,\n",
       " 'advises': 331,\n",
       " 'cease': 332,\n",
       " 'interaction': 333,\n",
       " 'World': 334,\n",
       " 'Spreading': 335,\n",
       " 'Organization': 336,\n",
       " 'Be': 337,\n",
       " 'market': 338,\n",
       " 'Market': 339,\n",
       " 'stock': 340,\n",
       " 'rules': 341,\n",
       " 'dictate': 342,\n",
       " 'Fundamentalists': 343,\n",
       " 'reality': 344,\n",
       " 'convince': 345,\n",
       " 'nature': 346,\n",
       " 'Free': 347,\n",
       " 'isn': 348,\n",
       " 'doing': 349,\n",
       " 'here': 350,\n",
       " 'what': 351,\n",
       " 'using': 352,\n",
       " 'kits': 353,\n",
       " 'previously': 354,\n",
       " 'owned': 355,\n",
       " 'company': 356,\n",
       " 'been': 357,\n",
       " 'there': 358,\n",
       " 'call': 359,\n",
       " 'racist': 360,\n",
       " 'claims': 361,\n",
       " 'Wuhan': 362,\n",
       " 'RepGosar': 363,\n",
       " 'integrity': 364,\n",
       " 'lol': 365,\n",
       " 'xenophobic': 366,\n",
       " 'Virus': 367,\n",
       " 'these': 368,\n",
       " 'its': 369,\n",
       " 'China': 370,\n",
       " 'flu': 371,\n",
       " 'governments': 372,\n",
       " 'infection': 373,\n",
       " 'says': 374,\n",
       " 'compares': 375,\n",
       " 'doctor': 376,\n",
       " 'hysterical': 377,\n",
       " 'Senior': 378,\n",
       " 'German': 379,\n",
       " 'So': 380,\n",
       " 'run': 381,\n",
       " 'get': 382,\n",
       " 'job': 383,\n",
       " 'theory': 384,\n",
       " 'conspiracy': 385,\n",
       " 'ought': 386,\n",
       " 'Somebody': 387,\n",
       " 'multinational': 388,\n",
       " 'put': 389,\n",
       " 'effect': 390,\n",
       " 'killing': 391,\n",
       " 'CoV': 392,\n",
       " 'fat': 393,\n",
       " 'surrounded': 394,\n",
       " 'effective': 395,\n",
       " 'dissolving': 396,\n",
       " 'Soap': 397,\n",
       " 'SARS': 398,\n",
       " 'MORE': 399,\n",
       " 'let': 400,\n",
       " 'better': 401,\n",
       " 'would': 402,\n",
       " 'anchor': 403,\n",
       " 'CNBC': 404,\n",
       " 'so': 405,\n",
       " 'Americans': 406,\n",
       " 'Hold': 407,\n",
       " 'Admits': 408,\n",
       " 'Stranded': 409,\n",
       " 'Flatly': 410,\n",
       " 'Leave': 411,\n",
       " 'Static': 412,\n",
       " 'He': 413,\n",
       " 'd': 414,\n",
       " 'lied': 415,\n",
       " 'symptoms': 416,\n",
       " 'LMFAOOOOOO': 417,\n",
       " 'Bro': 418,\n",
       " 'street': 419,\n",
       " 'wouldn': 420,\n",
       " 'having': 421,\n",
       " 'health': 422,\n",
       " 'always': 423,\n",
       " 'thing': 424,\n",
       " 'threat': 425,\n",
       " 'mass': 426,\n",
       " 'It': 427,\n",
       " 'because': 428,\n",
       " 'was': 429,\n",
       " 'never': 430,\n",
       " 'crisis': 431,\n",
       " 'probably': 432,\n",
       " 'guide': 433,\n",
       " 'phase': 434,\n",
       " 'CarmichaelKevin': 435,\n",
       " 'writes': 436,\n",
       " 'prices': 437,\n",
       " 'Bank': 438,\n",
       " 'Oil': 439,\n",
       " 'early': 440,\n",
       " 'Canada': 441,\n",
       " 'Let': 442,\n",
       " 'Democrats': 443,\n",
       " 'panic': 444,\n",
       " 'mainstream': 445,\n",
       " 'incite': 446,\n",
       " 'straight': 447,\n",
       " 'this': 448,\n",
       " 'Supplies': 449,\n",
       " 'Hell': 450,\n",
       " 'Medical': 451,\n",
       " 'Manufacturing': 452,\n",
       " 'Stupidity': 453,\n",
       " 'Pure': 454,\n",
       " 'Medicine': 455,\n",
       " 'Depending': 456,\n",
       " 'OR': 457,\n",
       " 'Death': 458,\n",
       " 'People': 459,\n",
       " 'like': 460,\n",
       " 'Flu': 461,\n",
       " 'Know': 462,\n",
       " 'Apples': 463,\n",
       " 'Ratio': 464,\n",
       " 'Oranges': 465,\n",
       " 'Intelligent': 466,\n",
       " 'Between': 467,\n",
       " 'collapsed': 468,\n",
       " 'may': 469,\n",
       " 'Recession': 470,\n",
       " 'called': 471,\n",
       " 'imminent': 472,\n",
       " 'Thousands': 473,\n",
       " 'And': 474,\n",
       " 'died': 475,\n",
       " 'tool': 476,\n",
       " 'hyperbole': 477,\n",
       " 'TrumpLies': 478,\n",
       " 'MoronInChief': 479,\n",
       " 'fix': 480,\n",
       " 'toolbox': 481,\n",
       " 'his': 482,\n",
       " 'Want': 483,\n",
       " 'Claims': 484,\n",
       " 'Goes': 485,\n",
       " 'Fox': 486,\n",
       " 'Use': 487,\n",
       " 'Host': 488,\n",
       " 'BONKERS': 489,\n",
       " 'Impeach': 490,\n",
       " 'Dems': 491,\n",
       " 'Fears': 492,\n",
       " 'Ya': 493,\n",
       " 'Destroy': 494,\n",
       " 'Purpose': 495,\n",
       " 'Paranoia': 496,\n",
       " 'Melting': 497,\n",
       " 'Someone': 498,\n",
       " 'Give': 499,\n",
       " 'Press': 500,\n",
       " 'Down': 501,\n",
       " 'Will': 502,\n",
       " 'Him': 503,\n",
       " 'hoax': 504,\n",
       " 'claiming': 505,\n",
       " 'weaponizing': 506,\n",
       " 'Mulvaney': 507,\n",
       " 'cuckoo': 508,\n",
       " 'rattled': 509,\n",
       " 'liberals': 510,\n",
       " 'CPAC': 511,\n",
       " 'Mick': 512,\n",
       " 'At': 513,\n",
       " 'off': 514,\n",
       " 'Fear': 515,\n",
       " 'via': 516,\n",
       " 'epidemic': 517,\n",
       " 'Hub': 518,\n",
       " 'volatility': 519,\n",
       " 'real': 520,\n",
       " 'if': 521,\n",
       " 'Donald': 522,\n",
       " 'rather': 523,\n",
       " 'something': 524,\n",
       " 'treating': 525,\n",
       " 'could': 526,\n",
       " 'somethi': 527,\n",
       " 'negatively': 528,\n",
       " 'as': 529,\n",
       " 'campaign': 530,\n",
       " 'Kills': 531,\n",
       " 'White': 532,\n",
       " 'Melania': 533,\n",
       " 'Pavillion': 534,\n",
       " 'House': 535,\n",
       " 'Tantrums': 536,\n",
       " 'Tennis': 537,\n",
       " 'As': 538,\n",
       " 'goes': 539,\n",
       " 'literally': 540,\n",
       " 'dont': 541,\n",
       " 'common': 542,\n",
       " 'realize': 543,\n",
       " 'berezin_goal': 544,\n",
       " 'Newmarket': 545,\n",
       " 'cold': 546,\n",
       " 'smh': 547,\n",
       " 'Same': 548,\n",
       " 'takes': 549,\n",
       " 'graphic': 550,\n",
       " 'sanitiser': 551,\n",
       " 'how': 552,\n",
       " 'hand': 553,\n",
       " 'death': 554,\n",
       " 'confirmed': 555,\n",
       " 'first': 556,\n",
       " 'patients': 557,\n",
       " 'none': 558,\n",
       " 'talks': 559,\n",
       " 'Netherlands': 560,\n",
       " '56': 561,\n",
       " 'business': 562,\n",
       " 'discharge': 563,\n",
       " 'But': 564,\n",
       " 'my': 565,\n",
       " 'RickSantelli': 566,\n",
       " 'charge': 567,\n",
       " 'exposed': 568,\n",
       " 'blocking': 569,\n",
       " 'MUST': 570,\n",
       " 'much': 571,\n",
       " 'GOP': 572,\n",
       " 'drugmakers': 573,\n",
       " 'limits': 574,\n",
       " 'bill': 575,\n",
       " 'NOW': 576,\n",
       " 'stop': 577,\n",
       " 'CoronaVirus': 578,\n",
       " 'Wear': 579,\n",
       " 'CoronaVirusChallenge': 580,\n",
       " 'HIV': 581,\n",
       " 'condoms': 582,\n",
       " 'hands': 583,\n",
       " 'fucked': 584,\n",
       " 'ring': 585,\n",
       " 'alarm': 586,\n",
       " 'CoronaVirusSeattle': 587,\n",
       " 'bells': 588,\n",
       " 'might': 589,\n",
       " 'm': 590,\n",
       " 'I': 591,\n",
       " 'countries': 592,\n",
       " 'different': 593,\n",
       " '10': 594,\n",
       " 'WHO': 595,\n",
       " 'produced': 596,\n",
       " 'seems': 597,\n",
       " 'say': 598,\n",
       " 'refuses': 599,\n",
       " 'tedlieu': 600,\n",
       " 'Mike': 601,\n",
       " 'Pompeo': 602,\n",
       " 'Deny': 603,\n",
       " 'even': 604,\n",
       " 'POTUS': 605,\n",
       " 'continue': 606,\n",
       " 'manage': 607,\n",
       " 'VP': 608,\n",
       " 'professionals': 609,\n",
       " 'nation': 610,\n",
       " 'decisively': 611,\n",
       " 'aside': 612,\n",
       " 'our': 613,\n",
       " 'Friday': 614,\n",
       " 'Daily': 615,\n",
       " 'published': 616,\n",
       " 'treat': 617,\n",
       " 'Politico': 618,\n",
       " 'article': 619,\n",
       " 'base': 620,\n",
       " 'rallies': 621,\n",
       " 'headlined': 622,\n",
       " 'emergency': 623,\n",
       " 'Pence': 624,\n",
       " 'solution': 625,\n",
       " 'team': 626,\n",
       " 'praying': 627,\n",
       " 'screwed': 628,\n",
       " 'thousands': 629,\n",
       " 'open': 630,\n",
       " 'One': 631,\n",
       " 'doors': 632,\n",
       " 'Hamilton': 633,\n",
       " 'Trudeau': 634,\n",
       " 'welcome': 635,\n",
       " 'throws': 636,\n",
       " 'flying': 637,\n",
       " 'really': 638,\n",
       " 'defend': 639,\n",
       " 'degree': 640,\n",
       " 'brilliant': 641,\n",
       " 'idea': 642,\n",
       " 'dotard': 643,\n",
       " 'against': 644,\n",
       " 'Iranian': 645,\n",
       " 'weapon': 646,\n",
       " 'regime': 647,\n",
       " 'attempts': 648,\n",
       " 'care': 649,\n",
       " 'taking': 650,\n",
       " 'continues': 651,\n",
       " 'nuclear': 652,\n",
       " 'Instead': 653,\n",
       " 'achieve': 654,\n",
       " 'victims': 655,\n",
       " 'Democrat': 656,\n",
       " 'Tweets': 657,\n",
       " 'Supporters': 658,\n",
       " 'Support': 659,\n",
       " 'Large': 660,\n",
       " 'Infecting': 661,\n",
       " 'Amounts': 662,\n",
       " 'Rallies': 663,\n",
       " 'Her': 664,\n",
       " 'By': 665,\n",
       " 'Gets': 666,\n",
       " 'Attempts': 667,\n",
       " 'Chelsea': 668,\n",
       " 'BRUTAL': 669,\n",
       " 'Clinton': 670,\n",
       " 'Lecture': 671,\n",
       " 'Destroyed': 672,\n",
       " 'Facts': 673,\n",
       " 'gonna': 674,\n",
       " 'eat': 675,\n",
       " 'sugar': 676,\n",
       " 'brown': 677,\n",
       " 'oatmeal': 678,\n",
       " 'sprinkles': 679,\n",
       " 'DS8': 680,\n",
       " 'DD6': 681,\n",
       " 'too': 682,\n",
       " 'Patient': 683,\n",
       " 'Zero': 684,\n",
       " 'DEAD': 685,\n",
       " '1st': 686,\n",
       " 'NorthKorea': 687,\n",
       " 'Problem': 688,\n",
       " 'quote': 689,\n",
       " 'Stalin': 690,\n",
       " 'loosely': 691,\n",
       " 'Shot': 692,\n",
       " 'For': 693,\n",
       " 'To': 694,\n",
       " 'No': 695,\n",
       " 'pictures': 696,\n",
       " 'Soooo': 697,\n",
       " 'Briefing': 698,\n",
       " 'Boring': 699,\n",
       " 'Instatute': 700,\n",
       " 'National': 701,\n",
       " 'show': 702,\n",
       " 'they': 703,\n",
       " 'had': 704,\n",
       " 'did': 705,\n",
       " 'Wash': 706,\n",
       " 'laugh': 707,\n",
       " 'ADVICE': 708,\n",
       " 'Book': 709,\n",
       " 'vacation': 710,\n",
       " 'll': 711,\n",
       " 'hold': 712,\n",
       " 'brexit': 713,\n",
       " 'beer': 714,\n",
       " 'fuelling': 715,\n",
       " 'sure': 716,\n",
       " 'racism': 717,\n",
       " 'great': 718,\n",
       " 'xenophobia': 719,\n",
       " 'cure': 720,\n",
       " 'use': 721,\n",
       " 'know': 722,\n",
       " 'powers': 723,\n",
       " 'keystroke': 724,\n",
       " 'ability': 725,\n",
       " 'ThePME': 726,\n",
       " 'Help': 727,\n",
       " 'Research': 728,\n",
       " 'Services': 729,\n",
       " 'Salary': 730,\n",
       " 'Donates': 731,\n",
       " 'Q4': 732,\n",
       " 'Human': 733,\n",
       " 'false': 734,\n",
       " 'helping': 735,\n",
       " 'pushing': 736,\n",
       " 'spin': 737,\n",
       " 'column': 738,\n",
       " 'some': 739,\n",
       " 'GOING': 740,\n",
       " 'LIVE': 741,\n",
       " 'DOESN': 742,\n",
       " 'T': 743,\n",
       " 'SPRAY': 744,\n",
       " 'DIE': 745,\n",
       " 'OUTSIDE': 746,\n",
       " 'FOREVER': 747,\n",
       " 'ANYWAY': 748,\n",
       " 'Disinfection': 749,\n",
       " 'HOST': 750,\n",
       " 'OFF': 751,\n",
       " 'WHY': 752,\n",
       " 'THEN': 753,\n",
       " 'IF': 754,\n",
       " 'S': 755,\n",
       " 'TO': 756,\n",
       " 'ITS': 757,\n",
       " 'IT': 758,\n",
       " 'lie': 759,\n",
       " 'administration': 760,\n",
       " 'another': 761,\n",
       " 'spins': 762,\n",
       " 'DC': 763,\n",
       " 'troubling': 764,\n",
       " 'most': 765,\n",
       " 'response': 766,\n",
       " 'Hoax': 767,\n",
       " 'U': 768,\n",
       " 'mischaracterized': 769,\n",
       " 'way': 770,\n",
       " 'NOT': 771,\n",
       " 'spreading': 772,\n",
       " 'Trying': 773,\n",
       " 'lies': 774,\n",
       " 'points': 775,\n",
       " 'political': 776,\n",
       " 'hurt': 777,\n",
       " 'score': 778,\n",
       " 'cheap': 779,\n",
       " 'Chinese': 780,\n",
       " 'America': 781,\n",
       " 'Thanks': 782,\n",
       " 'chance': 783,\n",
       " 'giving': 784,\n",
       " 'reliance': 785,\n",
       " 'supplies': 786,\n",
       " 'HudsonInstitute': 787,\n",
       " 'electronics': 788,\n",
       " 'vital': 789,\n",
       " 'medical': 790,\n",
       " 'discuss': 791,\n",
       " 'manufacturing': 792,\n",
       " 'economy': 793,\n",
       " 'dog': 794,\n",
       " 'germs': 795,\n",
       " 'fur': 796,\n",
       " 'beards': 797,\n",
       " 'hipsters': 798,\n",
       " 'Swiss': 799,\n",
       " 'Enjoy': 800,\n",
       " 'Lumberjack': 801,\n",
       " 'researchers': 802,\n",
       " 'carry': 803,\n",
       " 'according': 804,\n",
       " 'entire': 805,\n",
       " 'herd': 806,\n",
       " 'thin': 807,\n",
       " 'Maybe': 808,\n",
       " 'mission': 809,\n",
       " 'communications': 810,\n",
       " 'information': 811,\n",
       " 'interfere': 812,\n",
       " 'attempting': 813,\n",
       " 'interfering': 814,\n",
       " 'CREW': 815,\n",
       " 'requesting': 816,\n",
       " 'indicate': 817,\n",
       " 'release': 818,\n",
       " 'began': 819,\n",
       " 'see': 820,\n",
       " 'since': 821,\n",
       " 'diagnosed': 822,\n",
       " 'news': 823,\n",
       " '90': 824,\n",
       " 'often': 825,\n",
       " 'Idk': 826,\n",
       " 'stat': 827,\n",
       " 'why': 828,\n",
       " 'wants': 829,\n",
       " 'risk': 830,\n",
       " 'putting': 831,\n",
       " 'contained': 832,\n",
       " 'vulnerable': 833,\n",
       " 'pretend': 834,\n",
       " 'millions': 835,\n",
       " 'anyone': 836,\n",
       " 'ChrisCuomo': 837,\n",
       " 'goodness': 838,\n",
       " 'cal': 839,\n",
       " 'foreign': 840,\n",
       " 'townhallcom': 841,\n",
       " 'Oh': 842,\n",
       " 'Can': 843,\n",
       " 'Phase': 844,\n",
       " 'Getting': 845,\n",
       " 'Incredible': 846,\n",
       " 'ever': 847,\n",
       " 'occurred': 848,\n",
       " 'fastest': 849,\n",
       " 'Praises': 850,\n",
       " 'Thing': 851,\n",
       " 'Every': 852,\n",
       " 'California': 853,\n",
       " 'Single': 854,\n",
       " 'Governor': 855,\n",
       " 'Ship': 856,\n",
       " 'Said': 857,\n",
       " 'Response': 858,\n",
       " 'Cruise': 859,\n",
       " 'suggest': 860,\n",
       " 'leaked': 861,\n",
       " 'Norwegian': 862,\n",
       " 'emails': 863,\n",
       " 'wow': 864,\n",
       " 'persuade': 865,\n",
       " 'Lines': 866,\n",
       " 'cancel': 867,\n",
       " 'misinformation': 868,\n",
       " 'customers': 869,\n",
       " 'last': 870,\n",
       " 'week': 871,\n",
       " 'seriously': 872,\n",
       " 'overreaction': 873,\n",
       " 'blast': 874,\n",
       " 'blasted': 875,\n",
       " 'same': 876,\n",
       " 'Like': 877,\n",
       " 'Meet': 878,\n",
       " 'Childish': 879,\n",
       " 'Doesn': 880,\n",
       " 'Pelosi': 881,\n",
       " 'Won': 882,\n",
       " 'shutting': 883,\n",
       " 'Harvard': 884,\n",
       " 'home': 885,\n",
       " 'Corvid19': 886,\n",
       " 'ruling': 887,\n",
       " 'sit': 888,\n",
       " 'University': 889,\n",
       " 'class': 890,\n",
       " 'down': 891,\n",
       " 'British': 892,\n",
       " 'pay': 893,\n",
       " 'Any': 894,\n",
       " 'offering': 895,\n",
       " 'takers': 896,\n",
       " '4600': 897,\n",
       " 'means': 898,\n",
       " 'getting': 899,\n",
       " 'around': 900,\n",
       " 'start': 901,\n",
       " 'warmer': 902,\n",
       " 'suck': 903,\n",
       " 'mosquitos': 904,\n",
       " 'blood': 905,\n",
       " 'which': 906,\n",
       " 'global': 907,\n",
       " 'least': 908,\n",
       " 'cost': 909,\n",
       " 'development': 910,\n",
       " 'world': 911,\n",
       " 'creating': 912,\n",
       " 'perform': 913,\n",
       " 'ocean': 914,\n",
       " 'frank': 915,\n",
       " 'coachella': 916,\n",
       " 'respiratory': 917,\n",
       " 'MERS': 918,\n",
       " 'viral': 919,\n",
       " 'Middle': 920,\n",
       " 'East': 921,\n",
       " 'syndrome': 922,\n",
       " 'camel': 923,\n",
       " 'known': 924,\n",
       " 'caused': 925,\n",
       " 'Homeless': 926,\n",
       " 'Dirty': 927,\n",
       " 'Responible': 928,\n",
       " 'District': 929,\n",
       " 'Clean': 930,\n",
       " 'She': 931,\n",
       " 'holding': 932,\n",
       " 'Government': 933,\n",
       " 'accounts': 934,\n",
       " 'looks': 935,\n",
       " 'though': 936,\n",
       " 'certain': 937,\n",
       " 'worried': 938,\n",
       " 'bad': 939,\n",
       " 'noticed': 940,\n",
       " 'realdonaldtrump': 941,\n",
       " 'cool': 942,\n",
       " 'Have': 943,\n",
       " 'Owns': 944,\n",
       " 'Institute': 945,\n",
       " 'PIRBRIGHT': 946,\n",
       " 'Bill': 947,\n",
       " 'Gates': 948,\n",
       " 'Funded': 949,\n",
       " 'Patent': 950,\n",
       " 'Which': 951,\n",
       " 'beginning': 952,\n",
       " 'Communist': 953,\n",
       " 'help': 954,\n",
       " 'journalists': 955,\n",
       " 'arrested': 956,\n",
       " 'penny': 957,\n",
       " 'CoronavirusPandemic': 958,\n",
       " 'CoronaVirusCanada': 959,\n",
       " 'bring': 960,\n",
       " 'time': 961,\n",
       " 'Given': 962,\n",
       " 'properties': 963,\n",
       " 'copper': 964,\n",
       " 'antiviral': 965,\n",
       " 'CoronaVirusUpdate': 966,\n",
       " 'covid19': 967,\n",
       " 'ban': 968,\n",
       " 'afraid': 969,\n",
       " 'fuckin': 970,\n",
       " 'offend': 971,\n",
       " 'failed': 972,\n",
       " 'hundred': 973,\n",
       " 'arse': 974,\n",
       " 'PM': 975,\n",
       " 'globalnews': 976,\n",
       " 'country': 977,\n",
       " 'carrying': 978,\n",
       " 'COVID19': 979,\n",
       " 'kill': 980,\n",
       " 'Friends': 981,\n",
       " 'doesn': 982,\n",
       " 'take': 983,\n",
       " 'distancing': 984,\n",
       " 'social': 985,\n",
       " 'ask': 986,\n",
       " 'age': 987,\n",
       " 'etc': 988,\n",
       " 'fear': 989,\n",
       " 'hunter': 990,\n",
       " 'gatherers': 991,\n",
       " 'beginnings': 992,\n",
       " 'profit': 993,\n",
       " 'corporate': 994,\n",
       " 'amplify': 995,\n",
       " 'civilization': 996,\n",
       " 'evolved': 997,\n",
       " 'humble': 998,\n",
       " 'fridaynightmusings': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDict = dict()\n",
    "\n",
    "for i in range(len(vocab_list)):\n",
    "    myDict[vocab_list[i]] = i\n",
    "    \n",
    "myDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'you': 0,\n",
       " 'we': 1,\n",
       " 'Trump': 2,\n",
       " 'President': 3,\n",
       " 'You': 4,\n",
       " 'your': 5,\n",
       " 'TRUMP': 6,\n",
       " 'media': 7,\n",
       " 'look': 8,\n",
       " 'asteroid': 9,\n",
       " 'WWIII': 10,\n",
       " 'We': 11,\n",
       " 'BANKNOTES': 12,\n",
       " 'dangerous': 13,\n",
       " 'economic': 14,\n",
       " 'My': 15,\n",
       " 'believed': 16,\n",
       " 'knife': 17,\n",
       " 'fight': 18,\n",
       " 'EMERGENCY': 19,\n",
       " 'NEWS': 20,\n",
       " 'evil': 21,\n",
       " 'invented': 22,\n",
       " 'Iran': 23,\n",
       " 'chaos': 24,\n",
       " 'refuse': 25,\n",
       " 'lying': 26,\n",
       " 'me': 27,\n",
       " 'Misleading': 28,\n",
       " 'junkies': 29,\n",
       " 'Zombie': 30,\n",
       " 'market': 31,\n",
       " 'Market': 32,\n",
       " 'stock': 33,\n",
       " 'claims': 34,\n",
       " 'Wuhan': 35,\n",
       " 'China': 36,\n",
       " 'conspiracy': 37,\n",
       " 'killing': 38,\n",
       " 'lied': 39,\n",
       " 'LMFAOOOOOO': 40,\n",
       " 'threat': 41,\n",
       " 'crisis': 42,\n",
       " 'Bank': 43,\n",
       " 'panic': 44,\n",
       " 'Hell': 45,\n",
       " 'Stupidity': 46,\n",
       " 'Claims': 47,\n",
       " 'Fears': 48,\n",
       " 'Paranoia': 49,\n",
       " 'hoax': 50,\n",
       " 'claiming': 51,\n",
       " 'weaponizing': 52,\n",
       " 'Fear': 53,\n",
       " 'campaign': 54,\n",
       " 'Kills': 55,\n",
       " 'my': 56,\n",
       " 'fucked': 57,\n",
       " 'I': 58,\n",
       " 'our': 59,\n",
       " 'base': 60,\n",
       " 'rallies': 61,\n",
       " 'emergency': 62,\n",
       " 'screwed': 63,\n",
       " 'weapon': 64,\n",
       " 'Rallies': 65,\n",
       " 'Clinton': 66,\n",
       " 'laugh': 67,\n",
       " 'cure': 68,\n",
       " 'DIE': 69,\n",
       " 'lie': 70,\n",
       " 'Hoax': 71,\n",
       " 'lies': 72,\n",
       " 'economy': 73,\n",
       " 'dog': 74,\n",
       " 'see': 75,\n",
       " 'news': 76,\n",
       " 'pretend': 77,\n",
       " 'misinformation': 78,\n",
       " 'cost': 79,\n",
       " 'Communist': 80,\n",
       " 'fuckin': 81,\n",
       " 'kill': 82,\n",
       " 'fear': 83,\n",
       " 'financial': 84,\n",
       " 'hysteria': 85,\n",
       " 'fucking': 86,\n",
       " 'kills': 87,\n",
       " 'killed': 88,\n",
       " 'Fake': 89,\n",
       " 'war': 90,\n",
       " 'Killed': 91,\n",
       " 'fears': 92,\n",
       " 'impeachment': 93,\n",
       " 'Cure': 94,\n",
       " 'Conspiracy': 95,\n",
       " 'FAKE': 96,\n",
       " 'fake': 97,\n",
       " 'News': 98,\n",
       " 'STOCK': 99,\n",
       " 'MARKET': 100,\n",
       " 'ECONOMY': 101,\n",
       " 'money': 102,\n",
       " 'LIES': 103,\n",
       " 'LYING': 104,\n",
       " 'Japan': 105,\n",
       " 'Chernobyl': 106,\n",
       " 'USSR': 107,\n",
       " 'KILL': 108,\n",
       " 'MY': 109,\n",
       " 'Lies': 110,\n",
       " 'MAGA': 111,\n",
       " 'Propaganda': 112,\n",
       " 'Media': 113,\n",
       " 'antidote': 114,\n",
       " 'PropagandaWars': 115}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDictFiltered = dict()\n",
    "\n",
    "for i in range(len(filtered_vocab)):\n",
    "    myDictFiltered[filtered_vocab[i]] = i\n",
    "    \n",
    "myDictFiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_matrix = np.zeros((len(explanations), len(vocab_list)))\n",
    "\n",
    "for i in range(len(explanations)):\n",
    "    expl = explanations[i]\n",
    "    for j in range(len(expl)):\n",
    "        word = expl[j][0]\n",
    "        val = expl[j][1]\n",
    "        \n",
    "        if val > 0:\n",
    "            my_matrix[i, myDict[word]] = 1\n",
    "        \n",
    "        if val < 0:\n",
    "            my_matrix[i, myDict[word]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1., -1., -1., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ...,\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  1.,  1.,  1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "myMatFilt = np.zeros((len(explanations), len(vocab_list)))\n",
    "\n",
    "for i in range(len(explanations)):\n",
    "    expl = explanations[i]\n",
    "    for j in range(len(expl)):\n",
    "        word = expl[j][0]\n",
    "        val = expl[j][1]\n",
    "        \n",
    "        if word in filtered_vocab:\n",
    "        \n",
    "            if val > 0:\n",
    "                myMatFilt[i, myDictFiltered[word]] = 1\n",
    "\n",
    "            if val < 0:\n",
    "                myMatFilt[i, myDictFiltered[word]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myMatFilt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized by number of nonzero elements (per tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalize for associating Table 1 word with wrong class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in myMatFilt:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3531481481481481"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalNonZero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only count if Table 1 word associated with correct class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in myMatFilt:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        if el > 0:\n",
    "            rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5821296296296297"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to LIME, the algorithm associated 58.2% of Table 1 words with the unreliable class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized by total number of Table 1 vocab words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_scores = np.sum(myMatFilt, axis = 1)\n",
    "tweet_scores = tweet_scores/myMatFilt.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000482218203737191"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# overall score\n",
    "tweet_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think this is the wrong way to do things because it \"penalizes\" (drags down the tweet score by increasing the denominator) for words that didn't occur in each tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save list of explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('explanation_list', 'wb') as f:\n",
    "    pickle.dump(explanations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
