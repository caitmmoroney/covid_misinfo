{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load BERT embeddings\n",
    "embedded_tweets = np.load('bert_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, probability=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate classification algorithm\n",
    "\n",
    "# round 1 winner\n",
    "svc = SVC(C = 1, kernel = 'rbf', probability = True)\n",
    "\n",
    "class1_train_indices = list(range(100))\n",
    "class0_train_indices = list(range(280,380))\n",
    "\n",
    "train_X = embedded_tweets[[class1_train_indices + class0_train_indices],:][0]\n",
    "\n",
    "hundred_ones = [1]*100\n",
    "hundred_zeros = [0]*100\n",
    "train_Y = hundred_ones + hundred_zeros\n",
    "\n",
    "# fit SVC model on training subset of tweet embeddings\n",
    "svc.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disaggregate correct/incorrect predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test set\n",
    "test_indices = list(range(100,280))\n",
    "test_X = embedded_tweets[[test_indices],:][0]\n",
    "\n",
    "# model predictions for test set\n",
    "y_hat = svc.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct or not\n",
    "y_bool = y_hat==1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load list of explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in explanations list\n",
    "with open('bert_explanation_list', 'rb') as f:\n",
    "    explanations = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('buy', 0.051218169268151266),\n",
       " ('today', 0.04564912805488858),\n",
       " ('recovered', 0.03684671343606235),\n",
       " ('coronavirus', 0.03167357242077659),\n",
       " ('2700', 0.030506518954913325),\n",
       " ('stocks', 0.029581742274224374),\n",
       " ('Reasons', 0.028743866428244537),\n",
       " ('deaths', 0.027860092852483344),\n",
       " ('virus', 0.025716989358801717),\n",
       " ('markets', 0.022864906695504512),\n",
       " ('1500', 0.02231262230616804),\n",
       " ('60', 0.021412463259315447),\n",
       " ('portfolio', 0.016436537908448373),\n",
       " ('back', 0.01619796505973802),\n",
       " ('new', 0.013312404914688995),\n",
       " ('cases', 0.01311502314673957),\n",
       " ('to', -0.01199561987952933)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        True,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True, False, False,  True,  True,  True, False,  True,  True,\n",
       "       False,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True, False,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True, False,  True,  True,  True,  True, False,\n",
       "       False, False,  True, False, False,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True, False, False, False])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False,  True,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False,  True, False,\n",
       "       False, False, False,  True, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False,  True, False,\n",
       "       False,  True,  True, False, False, False,  True, False, False,\n",
       "        True, False,  True, False, False, False, False, False, False,\n",
       "        True, False, False, False, False, False, False,  True, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False,  True, False, False, False,  True, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False, False, False, False,  True,\n",
       "        True,  True, False,  True,  True, False, False,  True, False,\n",
       "       False, False, False, False, False,  True, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False,  True,  True,  True])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_boolF = y_bool==False\n",
    "y_boolF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate by correctness of prediction\n",
    "from itertools import compress\n",
    "correctPred = list(compress(explanations, y_bool))\n",
    "wrongPred = list(compress(explanations, y_boolF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('meth', 0.030987877447655538),\n",
       " ('your', 0.025280368484429324),\n",
       " ('for', 0.023607036528363858),\n",
       " ('This', 0.021277121458002424),\n",
       " ('with', 0.02115291971117502),\n",
       " ('coronavirus', 0.020399463946433768),\n",
       " ('contaminated', 0.020293806094854105),\n",
       " ('Florida', 0.01895198653657757),\n",
       " ('police', 0.01784072679125687),\n",
       " ('Is', 0.017506689596506714),\n",
       " ('will', 0.0156057483026649),\n",
       " ('dept', 0.011637419697194519),\n",
       " ('test', 0.011128987757850867),\n",
       " ('free', 0.005326991149549698),\n",
       " ('it', 0.002893041845161037)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrongPred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1659"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unique words from all explanations\n",
    "with open('tweets_vocab_list', 'rb') as f:\n",
    "    vocab_list = pickle.load(f)\n",
    "\n",
    "len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1530"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique words from explanations for correctly predicted unreliable tweets\n",
    "vocab_correct = []\n",
    "for subList in correctPred:\n",
    "    for el in subList:\n",
    "        if el[0] not in vocab_correct:\n",
    "            vocab_correct.append(el[0])\n",
    "len(vocab_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique words from explanations for incorrectly predicted unreliable tweets\n",
    "vocab_wrong = []\n",
    "for subList in wrongPred:\n",
    "    for el in subList:\n",
    "        if el[0] not in vocab_wrong:\n",
    "            vocab_wrong.append(el[0])\n",
    "len(vocab_wrong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Table 1 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionary\n",
    "t1 = [\n",
    "    'blame',\n",
    "    'accuse',\n",
    "    'refuse',\n",
    "    'catastrophe',\n",
    "    'chaos',\n",
    "    'evil',\n",
    "    'fight',\n",
    "    'danger',\n",
    "    'hysteria',\n",
    "    'panic',\n",
    "    'paranoia',\n",
    "    'laugh',\n",
    "    'stupidity',\n",
    "    'hear',\n",
    "    'see',\n",
    "    'feel',\n",
    "    'suppose',\n",
    "    'perceive',\n",
    "    'look',\n",
    "    'appear',\n",
    "    'suggest',\n",
    "    'believe',\n",
    "    'pretend',\n",
    "    'martial',\n",
    "    'kill',\n",
    "    'die',\n",
    "    'weapon',\n",
    "    'weaponizing',\n",
    "    'ussr',\n",
    "    'japan',\n",
    "    'fukushima',\n",
    "    'chernobyl',\n",
    "    'wuhan',\n",
    "    'china',\n",
    "    'foreigners',\n",
    "    'cats',\n",
    "    'dogs',\n",
    "    'i',\n",
    "    'me',\n",
    "    'mine',\n",
    "    'my',\n",
    "    'you',\n",
    "    'your',\n",
    "    'we',\n",
    "    'our',\n",
    "    'propaganda',\n",
    "    'fake',\n",
    "    'conspiracy',\n",
    "    'claim',\n",
    "    'misleading',\n",
    "    'hoax',\n",
    "    'cure',\n",
    "    'breakthrough',\n",
    "    'bitch',\n",
    "    'wtf',\n",
    "    'dogbreath',\n",
    "    'zombie',\n",
    "    'junkies',\n",
    "    'hell',\n",
    "    'screwed',\n",
    "    'secular',\n",
    "    'bible',\n",
    "    'maga',\n",
    "    'magat',\n",
    "    'genetic',\n",
    "    'hillary',\n",
    "    'chinese',\n",
    "    'fundamentalist',\n",
    "    'market',\n",
    "    'communist',\n",
    "    'nazi',\n",
    "    'stock',\n",
    "    'economy',\n",
    "    'money',\n",
    "    'cost',\n",
    "    'costs',\n",
    "    'election',\n",
    "    'campaign',\n",
    "    'presidential',\n",
    "    'impeachment',\n",
    "    'rallies',\n",
    "    'base',\n",
    "    'trump',\n",
    "    'war',\n",
    "    'iran'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disaggregated by correctness of prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correctly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered vocab based on analysis words\n",
    "filtered_vocab = [el for el in vocab_correct if el.lower() in t1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary from filtered vocab\n",
    "myDict = dict()\n",
    "\n",
    "for i in range(len(filtered_vocab)):\n",
    "    myDict[filtered_vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of class associations of filtered vocab\n",
    "my_matrix = np.zeros((len(correctPred), len(vocab_correct)))\n",
    "\n",
    "for i in range(len(correctPred)):\n",
    "    expl = correctPred[i]\n",
    "    for j in range(len(expl)):\n",
    "        word = expl[j][0]\n",
    "        val = expl[j][1]\n",
    "        \n",
    "        if word in filtered_vocab:\n",
    "            if val > 0:\n",
    "                my_matrix[i, myDict[word]] = 1\n",
    "\n",
    "            if val < 0:\n",
    "                my_matrix[i, myDict[word]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no penalty\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        if el > 0:\n",
    "            rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalNonZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7320261437908496"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty for wrong class association of word\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7320261437908496"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incorrectly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered vocab based on analysis words\n",
    "filtered_vocab = [el for el in vocab_wrong if el.lower() in t1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary from filtered vocab\n",
    "myDict = dict()\n",
    "\n",
    "for i in range(len(filtered_vocab)):\n",
    "    myDict[filtered_vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of class associations of filtered vocab\n",
    "my_matrix = np.zeros((len(wrongPred), len(vocab_wrong)))\n",
    "\n",
    "for i in range(len(wrongPred)):\n",
    "    expl = wrongPred[i]\n",
    "    for j in range(len(expl)):\n",
    "        word = expl[j][0]\n",
    "        val = expl[j][1]\n",
    "        \n",
    "        if word in filtered_vocab:\n",
    "            if val > 0:\n",
    "                my_matrix[i, myDict[word]] = 1\n",
    "\n",
    "            if val < 0:\n",
    "                my_matrix[i, myDict[word]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no penalty\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        if el > 0:\n",
    "            rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6296296296296297"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty for wrong class association of word\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6296296296296297"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered vocab based on analysis words\n",
    "filtered_vocab = [el for el in vocab_list if el.lower() in t1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you',\n",
       " 'we',\n",
       " 'Trump',\n",
       " 'You',\n",
       " 'your',\n",
       " 'look',\n",
       " 'TRUMP',\n",
       " 'We',\n",
       " 'My',\n",
       " 'fight',\n",
       " 'evil',\n",
       " 'Iran',\n",
       " 'chaos',\n",
       " 'refuse',\n",
       " 'me',\n",
       " 'Misleading',\n",
       " 'junkies',\n",
       " 'Zombie',\n",
       " 'Market',\n",
       " 'market',\n",
       " 'stock',\n",
       " 'Wuhan',\n",
       " 'China',\n",
       " 'conspiracy',\n",
       " 'panic',\n",
       " 'Stupidity',\n",
       " 'Hell',\n",
       " 'Paranoia',\n",
       " 'weaponizing',\n",
       " 'hoax',\n",
       " 'campaign',\n",
       " 'my',\n",
       " 'I',\n",
       " 'our',\n",
       " 'rallies',\n",
       " 'base',\n",
       " 'screwed',\n",
       " 'weapon',\n",
       " 'Rallies',\n",
       " 'laugh',\n",
       " 'cure',\n",
       " 'DIE',\n",
       " 'Hoax',\n",
       " 'economy',\n",
       " 'Chinese',\n",
       " 'see',\n",
       " 'pretend',\n",
       " 'suggest',\n",
       " 'cost',\n",
       " 'Communist',\n",
       " 'kill',\n",
       " 'hysteria',\n",
       " 'Fake',\n",
       " 'war',\n",
       " 'impeachment',\n",
       " 'Conspiracy',\n",
       " 'Cure',\n",
       " 'FAKE',\n",
       " 'fake',\n",
       " 'ECONOMY',\n",
       " 'MARKET',\n",
       " 'STOCK',\n",
       " 'money',\n",
       " 'Chernobyl',\n",
       " 'USSR',\n",
       " 'Fukushima',\n",
       " 'Japan',\n",
       " 'KILL',\n",
       " 'MY',\n",
       " 'feel',\n",
       " 'believe',\n",
       " 'MAGA',\n",
       " 'Propaganda']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary from filtered vocab\n",
    "myDict = dict()\n",
    "\n",
    "for i in range(len(filtered_vocab)):\n",
    "    myDict[filtered_vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of class associations of filtered vocab\n",
    "my_matrix = np.zeros((len(explanations), len(vocab_list)))\n",
    "\n",
    "for i in range(len(explanations)):\n",
    "    expl = explanations[i]\n",
    "    for j in range(len(expl)):\n",
    "        word = expl[j][0]\n",
    "        val = expl[j][1]\n",
    "        \n",
    "        if word in filtered_vocab:\n",
    "            if val > 0:\n",
    "                my_matrix[i, myDict[word]] = 1\n",
    "\n",
    "            if val < 0:\n",
    "                my_matrix[i, myDict[word]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no penalty\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        if el > 0:\n",
    "            rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalNonZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7166666666666667"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty for wrong class association of word\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7166666666666667"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2:  Table 1 + manual additions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionary\n",
    "t1PlusManual = [\n",
    "    'blame',\n",
    "    'accuse',\n",
    "    'refuse',\n",
    "    'catastrophe',\n",
    "    'emergency',\n",
    "    'chaos',\n",
    "    'crisis',\n",
    "    'evil',\n",
    "    'fight',\n",
    "    'danger',\n",
    "    'hysteria',\n",
    "    'panic',\n",
    "    'paranoia',\n",
    "    'fear',\n",
    "    'fears',\n",
    "    'laugh',\n",
    "    'stupidity',\n",
    "    'hear',\n",
    "    'see',\n",
    "    'feel',\n",
    "    'suppose',\n",
    "    'perceive',\n",
    "    'look',\n",
    "    'appear',\n",
    "    'suggest',\n",
    "    'believe',\n",
    "    'believed',\n",
    "    'pretend',\n",
    "    'martial',\n",
    "    'kill',\n",
    "    'killing',\n",
    "    'kills',\n",
    "    'killed',\n",
    "    'die',\n",
    "    'death',\n",
    "    'dies',\n",
    "    'dying',\n",
    "    'dead',\n",
    "    'died',\n",
    "    'threat',\n",
    "    'weapon',\n",
    "    'weaponize',\n",
    "    'weaponizing',\n",
    "    'knife',\n",
    "    'ussr',\n",
    "    'japan',\n",
    "    'chernobyl',\n",
    "    'wuhan',\n",
    "    'china',\n",
    "    'foreigners',\n",
    "    'cat',\n",
    "    'cats',\n",
    "    'dog',\n",
    "    'dogs',\n",
    "    'i',\n",
    "    'me',\n",
    "    'mine',\n",
    "    'my',\n",
    "    'you',\n",
    "    'yours',\n",
    "    'your',\n",
    "    'we',\n",
    "    'our',\n",
    "    'propaganda',\n",
    "    'fake',\n",
    "    'conspiracy',\n",
    "    'claim',\n",
    "    'claims',\n",
    "    'claiming',\n",
    "    'claimed',\n",
    "    'misleading',\n",
    "    'hoax',\n",
    "    'cure',\n",
    "    'breakthrough',\n",
    "    'bitch',\n",
    "    'wtf',\n",
    "    'dogbreath',\n",
    "    'zombie',\n",
    "    'junkies',\n",
    "    'hell',\n",
    "    'screwed',\n",
    "    'fuck',\n",
    "    'fucking',\n",
    "    'fucked',\n",
    "    'fuckin',\n",
    "    'wth',\n",
    "    'secular',\n",
    "    'bible',\n",
    "    'maga',\n",
    "    'magat',\n",
    "    'genetic',\n",
    "    'hillary',\n",
    "    'clinton',\n",
    "    'fundamentalist',\n",
    "    'market',\n",
    "    'communist',\n",
    "    'nazi',\n",
    "    'stock',\n",
    "    'bank',\n",
    "    'economy',\n",
    "    'economic',\n",
    "    'money',\n",
    "    'cost',\n",
    "    'costs',\n",
    "    'election',\n",
    "    'campaign',\n",
    "    'presidential',\n",
    "    'impeachment',\n",
    "    'rally',\n",
    "    'rallies',\n",
    "    'base',\n",
    "    'president',\n",
    "    'trump',\n",
    "    'war',\n",
    "    'wwiii',\n",
    "    'asteroid',\n",
    "    'banknotes',\n",
    "    'dangerous',\n",
    "    'invent',\n",
    "    'invented',\n",
    "    'iran',\n",
    "    'lie',\n",
    "    'lies',\n",
    "    'lying',\n",
    "    'lied',\n",
    "    'liar',\n",
    "    'liars',\n",
    "    'lmfao',\n",
    "    'lmfaoooooo',\n",
    "    'misinformation',\n",
    "    'news',\n",
    "    'media',\n",
    "    'financial',\n",
    "    'propagandawars',\n",
    "    'antidote'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disaggregated by correctness of prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correctly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered vocab based on analysis words\n",
    "filtered_vocab = [el for el in vocab_correct if el.lower() in t1PlusManual]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary from filtered vocab\n",
    "myDict = dict()\n",
    "\n",
    "for i in range(len(filtered_vocab)):\n",
    "    myDict[filtered_vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of class associations of filtered vocab\n",
    "my_matrix = np.zeros((len(correctPred), len(vocab_correct)))\n",
    "\n",
    "for i in range(len(correctPred)):\n",
    "    expl = correctPred[i]\n",
    "    for j in range(len(expl)):\n",
    "        word = expl[j][0]\n",
    "        val = expl[j][1]\n",
    "        \n",
    "        if word in filtered_vocab:\n",
    "            if val > 0:\n",
    "                my_matrix[i, myDict[word]] = 1\n",
    "\n",
    "            if val < 0:\n",
    "                my_matrix[i, myDict[word]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no penalty\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        if el > 0:\n",
    "            rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalNonZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8235294117647058"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty for wrong class association of word\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8235294117647058"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incorrectly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered vocab based on analysis words\n",
    "filtered_vocab = [el for el in vocab_wrong if el.lower() in t1PlusManual]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary from filtered vocab\n",
    "myDict = dict()\n",
    "\n",
    "for i in range(len(filtered_vocab)):\n",
    "    myDict[filtered_vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of class associations of filtered vocab\n",
    "my_matrix = np.zeros((len(wrongPred), len(vocab_wrong)))\n",
    "\n",
    "for i in range(len(wrongPred)):\n",
    "    expl = wrongPred[i]\n",
    "    for j in range(len(expl)):\n",
    "        word = expl[j][0]\n",
    "        val = expl[j][1]\n",
    "        \n",
    "        if word in filtered_vocab:\n",
    "            if val > 0:\n",
    "                my_matrix[i, myDict[word]] = 1\n",
    "\n",
    "            if val < 0:\n",
    "                my_matrix[i, myDict[word]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no penalty\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        if el > 0:\n",
    "            rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7777777777777778"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty for wrong class association of word\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7777777777777778"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered vocab based on analysis words\n",
    "filtered_vocab = [el for el in vocab_list if el.lower() in t1PlusManual]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you',\n",
       " 'we',\n",
       " 'President',\n",
       " 'Trump',\n",
       " 'You',\n",
       " 'your',\n",
       " 'look',\n",
       " 'media',\n",
       " 'TRUMP',\n",
       " 'We',\n",
       " 'WWIII',\n",
       " 'dying',\n",
       " 'asteroid',\n",
       " 'BANKNOTES',\n",
       " 'dangerous',\n",
       " 'economic',\n",
       " 'My',\n",
       " 'believed',\n",
       " 'knife',\n",
       " 'fight',\n",
       " 'NEWS',\n",
       " 'EMERGENCY',\n",
       " 'evil',\n",
       " 'invented',\n",
       " 'Iran',\n",
       " 'chaos',\n",
       " 'refuse',\n",
       " 'lying',\n",
       " 'me',\n",
       " 'Misleading',\n",
       " 'junkies',\n",
       " 'Zombie',\n",
       " 'Market',\n",
       " 'market',\n",
       " 'stock',\n",
       " 'Wuhan',\n",
       " 'claims',\n",
       " 'China',\n",
       " 'conspiracy',\n",
       " 'killing',\n",
       " 'LMFAOOOOOO',\n",
       " 'lied',\n",
       " 'threat',\n",
       " 'crisis',\n",
       " 'Bank',\n",
       " 'panic',\n",
       " 'Stupidity',\n",
       " 'Hell',\n",
       " 'Death',\n",
       " 'died',\n",
       " 'Claims',\n",
       " 'Fears',\n",
       " 'Paranoia',\n",
       " 'weaponizing',\n",
       " 'hoax',\n",
       " 'claiming',\n",
       " 'Fear',\n",
       " 'campaign',\n",
       " 'Kills',\n",
       " 'my',\n",
       " 'death',\n",
       " 'I',\n",
       " 'fucked',\n",
       " 'our',\n",
       " 'rallies',\n",
       " 'base',\n",
       " 'screwed',\n",
       " 'emergency',\n",
       " 'weapon',\n",
       " 'Rallies',\n",
       " 'Clinton',\n",
       " 'DEAD',\n",
       " 'laugh',\n",
       " 'cure',\n",
       " 'DIE',\n",
       " 'lie',\n",
       " 'Hoax',\n",
       " 'lies',\n",
       " 'economy',\n",
       " 'dog',\n",
       " 'see',\n",
       " 'news',\n",
       " 'pretend',\n",
       " 'misinformation',\n",
       " 'suggest',\n",
       " 'cost',\n",
       " 'Communist',\n",
       " 'fuckin',\n",
       " 'kill',\n",
       " 'fear',\n",
       " 'financial',\n",
       " 'fucking',\n",
       " 'hysteria',\n",
       " 'kills',\n",
       " 'killed',\n",
       " 'Fake',\n",
       " 'war',\n",
       " 'impeachment',\n",
       " 'Killed',\n",
       " 'fears',\n",
       " 'Conspiracy',\n",
       " 'Cure',\n",
       " 'FAKE',\n",
       " 'fake',\n",
       " 'News',\n",
       " 'ECONOMY',\n",
       " 'MARKET',\n",
       " 'STOCK',\n",
       " 'money',\n",
       " 'LYING',\n",
       " 'LIES',\n",
       " 'Chernobyl',\n",
       " 'USSR',\n",
       " 'Japan',\n",
       " 'KILL',\n",
       " 'MY',\n",
       " 'DYING',\n",
       " 'feel',\n",
       " 'believe',\n",
       " 'Lies',\n",
       " 'MAGA',\n",
       " 'Propaganda',\n",
       " 'Media',\n",
       " 'antidote',\n",
       " 'PropagandaWars']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary from filtered vocab\n",
    "myDict = dict()\n",
    "\n",
    "for i in range(len(filtered_vocab)):\n",
    "    myDict[filtered_vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of class associations of filtered vocab\n",
    "my_matrix = np.zeros((len(explanations), len(vocab_list)))\n",
    "\n",
    "for i in range(len(explanations)):\n",
    "    expl = explanations[i]\n",
    "    for j in range(len(expl)):\n",
    "        word = expl[j][0]\n",
    "        val = expl[j][1]\n",
    "        \n",
    "        if word in filtered_vocab:\n",
    "            if val > 0:\n",
    "                my_matrix[i, myDict[word]] = 1\n",
    "\n",
    "            if val < 0:\n",
    "                my_matrix[i, myDict[word]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no penalty\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        if el > 0:\n",
    "            rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "341"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalNonZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8166666666666667"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty for wrong class association of word\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8166666666666667"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Table 1 words + stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionary\n",
    "stems = [stemmer.stem(word) for word in t1]\n",
    "stems = list(set(stems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['antidot',\n",
       " 'catastroph',\n",
       " 'financi',\n",
       " 'appear',\n",
       " 'mine',\n",
       " 'fuck',\n",
       " 'wwiii',\n",
       " 'look',\n",
       " 'wuhan',\n",
       " 'elect',\n",
       " 'our',\n",
       " 'liar',\n",
       " 'china',\n",
       " 'conspiraci',\n",
       " 'bitch',\n",
       " 'refus',\n",
       " 'campaign',\n",
       " 'clinton',\n",
       " 'suppos',\n",
       " 'perceiv',\n",
       " 'fuckin',\n",
       " 'die',\n",
       " 'feel',\n",
       " 'believ',\n",
       " 'i',\n",
       " 'fear',\n",
       " 'hoax',\n",
       " 'evil',\n",
       " 'cat',\n",
       " 'accus',\n",
       " 'ralli',\n",
       " 'iran',\n",
       " 'magat',\n",
       " 'dog',\n",
       " 'death',\n",
       " 'your',\n",
       " 'threat',\n",
       " 'propagandawar',\n",
       " 'ussr',\n",
       " 'market',\n",
       " 'my',\n",
       " 'fake',\n",
       " 'me',\n",
       " 'hell',\n",
       " 'secular',\n",
       " 'wth',\n",
       " 'cure',\n",
       " 'wtf',\n",
       " 'blame',\n",
       " 'pretend',\n",
       " 'nazi',\n",
       " 'emerg',\n",
       " 'banknot',\n",
       " 'lmfao',\n",
       " 'genet',\n",
       " 'we',\n",
       " 'chernobyl',\n",
       " 'news',\n",
       " 'econom',\n",
       " 'impeach',\n",
       " 'you',\n",
       " 'misinform',\n",
       " 'breakthrough',\n",
       " 'trump',\n",
       " 'suggest',\n",
       " 'zombi',\n",
       " 'chao',\n",
       " 'presidenti',\n",
       " 'kill',\n",
       " 'hysteria',\n",
       " 'danger',\n",
       " 'hear',\n",
       " 'asteroid',\n",
       " 'base',\n",
       " 'crisi',\n",
       " 'see',\n",
       " 'bank',\n",
       " 'mislead',\n",
       " 'fight',\n",
       " 'war',\n",
       " 'money',\n",
       " 'screw',\n",
       " 'communist',\n",
       " 'hillari',\n",
       " 'martial',\n",
       " 'maga',\n",
       " 'cost',\n",
       " 'lie',\n",
       " 'panic',\n",
       " 'paranoia',\n",
       " 'dead',\n",
       " 'propaganda',\n",
       " 'junki',\n",
       " 'economi',\n",
       " 'laugh',\n",
       " 'presid',\n",
       " 'media',\n",
       " 'claim',\n",
       " 'stock',\n",
       " 'lmfaoooooo',\n",
       " 'dogbreath',\n",
       " 'bibl',\n",
       " 'japan',\n",
       " 'fundamentalist',\n",
       " 'foreign',\n",
       " 'weapon',\n",
       " 'knife',\n",
       " 'invent',\n",
       " 'stupid']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disaggregated by correctness of prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correctly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered vocab based on analysis words\n",
    "filtered_vocab = [el for el in vocab_correct if stemmer.stem(el).lower() in stems]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary from filtered vocab\n",
    "myDict = dict()\n",
    "\n",
    "for i in range(len(filtered_vocab)):\n",
    "    myDict[filtered_vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of class associations of filtered vocab\n",
    "my_matrix = np.zeros((len(correctPred), len(vocab_correct)))\n",
    "\n",
    "for i in range(len(correctPred)):\n",
    "    expl = correctPred[i]\n",
    "    for j in range(len(expl)):\n",
    "        word = expl[j][0]\n",
    "        val = expl[j][1]\n",
    "        \n",
    "        if word in filtered_vocab:\n",
    "            if val > 0:\n",
    "                my_matrix[i, myDict[word]] = 1\n",
    "\n",
    "            if val < 0:\n",
    "                my_matrix[i, myDict[word]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no penalty\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        if el > 0:\n",
    "            rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalNonZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8562091503267973"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty for wrong class association of word\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8562091503267973"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incorrectly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered vocab based on analysis words\n",
    "filtered_vocab = [el for el in vocab_wrong if stemmer.stem(el).lower() in stems]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary from filtered vocab\n",
    "myDict = dict()\n",
    "\n",
    "for i in range(len(filtered_vocab)):\n",
    "    myDict[filtered_vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of class associations of filtered vocab\n",
    "my_matrix = np.zeros((len(wrongPred), len(vocab_wrong)))\n",
    "\n",
    "for i in range(len(wrongPred)):\n",
    "    expl = wrongPred[i]\n",
    "    for j in range(len(expl)):\n",
    "        word = expl[j][0]\n",
    "        val = expl[j][1]\n",
    "        \n",
    "        if word in filtered_vocab:\n",
    "            if val > 0:\n",
    "                my_matrix[i, myDict[word]] = 1\n",
    "\n",
    "            if val < 0:\n",
    "                my_matrix[i, myDict[word]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no penalty\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        if el > 0:\n",
    "            rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8148148148148148"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty for wrong class association of word\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8148148148148148"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered vocab based on analysis words\n",
    "filtered_vocab = [el for el in vocab_list if stemmer.stem(el).lower() in stems]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stocks',\n",
       " 'deaths',\n",
       " 'markets',\n",
       " 'you',\n",
       " 'we',\n",
       " 'President',\n",
       " 'Trump',\n",
       " 'You',\n",
       " 'your',\n",
       " 'look',\n",
       " 'media',\n",
       " 'TRUMP',\n",
       " 'We',\n",
       " 'WWIII',\n",
       " 'dying',\n",
       " 'asteroid',\n",
       " 'BANKNOTES',\n",
       " 'dangerous',\n",
       " 'economic',\n",
       " 'suggests',\n",
       " 'My',\n",
       " 'believed',\n",
       " 'refused',\n",
       " 'knife',\n",
       " 'fight',\n",
       " 'EMERGENCY',\n",
       " 'evil',\n",
       " 'invented',\n",
       " 'Iran',\n",
       " 'chaos',\n",
       " 'refuse',\n",
       " 'lying',\n",
       " 'me',\n",
       " 'Misleading',\n",
       " 'junkies',\n",
       " 'Zombie',\n",
       " 'Fundamentalists',\n",
       " 'Market',\n",
       " 'market',\n",
       " 'stock',\n",
       " 'Wuhan',\n",
       " 'claims',\n",
       " 'China',\n",
       " 'conspiracy',\n",
       " 'killing',\n",
       " 'LMFAOOOOOO',\n",
       " 'lied',\n",
       " 'threat',\n",
       " 'crisis',\n",
       " 'Bank',\n",
       " 'panic',\n",
       " 'Stupidity',\n",
       " 'Hell',\n",
       " 'Death',\n",
       " 'died',\n",
       " 'Impeach',\n",
       " 'Claims',\n",
       " 'Fears',\n",
       " 'Paranoia',\n",
       " 'weaponizing',\n",
       " 'hoax',\n",
       " 'claiming',\n",
       " 'Fear',\n",
       " 'campaign',\n",
       " 'Kills',\n",
       " 'my',\n",
       " 'death',\n",
       " 'I',\n",
       " 'fucked',\n",
       " 'refuses',\n",
       " 'our',\n",
       " 'rallies',\n",
       " 'base',\n",
       " 'screwed',\n",
       " 'emergency',\n",
       " 'weapon',\n",
       " 'Rallies',\n",
       " 'Clinton',\n",
       " 'DEAD',\n",
       " 'laugh',\n",
       " 'cure',\n",
       " 'DIE',\n",
       " 'lie',\n",
       " 'Hoax',\n",
       " 'lies',\n",
       " 'economy',\n",
       " 'dog',\n",
       " 'see',\n",
       " 'news',\n",
       " 'pretend',\n",
       " 'foreign',\n",
       " 'misinformation',\n",
       " 'suggest',\n",
       " 'cost',\n",
       " 'looks',\n",
       " 'Communist',\n",
       " 'fuckin',\n",
       " 'kill',\n",
       " 'fear',\n",
       " 'financial',\n",
       " 'Genetically',\n",
       " 'fucking',\n",
       " 'hysteria',\n",
       " 'seeing',\n",
       " 'kills',\n",
       " 'killed',\n",
       " 'Fake',\n",
       " 'Nazis',\n",
       " 'war',\n",
       " 'impeachment',\n",
       " 'Killed',\n",
       " 'fears',\n",
       " 'conspiracies',\n",
       " 'Conspiracy',\n",
       " 'Cure',\n",
       " 'FAKE',\n",
       " 'fake',\n",
       " 'ECONOMY',\n",
       " 'MARKET',\n",
       " 'STOCK',\n",
       " 'elections',\n",
       " 'Appearing',\n",
       " 'money',\n",
       " 'LIES',\n",
       " 'Chernobyl',\n",
       " 'USSR',\n",
       " 'Japan',\n",
       " 'fearful',\n",
       " 'KILL',\n",
       " 'MY',\n",
       " 'feel',\n",
       " 'fighting',\n",
       " 'believe',\n",
       " 'Lies',\n",
       " 'MAGA',\n",
       " 'Propaganda',\n",
       " 'Media',\n",
       " 'stupid',\n",
       " 'antidote',\n",
       " 'PropagandaWars']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary from filtered vocab\n",
    "myDict = dict()\n",
    "\n",
    "for i in range(len(filtered_vocab)):\n",
    "    myDict[filtered_vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of class associations of filtered vocab\n",
    "my_matrix = np.zeros((len(explanations), len(vocab_list)))\n",
    "\n",
    "for i in range(len(explanations)):\n",
    "    expl = explanations[i]\n",
    "    for j in range(len(expl)):\n",
    "        word = expl[j][0]\n",
    "        val = expl[j][1]\n",
    "        \n",
    "        if word in filtered_vocab:\n",
    "            if val > 0:\n",
    "                my_matrix[i, myDict[word]] = 1\n",
    "\n",
    "            if val < 0:\n",
    "                my_matrix[i, myDict[word]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no penalty\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        if el > 0:\n",
    "            rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalNonZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty for wrong class association of word\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
