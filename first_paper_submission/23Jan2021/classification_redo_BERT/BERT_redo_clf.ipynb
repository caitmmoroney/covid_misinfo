{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification with Tweet Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Is_Unreliable</th>\n",
       "      <th>Category</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 3, 6, 9</td>\n",
       "      <td>We are living in scary times in Canada. Gov’t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 6, 8, 9</td>\n",
       "      <td>Just as bad in Canada. In fact, our government...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 4, 9</td>\n",
       "      <td>It was only a matter of time before the mainst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6, 8</td>\n",
       "      <td>Russia's taking no chances: Foreigners infecte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>6, 8, 9</td>\n",
       "      <td>Although there is now a presumptive confirmed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BREAKING: Harvard classes will move online sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Singularity University is hosting a FREE Virtu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coronavirus: how does it spread and what are t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stanford just cancelled classes for the rest o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tech conferences were cancelled in #Waterloo R...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Is_Unreliable    Category  \\\n",
       "0                1  1, 3, 6, 9   \n",
       "1                1  1, 6, 8, 9   \n",
       "2                1     1, 4, 9   \n",
       "3                1        6, 8   \n",
       "4                1     6, 8, 9   \n",
       "..             ...         ...   \n",
       "555              0         NaN   \n",
       "556              0         NaN   \n",
       "557              0         NaN   \n",
       "558              0         NaN   \n",
       "559              0         NaN   \n",
       "\n",
       "                                                 Tweet  \n",
       "0    We are living in scary times in Canada. Gov’t ...  \n",
       "1    Just as bad in Canada. In fact, our government...  \n",
       "2    It was only a matter of time before the mainst...  \n",
       "3    Russia's taking no chances: Foreigners infecte...  \n",
       "4    Although there is now a presumptive confirmed ...  \n",
       "..                                                 ...  \n",
       "555  BREAKING: Harvard classes will move online sta...  \n",
       "556  Singularity University is hosting a FREE Virtu...  \n",
       "557  Coronavirus: how does it spread and what are t...  \n",
       "558  Stanford just cancelled classes for the rest o...  \n",
       "559  Tech conferences were cancelled in #Waterloo R...  \n",
       "\n",
       "[560 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tweets\n",
    "tweets = pd.read_excel('COVID19_Dataset-CM-ZB-complete with sources.xlsx')\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tweet embeddings\n",
    "fname = 'tweet_embed_{}.npy'\n",
    "fname_A = fname.format('A')\n",
    "tweet_embeddings_A = np.load(fname_A)\n",
    "tweet_embeddings_BERT = np.load('bert_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of embeddings to iterate over\n",
    "#embeddings = [tweet_embeddings_A, tweet_embeddings_S, tweet_embeddings_BERT]\n",
    "\n",
    "# target y\n",
    "target = np.array(tweets['Is_Unreliable'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification: five-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC hyperparams to optimize\n",
    "kernel = ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "C = [0.001, 0.01, 0.1, 1, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tune_num = int(tweets.shape[0]/5)\n",
    "tune_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the folds\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits = num_folds, shuffle = True, random_state = 1)\n",
    "splits = kf.split(tweet_embeddings_BERT) # use any set of embeddings to get train/test indices splits\n",
    "\n",
    "training_sets = []\n",
    "testing_sets = []\n",
    "for train_idx, test_idx in splits:\n",
    "    training_sets.append(train_idx)\n",
    "    testing_sets.append(test_idx)\n",
    "\n",
    "# Construct tuning sets from training sets (20% of data ~ 1 fold) &\n",
    "# write over training sets (60% of data ~ 3 folds)\n",
    "tuning_sets = []\n",
    "for i in range(len(training_sets)):\n",
    "    train_set = training_sets[i]\n",
    "    np.random.seed(i)\n",
    "    tune_idx = np.random.choice(train_set,\n",
    "                                size = tune_num,\n",
    "                                replace = False)\n",
    "    tuning_sets.append(tune_idx)\n",
    "    new_train_set = train_set[~np.in1d(train_set, tune_idx)]\n",
    "    training_sets[i] = new_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create results dictionary\n",
    "#\n",
    "# Inputs: numpy array\n",
    "# Outputs: dictionary containing model performance stats\n",
    "\n",
    "def get_results(np_array):\n",
    "    # Initialize list/dict to store all model stats\n",
    "    performance = dict()\n",
    "\n",
    "    # Loop over folds\n",
    "    for i in range(num_folds):\n",
    "        key1 = 'Fold {}'.format(i+1) # key for the performance dict\n",
    "\n",
    "        train_idx = training_sets[i]\n",
    "        test_idx = testing_sets[i]\n",
    "        tune_idx = tuning_sets[i]\n",
    "\n",
    "        y_train = target[train_idx]\n",
    "        y_test = target[test_idx]\n",
    "        y_tune = target[tune_idx]\n",
    "\n",
    "        X_train = np_array[train_idx]\n",
    "        X_test = np_array[test_idx]\n",
    "        X_tune = np_array[tune_idx]\n",
    "\n",
    "        # Training & tuning\n",
    "        models = [] # store list of models in order to retrieve optimal model\n",
    "        tune_auc = [] # tune based on AUC\n",
    "        model_dict = dict() # to store model params & performance metric values\n",
    "\n",
    "        for ker in kernel:\n",
    "            for el in C:\n",
    "                # Training\n",
    "                svc = SVC(C = el, kernel = ker, probability = True)\n",
    "                svc.fit(X_train, y_train)\n",
    "                models.append(svc)\n",
    "\n",
    "                # Tuning\n",
    "                tune_predict_proba = svc.predict_proba(X_tune)[:,1] # check on this subscripting\n",
    "                auc = roc_auc_score(y_tune, tune_predict_proba)\n",
    "                tune_auc.append(auc)\n",
    "\n",
    "        # Get optimal model based on hyperparameter tuning\n",
    "        opt_model = models[tune_auc.index(max(tune_auc))] # tune based on AUC\n",
    "        opt_model_params = opt_model.get_params()\n",
    "        model_dict['params'] = opt_model_params # store optimal values for model hyperparameters\n",
    "\n",
    "        # Save training scores\n",
    "        train_scores = dict() # to store all training scores\n",
    "        train_predict = opt_model.predict(X_train)\n",
    "        train_predict_proba = opt_model.predict_proba(X_train)[:,1] # check on this subscripting\n",
    "        train_scores['auc'] = roc_auc_score(y_train, train_predict_proba)\n",
    "        train_scores['accuracy'] = accuracy_score(y_train, train_predict)\n",
    "        train_scores['recall_macro'] = recall_score(y_train, train_predict, average = 'macro')\n",
    "        train_scores['precision_macro'] = precision_score(y_train, train_predict, average = 'macro')\n",
    "        train_scores['f1_macro'] = f1_score(y_train, train_predict, average = 'macro')\n",
    "\n",
    "        # Save training scores dictionary to model dictionary\n",
    "        model_dict['training'] = train_scores\n",
    "\n",
    "        # Save tuning scores\n",
    "        tune_scores = dict() # to store all tuning scores\n",
    "        tune_predict = opt_model.predict(X_tune)\n",
    "        tune_predict_proba = opt_model.predict_proba(X_tune)[:,1] # check on this subscripting\n",
    "        tune_scores['auc'] = roc_auc_score(y_tune, tune_predict_proba)\n",
    "        tune_scores['accuracy'] = accuracy_score(y_tune, tune_predict)\n",
    "        tune_scores['recall_macro'] = recall_score(y_tune, tune_predict, average = 'macro')\n",
    "        tune_scores['precision_macro'] = precision_score(y_tune, tune_predict, average = 'macro')\n",
    "        tune_scores['f1_macro'] = f1_score(y_tune, tune_predict, average = 'macro')\n",
    "\n",
    "        # Save tuning scores dictionary to model dictionary\n",
    "        model_dict['tuning'] = tune_scores\n",
    "\n",
    "        # Testing\n",
    "        test_scores = dict() # to store all testing scores\n",
    "        test_predict = opt_model.predict(X_test)\n",
    "        test_predict_proba = opt_model.predict_proba(X_test)[:,1]\n",
    "        test_scores['auc'] = roc_auc_score(y_test, test_predict_proba)\n",
    "        test_scores['accuracy'] = accuracy_score(y_test, test_predict)\n",
    "        test_scores['recall_macro'] = recall_score(y_test, test_predict, average = 'macro')\n",
    "        test_scores['precision_macro'] = precision_score(y_test, test_predict, average = 'macro')\n",
    "        test_scores['f1_macro'] = f1_score(y_test, test_predict, average = 'macro')\n",
    "\n",
    "        # Save test scores dictionary to model dictionary\n",
    "        model_dict['testing'] = test_scores\n",
    "\n",
    "        # Save model dictionary to overall dictionary\n",
    "        performance[key1] = model_dict\n",
    "    \n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get results for BERT embeddings\n",
    "BERT_results = get_results(tweet_embeddings_BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get results for A embeddings\n",
    "A_results = get_results(tweet_embeddings_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fold 1': {'params': {'C': 1,\n",
       "   'break_ties': False,\n",
       "   'cache_size': 200,\n",
       "   'class_weight': None,\n",
       "   'coef0': 0.0,\n",
       "   'decision_function_shape': 'ovr',\n",
       "   'degree': 3,\n",
       "   'gamma': 'scale',\n",
       "   'kernel': 'rbf',\n",
       "   'max_iter': -1,\n",
       "   'probability': True,\n",
       "   'random_state': None,\n",
       "   'shrinking': True,\n",
       "   'tol': 0.001,\n",
       "   'verbose': False},\n",
       "  'training': {'auc': 0.9983338060124787,\n",
       "   'accuracy': 0.9821428571428571,\n",
       "   'recall_macro': 0.9824163357912649,\n",
       "   'precision_macro': 0.9821428571428572,\n",
       "   'f1_macro': 0.982140326009922},\n",
       "  'tuning': {'auc': 0.9115384615384614,\n",
       "   'accuracy': 0.8392857142857143,\n",
       "   'recall_macro': 0.8294871794871794,\n",
       "   'precision_macro': 0.8655761024182076,\n",
       "   'f1_macro': 0.8328358208955223},\n",
       "  'testing': {'auc': 0.8998724489795918,\n",
       "   'accuracy': 0.8303571428571429,\n",
       "   'recall_macro': 0.8303571428571428,\n",
       "   'precision_macro': 0.833011893281903,\n",
       "   'f1_macro': 0.830018372074447}},\n",
       " 'Fold 2': {'params': {'C': 1,\n",
       "   'break_ties': False,\n",
       "   'cache_size': 200,\n",
       "   'class_weight': None,\n",
       "   'coef0': 0.0,\n",
       "   'decision_function_shape': 'ovr',\n",
       "   'degree': 3,\n",
       "   'gamma': 'scale',\n",
       "   'kernel': 'linear',\n",
       "   'max_iter': -1,\n",
       "   'probability': True,\n",
       "   'random_state': None,\n",
       "   'shrinking': True,\n",
       "   'tol': 0.001,\n",
       "   'verbose': False},\n",
       "  'training': {'auc': 0.9540188914631973,\n",
       "   'accuracy': 0.875,\n",
       "   'recall_macro': 0.8677597576189627,\n",
       "   'precision_macro': 0.8887088874656218,\n",
       "   'f1_macro': 0.8716857610474633},\n",
       "  'tuning': {'auc': 0.944069431051109,\n",
       "   'accuracy': 0.8303571428571429,\n",
       "   'recall_macro': 0.8410478945676632,\n",
       "   'precision_macro': 0.8494729907773386,\n",
       "   'f1_macro': 0.8300183720744468},\n",
       "  'testing': {'auc': 0.9309895833333334,\n",
       "   'accuracy': 0.8035714285714286,\n",
       "   'recall_macro': 0.828125,\n",
       "   'precision_macro': 0.8428571428571429,\n",
       "   'f1_macro': 0.8030060761112888}},\n",
       " 'Fold 3': {'params': {'C': 10,\n",
       "   'break_ties': False,\n",
       "   'cache_size': 200,\n",
       "   'class_weight': None,\n",
       "   'coef0': 0.0,\n",
       "   'decision_function_shape': 'ovr',\n",
       "   'degree': 3,\n",
       "   'gamma': 'scale',\n",
       "   'kernel': 'linear',\n",
       "   'max_iter': -1,\n",
       "   'probability': True,\n",
       "   'random_state': None,\n",
       "   'shrinking': True,\n",
       "   'tol': 0.001,\n",
       "   'verbose': False},\n",
       "  'training': {'auc': 0.9902551381998582,\n",
       "   'accuracy': 0.9613095238095238,\n",
       "   'recall_macro': 0.9609851169383417,\n",
       "   'precision_macro': 0.9629043162651674,\n",
       "   'f1_macro': 0.9612515191569011},\n",
       "  'tuning': {'auc': 0.9680204669011833,\n",
       "   'accuracy': 0.8928571428571429,\n",
       "   'recall_macro': 0.8954269267668692,\n",
       "   'precision_macro': 0.8954269267668692,\n",
       "   'f1_macro': 0.8928571428571428},\n",
       "  'testing': {'auc': 0.9189792663476875,\n",
       "   'accuracy': 0.8571428571428571,\n",
       "   'recall_macro': 0.8574162679425836,\n",
       "   'precision_macro': 0.8574162679425836,\n",
       "   'f1_macro': 0.8571428571428571}},\n",
       " 'Fold 4': {'params': {'C': 1,\n",
       "   'break_ties': False,\n",
       "   'cache_size': 200,\n",
       "   'class_weight': None,\n",
       "   'coef0': 0.0,\n",
       "   'decision_function_shape': 'ovr',\n",
       "   'degree': 3,\n",
       "   'gamma': 'scale',\n",
       "   'kernel': 'linear',\n",
       "   'max_iter': -1,\n",
       "   'probability': True,\n",
       "   'random_state': None,\n",
       "   'shrinking': True,\n",
       "   'tol': 0.001,\n",
       "   'verbose': False},\n",
       "  'training': {'auc': 0.9616503476656734,\n",
       "   'accuracy': 0.9077380952380952,\n",
       "   'recall_macro': 0.9090038314176245,\n",
       "   'precision_macro': 0.909192546583851,\n",
       "   'f1_macro': 0.907737278001683},\n",
       "  'tuning': {'auc': 0.95242656449553,\n",
       "   'accuracy': 0.875,\n",
       "   'recall_macro': 0.8729246487867177,\n",
       "   'precision_macro': 0.8802083333333333,\n",
       "   'f1_macro': 0.87399549983928},\n",
       "  'testing': {'auc': 0.9259615384615384,\n",
       "   'accuracy': 0.8571428571428571,\n",
       "   'recall_macro': 0.8551282051282051,\n",
       "   'precision_macro': 0.8574193548387097,\n",
       "   'f1_macro': 0.8559948569591771}},\n",
       " 'Fold 5': {'params': {'C': 1,\n",
       "   'break_ties': False,\n",
       "   'cache_size': 200,\n",
       "   'class_weight': None,\n",
       "   'coef0': 0.0,\n",
       "   'decision_function_shape': 'ovr',\n",
       "   'degree': 3,\n",
       "   'gamma': 'scale',\n",
       "   'kernel': 'linear',\n",
       "   'max_iter': -1,\n",
       "   'probability': True,\n",
       "   'random_state': None,\n",
       "   'shrinking': True,\n",
       "   'tol': 0.001,\n",
       "   'verbose': False},\n",
       "  'training': {'auc': 0.970679286908871,\n",
       "   'accuracy': 0.9017857142857143,\n",
       "   'recall_macro': 0.9046898907589938,\n",
       "   'precision_macro': 0.9034695615155386,\n",
       "   'f1_macro': 0.9017639606276191},\n",
       "  'tuning': {'auc': 0.9049479166666667,\n",
       "   'accuracy': 0.8392857142857143,\n",
       "   'recall_macro': 0.8385416666666667,\n",
       "   'precision_macro': 0.8354838709677419,\n",
       "   'f1_macro': 0.8367346938775511},\n",
       "  'testing': {'auc': 0.9309242085065559,\n",
       "   'accuracy': 0.8392857142857143,\n",
       "   'recall_macro': 0.8388231531819635,\n",
       "   'precision_macro': 0.8388231531819635,\n",
       "   'f1_macro': 0.8388231531819635}}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create results df from nested dictionary\n",
    "def create_df(input_dict):\n",
    "    df = pd.DataFrame(input_dict)\n",
    "    df = df.transpose()\n",
    "    \n",
    "    df_params = df['params'].apply(pd.Series)\n",
    "    \n",
    "    df_training = df['training'].apply(pd.Series)\n",
    "    df_training.columns = ['train_' + str(col) for col in df_training.columns]\n",
    "    \n",
    "    df_tuning = df['tuning'].apply(pd.Series)\n",
    "    df_tuning.columns = ['tune_' + str(col) for col in df_tuning.columns]\n",
    "    \n",
    "    df_testing = df['testing'].apply(pd.Series)\n",
    "    df_testing.columns = ['test_' + str(col) for col in df_testing.columns]\n",
    "    \n",
    "    final_df = pd.concat([df_training, df_tuning, df_testing, df_params], axis = 1).reset_index()\n",
    "    final_df = final_df.rename({'index': 'fold_num'}, axis = 1)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get means for test results from dataframe of full results\n",
    "def get_test_means(df):\n",
    "    filter_cols = [col for col in df if col.startswith('test_')]\n",
    "    df_test = df[filter_cols]\n",
    "    df_test_mean = pd.DataFrame(df_test.mean(axis = 0)).transpose()\n",
    "    \n",
    "    return df_test_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save BERT embedding results\n",
    "BERT_full = create_df(BERT_results)\n",
    "BERT_full.to_csv('BERT_svm_full_results.csv')\n",
    "\n",
    "BERT_test_mean = get_test_means(BERT_full)\n",
    "BERT_test_mean.to_csv('BERT_svm_testmean_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save A embedding results\n",
    "A_full = create_df(A_results)\n",
    "A_full.to_csv('A_svm_full_results.csv')\n",
    "\n",
    "A_test_mean = get_test_means(A_full)\n",
    "A_test_mean.to_csv('A_svm_testmean_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
