{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification with Tweet Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../COVID19_Dataset-CM-ZB-complete with sources.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-11ff72993c7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load tweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../COVID19_Dataset-CM-ZB-complete with sources.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, io, engine)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstringify_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Install xlrd >= 1.0.0 for Excel support\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"xlrd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36mload_workbook\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# a ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../COVID19_Dataset-CM-ZB-complete with sources.xlsx'"
     ]
    }
   ],
   "source": [
    "# Load tweets\n",
    "tweets = pd.read_excel('COVID19_Dataset-CM-ZB-complete with sources.xlsx')\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tweet embeddings\n",
    "fname = 'tweet_embed_{}.npy'\n",
    "fname_A = fname.format('A')\n",
    "tweet_embeddings_A = np.load(fname_A)\n",
    "tweet_embeddings_BERT = np.load('bert_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of embeddings to iterate over\n",
    "#embeddings = [tweet_embeddings_A, tweet_embeddings_S, tweet_embeddings_BERT]\n",
    "\n",
    "# target y\n",
    "target = np.array(tweets['Is_Unreliable'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification: five-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC hyperparams to optimize\n",
    "kernel = ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "C = [0.001, 0.01, 0.1, 1, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_num = int(tweets.shape[0]/5)\n",
    "tune_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the folds\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits = num_folds, shuffle = True, random_state = 1)\n",
    "splits = kf.split(tweet_embeddings_BERT) # use any set of embeddings to get train/test indices splits\n",
    "\n",
    "training_sets = []\n",
    "testing_sets = []\n",
    "for train_idx, test_idx in splits:\n",
    "    training_sets.append(train_idx)\n",
    "    testing_sets.append(test_idx)\n",
    "\n",
    "# Construct tuning sets from training sets (20% of data ~ 1 fold) &\n",
    "# write over training sets (60% of data ~ 3 folds)\n",
    "tuning_sets = []\n",
    "for i in range(len(training_sets)):\n",
    "    train_set = training_sets[i]\n",
    "    np.random.seed(i)\n",
    "    tune_idx = np.random.choice(train_set,\n",
    "                                size = tune_num,\n",
    "                                replace = False)\n",
    "    tuning_sets.append(tune_idx)\n",
    "    new_train_set = train_set[~np.in1d(train_set, tune_idx)]\n",
    "    training_sets[i] = new_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create results dictionary\n",
    "#\n",
    "# Inputs: numpy array\n",
    "# Outputs: dictionary containing model performance stats\n",
    "\n",
    "def get_results(np_array):\n",
    "    # Initialize list/dict to store all model stats\n",
    "    performance = dict()\n",
    "\n",
    "    # Loop over folds\n",
    "    for i in range(num_folds):\n",
    "        key1 = 'Fold {}'.format(i+1) # key for the performance dict\n",
    "\n",
    "        train_idx = training_sets[i]\n",
    "        test_idx = testing_sets[i]\n",
    "        tune_idx = tuning_sets[i]\n",
    "\n",
    "        y_train = target[train_idx]\n",
    "        y_test = target[test_idx]\n",
    "        y_tune = target[tune_idx]\n",
    "\n",
    "        X_train = np_array[train_idx]\n",
    "        X_test = np_array[test_idx]\n",
    "        X_tune = np_array[tune_idx]\n",
    "\n",
    "        # Training & tuning\n",
    "        models = [] # store list of models in order to retrieve optimal model\n",
    "        tune_auc = [] # tune based on AUC\n",
    "        model_dict = dict() # to store model params & performance metric values\n",
    "\n",
    "        for ker in kernel:\n",
    "            for el in C:\n",
    "                # Training\n",
    "                svc = SVC(C = el, kernel = ker, probability = True)\n",
    "                svc.fit(X_train, y_train)\n",
    "                models.append(svc)\n",
    "\n",
    "                # Tuning\n",
    "                tune_predict_proba = svc.predict_proba(X_tune)[:,1] # check on this subscripting\n",
    "                auc = roc_auc_score(y_tune, tune_predict_proba)\n",
    "                tune_auc.append(auc)\n",
    "\n",
    "        # Get optimal model based on hyperparameter tuning\n",
    "        opt_model = models[tune_auc.index(max(tune_auc))] # tune based on AUC\n",
    "        opt_model_params = opt_model.get_params()\n",
    "        model_dict['params'] = opt_model_params # store optimal values for model hyperparameters\n",
    "\n",
    "        # Save training scores\n",
    "        train_scores = dict() # to store all training scores\n",
    "        train_predict = opt_model.predict(X_train)\n",
    "        train_predict_proba = opt_model.predict_proba(X_train)[:,1] # check on this subscripting\n",
    "        train_scores['auc'] = roc_auc_score(y_train, train_predict_proba)\n",
    "        train_scores['accuracy'] = accuracy_score(y_train, train_predict)\n",
    "        train_scores['recall_macro'] = recall_score(y_train, train_predict, average = 'macro')\n",
    "        train_scores['precision_macro'] = precision_score(y_train, train_predict, average = 'macro')\n",
    "        train_scores['f1_macro'] = f1_score(y_train, train_predict, average = 'macro')\n",
    "\n",
    "        # Save training scores dictionary to model dictionary\n",
    "        model_dict['training'] = train_scores\n",
    "\n",
    "        # Save tuning scores\n",
    "        tune_scores = dict() # to store all tuning scores\n",
    "        tune_predict = opt_model.predict(X_tune)\n",
    "        tune_predict_proba = opt_model.predict_proba(X_tune)[:,1] # check on this subscripting\n",
    "        tune_scores['auc'] = roc_auc_score(y_tune, tune_predict_proba)\n",
    "        tune_scores['accuracy'] = accuracy_score(y_tune, tune_predict)\n",
    "        tune_scores['recall_macro'] = recall_score(y_tune, tune_predict, average = 'macro')\n",
    "        tune_scores['precision_macro'] = precision_score(y_tune, tune_predict, average = 'macro')\n",
    "        tune_scores['f1_macro'] = f1_score(y_tune, tune_predict, average = 'macro')\n",
    "\n",
    "        # Save tuning scores dictionary to model dictionary\n",
    "        model_dict['tuning'] = tune_scores\n",
    "\n",
    "        # Testing\n",
    "        test_scores = dict() # to store all testing scores\n",
    "        test_predict = opt_model.predict(X_test)\n",
    "        test_predict_proba = opt_model.predict_proba(X_test)[:,1]\n",
    "        test_scores['auc'] = roc_auc_score(y_test, test_predict_proba)\n",
    "        test_scores['accuracy'] = accuracy_score(y_test, test_predict)\n",
    "        test_scores['recall_macro'] = recall_score(y_test, test_predict, average = 'macro')\n",
    "        test_scores['precision_macro'] = precision_score(y_test, test_predict, average = 'macro')\n",
    "        test_scores['f1_macro'] = f1_score(y_test, test_predict, average = 'macro')\n",
    "\n",
    "        # Save test scores dictionary to model dictionary\n",
    "        model_dict['testing'] = test_scores\n",
    "\n",
    "        # Save model dictionary to overall dictionary\n",
    "        performance[key1] = model_dict\n",
    "    \n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get results for BERT embeddings\n",
    "BERT_results = get_results(tweet_embeddings_BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get results for A embeddings\n",
    "A_results = get_results(tweet_embeddings_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create results df from nested dictionary\n",
    "def create_df(input_dict):\n",
    "    df = pd.DataFrame(input_dict)\n",
    "    df = df.transpose()\n",
    "    \n",
    "    df_params = df['params'].apply(pd.Series)\n",
    "    \n",
    "    df_training = df['training'].apply(pd.Series)\n",
    "    df_training.columns = ['train_' + str(col) for col in df_training.columns]\n",
    "    \n",
    "    df_tuning = df['tuning'].apply(pd.Series)\n",
    "    df_tuning.columns = ['tune_' + str(col) for col in df_tuning.columns]\n",
    "    \n",
    "    df_testing = df['testing'].apply(pd.Series)\n",
    "    df_testing.columns = ['test_' + str(col) for col in df_testing.columns]\n",
    "    \n",
    "    final_df = pd.concat([df_training, df_tuning, df_testing, df_params], axis = 1).reset_index()\n",
    "    final_df = final_df.rename({'index': 'fold_num'}, axis = 1)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get means for test results from dataframe of full results\n",
    "def get_test_means(df):\n",
    "    filter_cols = [col for col in df if col.startswith('test_')]\n",
    "    df_test = df[filter_cols]\n",
    "    df_test_mean = pd.DataFrame(df_test.mean(axis = 0)).transpose()\n",
    "    \n",
    "    return df_test_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save BERT embedding results\n",
    "BERT_full = create_df(BERT_results)\n",
    "BERT_full.to_csv('BERT_svm_full_results.csv')\n",
    "\n",
    "BERT_test_mean = get_test_means(BERT_full)\n",
    "BERT_test_mean.to_csv('BERT_svm_testmean_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save A embedding results\n",
    "A_full = create_df(A_results)\n",
    "A_full.to_csv('A_svm_full_results.csv')\n",
    "\n",
    "A_test_mean = get_test_means(A_full)\n",
    "A_test_mean.to_csv('A_svm_testmean_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
