{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy.io import loadmat\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Is_Unreliable</th>\n",
       "      <th>Category</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 3, 6, 9</td>\n",
       "      <td>We are living in scary times in Canada. Gov’t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 6, 8, 9</td>\n",
       "      <td>Just as bad in Canada. In fact, our government...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 4, 9</td>\n",
       "      <td>It was only a matter of time before the mainst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6, 8</td>\n",
       "      <td>Russia's taking no chances: Foreigners infecte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>6, 8, 9</td>\n",
       "      <td>Although there is now a presumptive confirmed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BREAKING: Harvard classes will move online sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Singularity University is hosting a FREE Virtu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coronavirus: how does it spread and what are t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stanford just cancelled classes for the rest o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tech conferences were cancelled in #Waterloo R...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Is_Unreliable    Category  \\\n",
       "0                1  1, 3, 6, 9   \n",
       "1                1  1, 6, 8, 9   \n",
       "2                1     1, 4, 9   \n",
       "3                1        6, 8   \n",
       "4                1     6, 8, 9   \n",
       "..             ...         ...   \n",
       "555              0         NaN   \n",
       "556              0         NaN   \n",
       "557              0         NaN   \n",
       "558              0         NaN   \n",
       "559              0         NaN   \n",
       "\n",
       "                                                 Tweet  \n",
       "0    We are living in scary times in Canada. Gov’t ...  \n",
       "1    Just as bad in Canada. In fact, our government...  \n",
       "2    It was only a matter of time before the mainst...  \n",
       "3    Russia's taking no chances: Foreigners infecte...  \n",
       "4    Although there is now a presumptive confirmed ...  \n",
       "..                                                 ...  \n",
       "555  BREAKING: Harvard classes will move online sta...  \n",
       "556  Singularity University is hosting a FREE Virtu...  \n",
       "557  Coronavirus: how does it spread and what are t...  \n",
       "558  Stanford just cancelled classes for the rest o...  \n",
       "559  Tech conferences were cancelled in #Waterloo R...  \n",
       "\n",
       "[560 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tweets\n",
    "tweets = pd.read_excel('../COVID19_Dataset-CM-ZB-complete with sources.xlsx')\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in word embeddings\n",
    "A_mat_contents = loadmat('../18Jan2021_Zorro_output/A.mat')\n",
    "S_mat_contents = loadmat('../18Jan2021_Zorro_output/S.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2327, 250)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = A_mat_contents['A']\n",
    "A.shape # context words by components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 2327)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = S_mat_contents['S']\n",
    "S.shape # components by target words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2327, 250)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_T = S.T\n",
    "S_T.shape # target words by components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabulary list\n",
    "with open('tweet_vocab_list', 'rb') as f:\n",
    "    tweet_vocab_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2327"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweet_vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 0,\n",
       " '#': 1,\n",
       " '(': 2,\n",
       " ')': 3,\n",
       " ',': 4,\n",
       " '-': 5,\n",
       " '--': 6,\n",
       " '.': 7,\n",
       " '..': 8,\n",
       " '...': 9,\n",
       " '....': 10,\n",
       " '1': 11,\n",
       " '1,600': 12,\n",
       " '1,975': 13,\n",
       " '1-': 14,\n",
       " '10': 15,\n",
       " '10,000': 16,\n",
       " '100': 17,\n",
       " '1000': 18,\n",
       " '10:30': 19,\n",
       " '10:30am': 20,\n",
       " '10th': 21,\n",
       " '11': 22,\n",
       " '115': 23,\n",
       " '118,000': 24,\n",
       " '12': 25,\n",
       " '133': 26,\n",
       " '14': 27,\n",
       " '14:21': 28,\n",
       " '1500': 29,\n",
       " '16-18': 30,\n",
       " '17': 31,\n",
       " '170': 32,\n",
       " '18': 33,\n",
       " '19': 34,\n",
       " '1900': 35,\n",
       " '19th': 36,\n",
       " '1:30': 37,\n",
       " '1b': 38,\n",
       " '1st': 39,\n",
       " '2': 40,\n",
       " '2-': 41,\n",
       " '2.5': 42,\n",
       " '20': 43,\n",
       " '20-40s': 44,\n",
       " '200': 45,\n",
       " '2019': 46,\n",
       " '2019-ncovid': 47,\n",
       " '2019ncov': 48,\n",
       " '2020': 49,\n",
       " '2021': 50,\n",
       " '21': 51,\n",
       " '22': 52,\n",
       " '23': 53,\n",
       " '24': 54,\n",
       " '24thminute': 55,\n",
       " '25': 56,\n",
       " '26': 57,\n",
       " '27': 58,\n",
       " '2700': 59,\n",
       " '29th': 60,\n",
       " '2\\uf30e1': 61,\n",
       " '3': 62,\n",
       " '3,000': 63,\n",
       " '3.4': 64,\n",
       " '30': 65,\n",
       " '30.8': 66,\n",
       " '32': 67,\n",
       " '3200': 68,\n",
       " '36': 69,\n",
       " '366': 70,\n",
       " '38': 71,\n",
       " '39': 72,\n",
       " '4': 73,\n",
       " '40': 74,\n",
       " '40-70': 75,\n",
       " '409k': 76,\n",
       " '42nd': 77,\n",
       " '43': 78,\n",
       " '4600': 79,\n",
       " '48': 80,\n",
       " '5': 81,\n",
       " '50': 82,\n",
       " '54,000': 83,\n",
       " '56,000': 84,\n",
       " '57': 85,\n",
       " '577': 86,\n",
       " '5:30pm': 87,\n",
       " '5g': 88,\n",
       " '5th': 89,\n",
       " '6': 90,\n",
       " '6,000': 91,\n",
       " '60': 92,\n",
       " '600': 93,\n",
       " '60m': 94,\n",
       " '61k': 95,\n",
       " '66': 96,\n",
       " '680news': 97,\n",
       " '680newsweather': 98,\n",
       " '691': 99,\n",
       " '6ixside': 100,\n",
       " '7': 101,\n",
       " '7,800': 102,\n",
       " '71': 103,\n",
       " '75': 104,\n",
       " '76': 105,\n",
       " '7th': 106,\n",
       " '8': 107,\n",
       " '8,400': 108,\n",
       " '8.3': 109,\n",
       " '80': 110,\n",
       " '800': 111,\n",
       " '80yrs': 112,\n",
       " '88-year-old': 113,\n",
       " '8th': 114,\n",
       " '9': 115,\n",
       " '90': 116,\n",
       " '90,000': 117,\n",
       " '99': 118,\n",
       " '9pm': 119,\n",
       " ':': 120,\n",
       " ';': 121,\n",
       " '<END>': 122,\n",
       " '<START>': 123,\n",
       " '?': 124,\n",
       " 'abandon': 125,\n",
       " 'abc': 126,\n",
       " 'ability': 127,\n",
       " 'aboard': 128,\n",
       " 'absolutley': 129,\n",
       " 'abuse': 130,\n",
       " 'accept': 131,\n",
       " 'accord': 132,\n",
       " 'account': 133,\n",
       " 'accountability': 134,\n",
       " 'accurate': 135,\n",
       " 'accuse': 136,\n",
       " 'achieve': 137,\n",
       " 'across': 138,\n",
       " 'act': 139,\n",
       " 'action': 140,\n",
       " 'acute': 141,\n",
       " 'ad': 142,\n",
       " 'adamsmiller': 143,\n",
       " 'adapt': 144,\n",
       " 'addendum': 145,\n",
       " 'addition': 146,\n",
       " 'additional': 147,\n",
       " 'address': 148,\n",
       " 'administration': 149,\n",
       " 'admit': 150,\n",
       " 'admits': 151,\n",
       " 'advice': 152,\n",
       " 'advises': 153,\n",
       " 'afford': 154,\n",
       " 'affordable': 155,\n",
       " 'afp': 156,\n",
       " 'afraid': 157,\n",
       " 'african': 158,\n",
       " 'afternoon': 159,\n",
       " 'age': 160,\n",
       " 'agency': 161,\n",
       " 'agenda': 162,\n",
       " 'ago': 163,\n",
       " 'agrabah': 164,\n",
       " 'agree': 165,\n",
       " 'ai': 166,\n",
       " 'aim': 167,\n",
       " 'air': 168,\n",
       " 'aircraft': 169,\n",
       " 'airlift': 170,\n",
       " 'airline': 171,\n",
       " 'airport': 172,\n",
       " 'aka': 173,\n",
       " 'al': 174,\n",
       " 'aladdin': 175,\n",
       " 'alarm': 176,\n",
       " 'alarmed': 177,\n",
       " 'alberta': 178,\n",
       " 'alert': 179,\n",
       " 'allow': 180,\n",
       " 'almost': 181,\n",
       " 'already': 182,\n",
       " 'also': 183,\n",
       " 'although': 184,\n",
       " 'always': 185,\n",
       " 'am740': 186,\n",
       " 'amazon': 187,\n",
       " 'america': 188,\n",
       " 'american': 189,\n",
       " 'americans.': 190,\n",
       " 'amid': 191,\n",
       " 'among': 192,\n",
       " 'amount': 193,\n",
       " 'amp': 194,\n",
       " 'amplify': 195,\n",
       " 'anchor': 196,\n",
       " 'andrewlawton': 197,\n",
       " 'animal': 198,\n",
       " 'announce': 199,\n",
       " 'announcement': 200,\n",
       " 'announces': 201,\n",
       " 'another': 202,\n",
       " 'answer': 203,\n",
       " 'anthony': 204,\n",
       " 'anthonyfurey': 205,\n",
       " 'anti-vaxx': 206,\n",
       " 'anti-vaxxers': 207,\n",
       " 'antidote': 208,\n",
       " 'antiviral': 209,\n",
       " 'anus': 210,\n",
       " 'anxiety': 211,\n",
       " 'anxious': 212,\n",
       " 'anyone': 213,\n",
       " 'anything': 214,\n",
       " 'anyway': 215,\n",
       " 'apartment': 216,\n",
       " 'apocalyptic': 217,\n",
       " 'apparently': 218,\n",
       " 'appear': 219,\n",
       " 'apple': 220,\n",
       " 'application': 221,\n",
       " 'approach': 222,\n",
       " 'approve': 223,\n",
       " 'approves': 224,\n",
       " 'april': 225,\n",
       " 'area': 226,\n",
       " 'around': 227,\n",
       " 'arrest': 228,\n",
       " 'arrogant': 229,\n",
       " 'arse': 230,\n",
       " 'arsenal': 231,\n",
       " 'arteta': 232,\n",
       " 'article': 233,\n",
       " 'asia': 234,\n",
       " 'aside': 235,\n",
       " 'ask': 236,\n",
       " 'aspect': 237,\n",
       " 'asserts': 238,\n",
       " 'association': 239,\n",
       " 'asteroid': 240,\n",
       " 'atexan575': 241,\n",
       " 'attack': 242,\n",
       " 'attempt': 243,\n",
       " 'attend': 244,\n",
       " 'attn': 245,\n",
       " 'australian': 246,\n",
       " 'authoritarian': 247,\n",
       " 'authority': 248,\n",
       " 'auto': 249,\n",
       " 'available': 250,\n",
       " 'avoid': 251,\n",
       " 'await': 252,\n",
       " 'awake': 253,\n",
       " 'award': 254,\n",
       " 'aware': 255,\n",
       " 'away': 256,\n",
       " 'awesome': 257,\n",
       " 'ayatollah': 258,\n",
       " 'azar': 259,\n",
       " 'b.c': 260,\n",
       " 'b.j': 261,\n",
       " 'baby': 262,\n",
       " 'back': 263,\n",
       " 'backwards': 264,\n",
       " 'backyard': 265,\n",
       " 'bad': 266,\n",
       " 'badge': 267,\n",
       " 'ban': 268,\n",
       " 'band': 269,\n",
       " 'bank': 270,\n",
       " 'banknote': 271,\n",
       " 'bankruptcy': 272,\n",
       " 'banner': 273,\n",
       " 'bannon': 274,\n",
       " 'barbaric': 275,\n",
       " 'barcelona': 276,\n",
       " 'base': 277,\n",
       " 'bat': 278,\n",
       " 'battle': 279,\n",
       " 'battling': 280,\n",
       " 'bbc': 281,\n",
       " 'bc': 282,\n",
       " 'bcc2020': 283,\n",
       " 'beard': 284,\n",
       " 'become': 285,\n",
       " 'becomes': 286,\n",
       " 'bee': 287,\n",
       " 'beer': 288,\n",
       " 'begin': 289,\n",
       " 'beginning': 290,\n",
       " 'behind': 291,\n",
       " 'believe': 292,\n",
       " 'bell': 293,\n",
       " 'bellletstalk': 294,\n",
       " 'benefit': 295,\n",
       " 'benshapiro': 296,\n",
       " 'berezingoal': 297,\n",
       " 'berkeley': 298,\n",
       " 'best': 299,\n",
       " 'bet': 300,\n",
       " 'betraying': 301,\n",
       " 'beyond': 302,\n",
       " 'bible': 303,\n",
       " 'biden': 304,\n",
       " 'big': 305,\n",
       " 'bill': 306,\n",
       " 'billion': 307,\n",
       " 'billionaire': 308,\n",
       " 'bio': 309,\n",
       " 'bioinformatics': 310,\n",
       " 'bioweapon': 311,\n",
       " 'bird': 312,\n",
       " 'bitch': 313,\n",
       " 'blame': 314,\n",
       " 'blast': 315,\n",
       " 'bleac': 316,\n",
       " 'blessing': 317,\n",
       " 'block': 318,\n",
       " 'blockades-racism': 319,\n",
       " 'blog': 320,\n",
       " 'blogto': 321,\n",
       " 'blood': 322,\n",
       " 'board': 323,\n",
       " 'boastful': 324,\n",
       " 'bolster': 325,\n",
       " 'bombing': 326,\n",
       " 'bond': 327,\n",
       " 'bonkers': 328,\n",
       " 'book': 329,\n",
       " 'boom': 330,\n",
       " 'boomer': 331,\n",
       " 'boost': 332,\n",
       " 'border': 333,\n",
       " 'bore': 334,\n",
       " 'boring': 335,\n",
       " 'bos': 336,\n",
       " 'bottle': 337,\n",
       " 'bottom': 338,\n",
       " 'brampton': 339,\n",
       " 'brawn': 340,\n",
       " 'breach': 341,\n",
       " 'break': 342,\n",
       " 'breakthrough': 343,\n",
       " 'bresnick': 344,\n",
       " 'brexit': 345,\n",
       " 'bribe': 346,\n",
       " 'briefing': 347,\n",
       " 'brilliant': 348,\n",
       " 'bring': 349,\n",
       " 'british': 350,\n",
       " 'bro': 351,\n",
       " 'brown': 352,\n",
       " 'brunswick': 353,\n",
       " 'brutal': 354,\n",
       " 'build': 355,\n",
       " 'building': 356,\n",
       " 'bunch': 357,\n",
       " 'bungie': 358,\n",
       " 'burn': 359,\n",
       " 'bus': 360,\n",
       " 'business': 361,\n",
       " 'busy': 362,\n",
       " 'buy': 363,\n",
       " 'c.d.c': 364,\n",
       " 'cadre': 365,\n",
       " 'cal': 366,\n",
       " 'california': 367,\n",
       " 'call': 368,\n",
       " 'calm': 369,\n",
       " 'calorie': 370,\n",
       " 'camel': 371,\n",
       " 'camp': 372,\n",
       " 'campaign': 373,\n",
       " 'canada': 374,\n",
       " 'canadian': 375,\n",
       " 'cancel': 376,\n",
       " 'cancellation': 377,\n",
       " 'cancer': 378,\n",
       " 'candidate': 379,\n",
       " 'canpoli': 380,\n",
       " 'cant': 381,\n",
       " 'capacity': 382,\n",
       " 'cardiovascular': 383,\n",
       " 'care': 384,\n",
       " 'carletonu': 385,\n",
       " 'carmichaelkevin': 386,\n",
       " 'carrier': 387,\n",
       " 'carry': 388,\n",
       " 'case': 389,\n",
       " 'cash': 390,\n",
       " 'cashmere': 391,\n",
       " 'caslernoel': 392,\n",
       " 'cast': 393,\n",
       " 'cat': 394,\n",
       " 'catastrophy': 395,\n",
       " 'catch': 396,\n",
       " 'caught': 397,\n",
       " 'causal': 398,\n",
       " 'cause': 399,\n",
       " 'cbcnews': 400,\n",
       " 'cbcsask': 401,\n",
       " 'ccp': 402,\n",
       " 'cdc': 403,\n",
       " 'cdnhealth': 404,\n",
       " 'cdnmedia': 405,\n",
       " 'cdnpoli': 406,\n",
       " 'cdnpressnews': 407,\n",
       " 'cease': 408,\n",
       " 'cent': 409,\n",
       " 'center': 410,\n",
       " 'centre': 411,\n",
       " 'ceremony': 412,\n",
       " 'certain': 413,\n",
       " 'certainly': 414,\n",
       " 'ch.167': 415,\n",
       " 'champion': 416,\n",
       " 'chance': 417,\n",
       " 'change': 418,\n",
       " 'changesforthebetter': 419,\n",
       " 'chaos': 420,\n",
       " 'charge': 421,\n",
       " 'chase': 422,\n",
       " 'chat': 423,\n",
       " 'cheap': 424,\n",
       " 'check': 425,\n",
       " 'cheer': 426,\n",
       " 'chelsea': 427,\n",
       " 'chernobyl': 428,\n",
       " 'chick-fil-a': 429,\n",
       " 'chief': 430,\n",
       " 'child': 431,\n",
       " 'childcare': 432,\n",
       " 'childish': 433,\n",
       " 'china': 434,\n",
       " 'chinavirus': 435,\n",
       " 'chinese': 436,\n",
       " 'chinese-canadian': 437,\n",
       " 'choice': 438,\n",
       " 'choose': 439,\n",
       " 'chriscuomo': 440,\n",
       " 'chrislhayes': 441,\n",
       " 'christian': 442,\n",
       " 'chronic': 443,\n",
       " 'chuck': 444,\n",
       " 'churchill': 445,\n",
       " 'cia': 446,\n",
       " 'cite': 447,\n",
       " 'citizen': 448,\n",
       " 'city': 449,\n",
       " 'citywide': 450,\n",
       " 'civilization': 451,\n",
       " 'ckont': 452,\n",
       " 'ckpublichealth': 453,\n",
       " 'claim': 454,\n",
       " 'clarity': 455,\n",
       " 'class': 456,\n",
       " 'classifies': 457,\n",
       " 'clean': 458,\n",
       " 'cleaning': 459,\n",
       " 'clear': 460,\n",
       " 'cleric': 461,\n",
       " 'climate': 462,\n",
       " 'climb': 463,\n",
       " 'clinton': 464,\n",
       " 'close': 465,\n",
       " 'closing': 466,\n",
       " 'cloud': 467,\n",
       " 'clout': 468,\n",
       " 'cnbc': 469,\n",
       " 'cnn': 470,\n",
       " 'coach': 471,\n",
       " 'coachella': 472,\n",
       " 'coincidence': 473,\n",
       " 'cold': 474,\n",
       " 'collapse': 475,\n",
       " 'college': 476,\n",
       " 'colorado': 477,\n",
       " 'columbia': 478,\n",
       " 'column': 479,\n",
       " 'columnist': 480,\n",
       " 'combat': 481,\n",
       " 'come': 482,\n",
       " 'commits': 483,\n",
       " 'common': 484,\n",
       " 'commonly': 485,\n",
       " 'communication': 486,\n",
       " 'communist': 487,\n",
       " 'community': 488,\n",
       " 'commute': 489,\n",
       " 'company': 490,\n",
       " 'comparative': 491,\n",
       " 'compare': 492,\n",
       " 'complete': 493,\n",
       " 'concern': 494,\n",
       " 'concerned': 495,\n",
       " 'condition': 496,\n",
       " 'conditioning': 497,\n",
       " 'condom': 498,\n",
       " 'conf': 499,\n",
       " 'conference': 500,\n",
       " 'confine': 501,\n",
       " 'confirm': 502,\n",
       " 'confirms': 503,\n",
       " 'congress': 504,\n",
       " 'consequence': 505,\n",
       " 'conservative': 506,\n",
       " 'conspiracy': 507,\n",
       " 'consume': 508,\n",
       " 'contact': 509,\n",
       " 'contagion': 510,\n",
       " 'contagious': 511,\n",
       " 'contain': 512,\n",
       " 'contaminate': 513,\n",
       " 'continent': 514,\n",
       " 'continue': 515,\n",
       " 'contract': 516,\n",
       " 'contradicts': 517,\n",
       " 'control': 518,\n",
       " 'conversation': 519,\n",
       " 'convince': 520,\n",
       " 'cooky': 521,\n",
       " 'cool': 522,\n",
       " 'cop': 523,\n",
       " 'cope': 524,\n",
       " 'copper': 525,\n",
       " 'cornwall': 526,\n",
       " 'corona': 527,\n",
       " 'coronarvirus': 528,\n",
       " 'coronavirus': 529,\n",
       " 'coronavirus-': 530,\n",
       " 'coronavirus-hit': 531,\n",
       " 'coronavirus-racism': 532,\n",
       " 'coronavirus-video': 533,\n",
       " 'coronaviruscanada': 534,\n",
       " 'coronaviruschallenge': 535,\n",
       " 'coronaviruse': 536,\n",
       " 'coronaviruses': 537,\n",
       " 'coronavirusoutbreak': 538,\n",
       " 'coronavirusoutbreak-': 539,\n",
       " 'coronaviruspandemic': 540,\n",
       " 'coronavirusseattle': 541,\n",
       " 'coronavirusupdate': 542,\n",
       " 'coronavirusupdates': 543,\n",
       " 'coronovirus': 544,\n",
       " 'coronviru': 545,\n",
       " 'corporate': 546,\n",
       " 'correct': 547,\n",
       " 'correlation': 548,\n",
       " 'corvid19': 549,\n",
       " 'cost': 550,\n",
       " 'cotton': 551,\n",
       " 'cough': 552,\n",
       " 'could': 553,\n",
       " 'councillor': 554,\n",
       " 'counter': 555,\n",
       " 'country': 556,\n",
       " 'county': 557,\n",
       " 'course': 558,\n",
       " 'court': 559,\n",
       " 'cover': 560,\n",
       " 'coverage': 561,\n",
       " 'covid': 562,\n",
       " 'covid-19': 563,\n",
       " 'covid-19coronavirus': 564,\n",
       " 'covid-19—as': 565,\n",
       " 'covid19': 566,\n",
       " 'covid19canada': 567,\n",
       " 'covid19italia': 568,\n",
       " 'covid2019': 569,\n",
       " 'covid–19': 570,\n",
       " 'covidー19': 571,\n",
       " 'cp24': 572,\n",
       " 'cpac': 573,\n",
       " 'crap': 574,\n",
       " 'crash': 575,\n",
       " 'create': 576,\n",
       " 'credible': 577,\n",
       " 'crew': 578,\n",
       " 'crisis': 579,\n",
       " 'critical': 580,\n",
       " 'crook': 581,\n",
       " 'cruise': 582,\n",
       " 'csse': 583,\n",
       " 'ctvtoronto': 584,\n",
       " 'cuckoo': 585,\n",
       " 'cuomo': 586,\n",
       " 'cure': 587,\n",
       " 'current': 588,\n",
       " 'currently': 589,\n",
       " 'cusp': 590,\n",
       " 'customer': 591,\n",
       " 'cut': 592,\n",
       " 'd19vp': 593,\n",
       " 'daily': 594,\n",
       " 'damage': 595,\n",
       " 'damn': 596,\n",
       " 'danger': 597,\n",
       " 'dangerous': 598,\n",
       " 'daniele': 599,\n",
       " 'data': 600,\n",
       " 'date': 601,\n",
       " 'daveaurkov': 602,\n",
       " 'david': 603,\n",
       " 'davidculver': 604,\n",
       " 'davidptarrant': 605,\n",
       " 'day': 606,\n",
       " 'dc': 607,\n",
       " 'dd6': 608,\n",
       " 'dead': 609,\n",
       " 'deadly': 610,\n",
       " 'deal': 611,\n",
       " 'death': 612,\n",
       " 'dec': 613,\n",
       " 'decide': 614,\n",
       " 'decisively': 615,\n",
       " 'declare': 616,\n",
       " 'declares': 617,\n",
       " 'deep': 618,\n",
       " 'deeply': 619,\n",
       " 'deepmind': 620,\n",
       " 'defend': 621,\n",
       " 'defense': 622,\n",
       " 'definitely': 623,\n",
       " 'defs': 624,\n",
       " 'degree': 625,\n",
       " 'delay': 626,\n",
       " 'delayed': 627,\n",
       " 'deliberately': 628,\n",
       " 'delivery': 629,\n",
       " 'dem': 630,\n",
       " 'demagoguery': 631,\n",
       " 'demand': 632,\n",
       " 'democrat': 633,\n",
       " 'democratic': 634,\n",
       " 'demonstrate': 635,\n",
       " 'dems': 636,\n",
       " 'deny': 637,\n",
       " 'department': 638,\n",
       " 'depend': 639,\n",
       " 'deploys': 640,\n",
       " 'deport': 641,\n",
       " 'dept': 642,\n",
       " 'deputy': 643,\n",
       " 'descend': 644,\n",
       " 'deserves': 645,\n",
       " 'design': 646,\n",
       " 'desperate': 647,\n",
       " 'despite': 648,\n",
       " 'destroy': 649,\n",
       " 'destroyed': 650,\n",
       " 'detail': 651,\n",
       " 'detect': 652,\n",
       " 'develop': 653,\n",
       " 'developer': 654,\n",
       " 'development': 655,\n",
       " 'diagnose': 656,\n",
       " 'diamond': 657,\n",
       " 'dictate': 658,\n",
       " 'didnt': 659,\n",
       " 'die': 660,\n",
       " 'different': 661,\n",
       " 'difficulty': 662,\n",
       " 'diligently': 663,\n",
       " 'dinosaur': 664,\n",
       " 'dip': 665,\n",
       " 'direct': 666,\n",
       " 'direction': 667,\n",
       " 'director': 668,\n",
       " 'dirty': 669,\n",
       " 'disappointed': 670,\n",
       " 'discharge': 671,\n",
       " 'discus': 672,\n",
       " 'discuss': 673,\n",
       " 'discussion': 674,\n",
       " 'disease': 675,\n",
       " 'dishonestly': 676,\n",
       " 'disinfection': 677,\n",
       " 'disinformation': 678,\n",
       " 'dispel': 679,\n",
       " 'disruption': 680,\n",
       " 'dissension': 681,\n",
       " 'dissolve': 682,\n",
       " 'distance': 683,\n",
       " 'district': 684,\n",
       " 'disturb': 685,\n",
       " 'dividend': 686,\n",
       " 'diy': 687,\n",
       " 'dm': 688,\n",
       " 'dna': 689,\n",
       " 'do': 690,\n",
       " 'doc': 691,\n",
       " 'docked': 692,\n",
       " 'doctor': 693,\n",
       " 'dodge': 694,\n",
       " 'doesnt': 695,\n",
       " 'dog': 696,\n",
       " 'dogbreath': 697,\n",
       " 'dominate': 698,\n",
       " 'donald': 699,\n",
       " 'donates': 700,\n",
       " 'donorrhea': 701,\n",
       " 'donovan': 702,\n",
       " 'dont': 703,\n",
       " 'door': 704,\n",
       " 'dorisgrinspun': 705,\n",
       " 'dorries': 706,\n",
       " 'dotard': 707,\n",
       " 'double': 708,\n",
       " 'doubt': 709,\n",
       " 'doug': 710,\n",
       " 'dow': 711,\n",
       " 'downplay': 712,\n",
       " 'downplays': 713,\n",
       " 'downsize': 714,\n",
       " 'dozen': 715,\n",
       " 'dr': 716,\n",
       " 'dr.': 717,\n",
       " 'draw': 718,\n",
       " 'drdagly': 719,\n",
       " 'dream': 720,\n",
       " 'drink': 721,\n",
       " 'drinking': 722,\n",
       " 'drizz061': 723,\n",
       " 'drjabra': 724,\n",
       " 'drlynnwilson': 725,\n",
       " 'drop': 726,\n",
       " 'drpauloffit': 727,\n",
       " 'drug': 728,\n",
       " 'drugmakers': 729,\n",
       " 'ds8': 730,\n",
       " 'due': 731,\n",
       " 'dug': 732,\n",
       " 'ea': 733,\n",
       " 'ear': 734,\n",
       " 'early': 735,\n",
       " 'earth': 736,\n",
       " 'east': 737,\n",
       " 'eat': 738,\n",
       " 'ebola': 739,\n",
       " 'economic': 740,\n",
       " 'economy': 741,\n",
       " 'edge': 742,\n",
       " 'educate': 743,\n",
       " 'education': 744,\n",
       " 'eek': 745,\n",
       " 'effect': 746,\n",
       " 'effective': 747,\n",
       " 'effort': 748,\n",
       " 'ego': 749,\n",
       " 'egypt': 750,\n",
       " 'eight': 751,\n",
       " 'elad': 752,\n",
       " 'elderly': 753,\n",
       " 'elect': 754,\n",
       " 'election': 755,\n",
       " 'electronics': 756,\n",
       " 'eliminate': 757,\n",
       " 'else': 758,\n",
       " 'elsewhere': 759,\n",
       " 'email': 760,\n",
       " 'embrace': 761,\n",
       " 'emerge': 762,\n",
       " 'emergency': 763,\n",
       " 'emergency-': 764,\n",
       " 'emerita': 765,\n",
       " 'emplaw': 766,\n",
       " 'employee': 767,\n",
       " 'employer': 768,\n",
       " 'encourage': 769,\n",
       " 'end': 770,\n",
       " 'endless': 771,\n",
       " 'engineer': 772,\n",
       " 'engineering': 773,\n",
       " 'england': 774,\n",
       " 'enjoy': 775,\n",
       " 'enough': 776,\n",
       " 'ensure': 777,\n",
       " 'entire': 778,\n",
       " 'eoir': 779,\n",
       " 'epicentre': 780,\n",
       " 'epidemic': 781,\n",
       " 'epidemiologist': 782,\n",
       " 'episode': 783,\n",
       " 'equip': 784,\n",
       " 'equivalent': 785,\n",
       " 'esfandyar': 786,\n",
       " 'establishes': 787,\n",
       " 'estate': 788,\n",
       " 'et': 789,\n",
       " 'etc': 790,\n",
       " 'etc.c291': 791,\n",
       " 'europe': 792,\n",
       " 'european': 793,\n",
       " 'europe—but': 794,\n",
       " 'euthanasia': 795,\n",
       " 'evacuee': 796,\n",
       " 'even': 797,\n",
       " 'event': 798,\n",
       " 'ever': 799,\n",
       " 'every': 800,\n",
       " 'everyone': 801,\n",
       " 'everything': 802,\n",
       " 'evidence': 803,\n",
       " 'evil': 804,\n",
       " 'evolve': 805,\n",
       " 'exactly': 806,\n",
       " 'excellent': 807,\n",
       " 'except': 808,\n",
       " 'excite': 809,\n",
       " 'exclusive': 810,\n",
       " 'executive': 811,\n",
       " 'existential': 812,\n",
       " 'expands': 813,\n",
       " 'expect': 814,\n",
       " 'expectation': 815,\n",
       " 'experienced': 816,\n",
       " 'expert': 817,\n",
       " 'expire': 818,\n",
       " 'explain': 819,\n",
       " 'explains': 820,\n",
       " 'expose': 821,\n",
       " 'exposure': 822,\n",
       " 'extent': 823,\n",
       " 'eye-opener': 824,\n",
       " 'f1': 825,\n",
       " 'f8': 826,\n",
       " 'face': 827,\n",
       " 'facebook': 828,\n",
       " 'facial': 829,\n",
       " 'facility': 830,\n",
       " 'fact': 831,\n",
       " 'factor': 832,\n",
       " 'faculty': 833,\n",
       " 'fail': 834,\n",
       " 'fair': 835,\n",
       " 'fake': 836,\n",
       " 'fall': 837,\n",
       " 'false': 838,\n",
       " 'falsely': 839,\n",
       " 'family': 840,\n",
       " 'fan': 841,\n",
       " 'far': 842,\n",
       " 'fast': 843,\n",
       " 'faster': 844,\n",
       " 'fat': 845,\n",
       " 'fatality': 846,\n",
       " 'fda': 847,\n",
       " 'fear': 848,\n",
       " 'fearful': 849,\n",
       " 'fearmongering': 850,\n",
       " 'feature': 851,\n",
       " 'feb': 852,\n",
       " 'federal': 853,\n",
       " 'feedly': 854,\n",
       " 'feel': 855,\n",
       " 'festival': 856,\n",
       " 'fever': 857,\n",
       " 'fictional': 858,\n",
       " 'fictitious': 859,\n",
       " 'fifth': 860,\n",
       " 'fight': 861,\n",
       " 'figure': 862,\n",
       " 'film': 863,\n",
       " 'finally': 864,\n",
       " 'financial': 865,\n",
       " 'financially': 866,\n",
       " 'find': 867,\n",
       " 'first': 868,\n",
       " 'fischer': 869,\n",
       " 'five': 870,\n",
       " 'fix': 871,\n",
       " 'flag': 872,\n",
       " 'flat': 873,\n",
       " 'flatly': 874,\n",
       " 'flaw': 875,\n",
       " 'flight': 876,\n",
       " 'flood': 877,\n",
       " 'florida': 878,\n",
       " 'flu': 879,\n",
       " 'fly': 880,\n",
       " 'focus': 881,\n",
       " 'folk': 882,\n",
       " 'follow': 883,\n",
       " 'food': 884,\n",
       " 'forbid': 885,\n",
       " 'force': 886,\n",
       " 'ford': 887,\n",
       " 'forecast': 888,\n",
       " 'foreign': 889,\n",
       " 'foreigner': 890,\n",
       " 'forever': 891,\n",
       " 'former': 892,\n",
       " 'fossil': 893,\n",
       " 'found': 894,\n",
       " 'four': 895,\n",
       " 'fourth': 896,\n",
       " 'fox': 897,\n",
       " 'foxnews': 898,\n",
       " 'fr': 899,\n",
       " 'france': 900,\n",
       " 'frank': 901,\n",
       " 'fraud': 902,\n",
       " 'free': 903,\n",
       " 'freeze': 904,\n",
       " 'friday': 905,\n",
       " 'fridaynightmusings': 906,\n",
       " 'friend': 907,\n",
       " 'fuck': 908,\n",
       " 'fuckin': 909,\n",
       " 'fuel': 910,\n",
       " 'fukushima': 911,\n",
       " 'full': 912,\n",
       " 'fund': 913,\n",
       " 'fundamentalist': 914,\n",
       " 'funding': 915,\n",
       " 'fur': 916,\n",
       " 'game': 917,\n",
       " 'garbage': 918,\n",
       " 'gaslight': 919,\n",
       " 'gate': 920,\n",
       " 'gdc': 921,\n",
       " 'gear': 922,\n",
       " 'general': 923,\n",
       " 'genetic': 924,\n",
       " 'genetically': 925,\n",
       " 'geraldo': 926,\n",
       " 'germ': 927,\n",
       " 'german': 928,\n",
       " 'get': 929,\n",
       " 'giant': 930,\n",
       " 'gics': 931,\n",
       " 'give': 932,\n",
       " 'glad': 933,\n",
       " 'glass': 934,\n",
       " 'global': 935,\n",
       " 'globalhealth': 936,\n",
       " 'globalism': 937,\n",
       " 'globalist': 938,\n",
       " 'globalnews': 939,\n",
       " 'globe': 940,\n",
       " 'globeandmail': 941,\n",
       " 'go': 942,\n",
       " 'god': 943,\n",
       " 'goes-home': 944,\n",
       " 'golfing': 945,\n",
       " 'gon': 946,\n",
       " 'good': 947,\n",
       " 'goodness': 948,\n",
       " 'google': 949,\n",
       " 'gop': 950,\n",
       " 'gotten': 951,\n",
       " 'gouge': 952,\n",
       " 'gov': 953,\n",
       " 'government': 954,\n",
       " 'governor': 955,\n",
       " 'gpowers03878337': 956,\n",
       " 'gqinsk': 957,\n",
       " 'graf': 958,\n",
       " 'grand': 959,\n",
       " 'graphic': 960,\n",
       " 'grasped': 961,\n",
       " 'grateful': 962,\n",
       " 'great': 963,\n",
       " 'greece': 964,\n",
       " 'greet': 965,\n",
       " 'grossly': 966,\n",
       " 'group': 967,\n",
       " 'grow': 968,\n",
       " 'grows': 969,\n",
       " 'gta': 970,\n",
       " 'guess': 971,\n",
       " 'guidance': 972,\n",
       " 'guide': 973,\n",
       " 'guy': 974,\n",
       " 'gvmnt': 975,\n",
       " 'h': 976,\n",
       " 'hair': 977,\n",
       " 'half': 978,\n",
       " 'hall': 979,\n",
       " 'hallucinate': 980,\n",
       " 'halton': 981,\n",
       " 'hamilton': 982,\n",
       " 'hammer': 983,\n",
       " 'hand': 984,\n",
       " 'handle': 985,\n",
       " 'hank': 986,\n",
       " 'hannity': 987,\n",
       " 'happen': 988,\n",
       " 'hard': 989,\n",
       " 'hardick': 990,\n",
       " 'hare': 991,\n",
       " 'harvard': 992,\n",
       " 'hat': 993,\n",
       " 'hate': 994,\n",
       " 'he': 995,\n",
       " 'head': 996,\n",
       " 'headline': 997,\n",
       " 'health': 998,\n",
       " 'health-care': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_dict = dict()\n",
    "for i in range(len(tweet_vocab_list)):\n",
    "    word = tweet_vocab_list[i]\n",
    "    vocabulary_dict[word] = i\n",
    "vocabulary_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derive Tweet Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to convert contractions picked up by word_tokenize() into full words\n",
    "contractions = {\n",
    "    \"n't\": 'not',\n",
    "    \"'ve\": 'have',\n",
    "    \"'s\": 'is', # note that this will include possessive nouns\n",
    "    'gonna': 'going to',\n",
    "    'gotta': 'got to',\n",
    "    \"'d\": 'would',\n",
    "    \"'ll\": 'will',\n",
    "    \"'re\": 'are',\n",
    "    \"'m\": 'am',\n",
    "    'wanna': 'want to'\n",
    "}\n",
    "\n",
    "# to convert nltk_pos tags to wordnet-compatible PoS tags\n",
    "def convert_pos_wordnet(tag):\n",
    "    tag_abbr = tag[0].upper()\n",
    "    tag_dict = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "                \n",
    "    if tag_abbr in tag_dict:\n",
    "        return tag_dict[tag_abbr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to get string embeddings from word embeddings\n",
    "\n",
    "def get_text_vectors(word_embeddings, # numpy array\n",
    "                     word_index_dict, # dictionary mapping words to index in array\n",
    "                     text_list, # list of strings to derive embeddings for\n",
    "                     remove_stopwords = True,\n",
    "                     lowercase = True,\n",
    "                     lemmatize = True,\n",
    "                     add_start_end_tokens = True):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    for k in range(len(text_list)):\n",
    "        text = text_list[k]\n",
    "        text = re.sub(r'[_~`@$%^&*[\\]+=\\|}{\\\"\\'<>/]+', '', text)\n",
    "        text_vec = np.zeros(word_embeddings.shape[1])\n",
    "        words = word_tokenize(text)\n",
    "        tracker = 0 # to track whether we've encountered a word for which we have an embedding (in each tweet)\n",
    "        \n",
    "        if remove_stopwords:\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                if word.lower() not in set(stopwords.words('english')):\n",
    "                    clean_words.append(word)\n",
    "            words = clean_words\n",
    "\n",
    "        if lowercase:\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                clean_words.append(word.lower())\n",
    "\n",
    "            words = clean_words\n",
    "\n",
    "        if lemmatize:\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                PoS_tag = pos_tag([word])[0][1]\n",
    "\n",
    "                # to change contractions to full word form\n",
    "                if word in contractions:\n",
    "                    word = contractions[word]\n",
    "\n",
    "                if PoS_tag[0].upper() in 'JNVR':\n",
    "                    word = lemmatizer.lemmatize(word, convert_pos_wordnet(PoS_tag))\n",
    "                else:\n",
    "                    word = lemmatizer.lemmatize(word)\n",
    "\n",
    "                clean_words.append(word)\n",
    "\n",
    "            words = clean_words\n",
    "\n",
    "        if add_start_end_tokens:\n",
    "            words = ['<START>'] + words + ['<END>']\n",
    "        \n",
    "        for i in range(len(words)):\n",
    "            word = words[i]\n",
    "            if word in word_index_dict:\n",
    "                word_embed_vec = word_embeddings[word_index_dict[word],:]\n",
    "                if tracker == 0:\n",
    "                    text_matrix = word_embed_vec\n",
    "                else:\n",
    "                    text_matrix = np.vstack((text_matrix, word_embed_vec))\n",
    "                    \n",
    "                # only increment if we have come across a word in the embeddings dictionary\n",
    "                tracker += 1\n",
    "                    \n",
    "        for j in range(len(text_vec)):\n",
    "            text_vec[j] = text_matrix[:,j].mean()\n",
    "            \n",
    "        if k == 0:\n",
    "            full_matrix = text_vec\n",
    "        else:\n",
    "            full_matrix = np.vstack((full_matrix, text_vec))\n",
    "            \n",
    "    return full_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of word embeddings\n",
    "emb = {'A': A.copy(), # context word embeddings\n",
    "       'ST': S_T.copy()} # target word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(emb)):\n",
    "    keys = list(emb.keys())\n",
    "    key = keys[i]\n",
    "    embeddings = emb[key]\n",
    "    \n",
    "    # get tweet embeddings from word embeddings\n",
    "    X = get_text_vectors(embeddings, vocabulary_dict, tweets['Tweet'])\n",
    "    \n",
    "    file_name = 'tweet_embed_{}.npy'.format(key)\n",
    "    np.save(file_name, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
