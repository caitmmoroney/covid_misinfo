{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Word Context Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.decomposition import FastICA, TruncatedSVD, PCA, NMF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to convert contractions picked up by word_tokenize() into full words\n",
    "contractions = {\n",
    "    \"n't\": 'not',\n",
    "    \"'ve\": 'have',\n",
    "    \"'s\": 'is', # note that this will include possessive nouns\n",
    "    'gonna': 'going to',\n",
    "    'gotta': 'got to',\n",
    "    \"'d\": 'would',\n",
    "    \"'ll\": 'will',\n",
    "    \"'re\": 'are',\n",
    "    \"'m\": 'am',\n",
    "    'wanna': 'want to'\n",
    "}\n",
    "\n",
    "# to convert nltk_pos tags to wordnet-compatible PoS tags\n",
    "def convert_pos_wordnet(tag):\n",
    "    tag_abbr = tag[0].upper()\n",
    "    tag_dict = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "                \n",
    "    if tag_abbr in tag_dict:\n",
    "        return tag_dict[tag_abbr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextMatrix(TransformerMixin):\n",
    "    \n",
    "    # initialize class & private variables\n",
    "    def __init__(self,\n",
    "                 window_size = 4,\n",
    "                 remove_stopwords = True,\n",
    "                 add_start_end_tokens = True,\n",
    "                 lowercase = False,\n",
    "                 lemmatize = False,\n",
    "                 pmi = False,\n",
    "                 spmi_k = 1,\n",
    "                 laplace_smoothing = 0,\n",
    "                 pmi_positive = False,\n",
    "                 sppmi_k = 1):\n",
    "        \n",
    "        \"\"\" Params:\n",
    "                window_size: size of +/- context window (default = 4)\n",
    "                remove_stopwords: boolean, whether or not to remove NLTK English stopwords\n",
    "                add_start_end_tokens: boolean, whether or not to append <START> and <END> to the\n",
    "                beginning/end of each document in the corpus (default = True)\n",
    "                lowercase: boolean, whether or not to convert words to all lowercase\n",
    "                lemmatize: boolean, whether or not to lemmatize input text\n",
    "                pmi: boolean, whether or not to compute pointwise mutual information\n",
    "                pmi_positive: boolean, whether or not to compute positive PMI\n",
    "        \"\"\"\n",
    "        self.window_size = window_size\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.add_start_end_tokens = add_start_end_tokens\n",
    "        self.lowercase = lowercase\n",
    "        self.lemmatize = lemmatize\n",
    "        self.pmi = pmi\n",
    "        self.spmi_k = spmi_k\n",
    "        self.laplace_smoothing = laplace_smoothing\n",
    "        self.pmi_positive = pmi_positive\n",
    "        self.sppmi_k = sppmi_k\n",
    "        self.corpus = None\n",
    "        self.clean_corpus = None\n",
    "        self.vocabulary = None\n",
    "        self.X = None\n",
    "        self.doc_terms_lists = None\n",
    "    \n",
    "    def fit(self, corpus, y = None):\n",
    "        \n",
    "        \"\"\" Learn the dictionary of all unique tokens for given corpus.\n",
    "        \n",
    "            Params:\n",
    "                corpus: list of strings\n",
    "            \n",
    "            Returns: self\n",
    "        \"\"\"\n",
    "        self.corpus = corpus\n",
    "        \n",
    "        term_dict = dict()\n",
    "        k = 0\n",
    "        corpus_words = []\n",
    "        clean_corpus = []\n",
    "        doc_terms_lists = []\n",
    "        detokenizer = TreebankWordDetokenizer()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        for text in corpus:\n",
    "            text = re.sub(r'[_~`@$%^&*[\\]+=\\|}{\\\"\\'<>/]+', '', text)\n",
    "            \n",
    "            words = word_tokenize(text)\n",
    "            \n",
    "            if self.remove_stopwords:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    if word.lower() not in set(stopwords.words('english')):\n",
    "                        clean_words.append(word)\n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lowercase:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    clean_words.append(word.lower())\n",
    "                \n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lemmatize:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    PoS_tag = pos_tag([word])[0][1]\n",
    "                    \n",
    "                    # to change contractions to full word form\n",
    "                    if word in contractions:\n",
    "                        word = contractions[word]\n",
    "\n",
    "                    if PoS_tag[0].upper() in 'JNVR':\n",
    "                        word = lemmatizer.lemmatize(word, convert_pos_wordnet(PoS_tag))\n",
    "                    else:\n",
    "                        word = lemmatizer.lemmatize(word)\n",
    "\n",
    "                    clean_words.append(word)\n",
    "                    \n",
    "                words = clean_words\n",
    "            \n",
    "            # detokenize trick taken from this StackOverflow post:\n",
    "            # https://stackoverflow.com/questions/21948019/python-untokenize-a-sentence\n",
    "            # and NLTK treebank documentation:\n",
    "            # https://www.nltk.org/_modules/nltk/tokenize/treebank.html\n",
    "            text = detokenizer.detokenize(words)\n",
    "            clean_corpus.append(text)\n",
    "            \n",
    "            [corpus_words.append(word) for word in words]\n",
    "            \n",
    "            if self.add_start_end_tokens:\n",
    "                words = ['<START>'] + words + ['<END>']\n",
    "            \n",
    "            doc_terms_lists.append(words)\n",
    "            \n",
    "        self.clean_corpus = clean_corpus\n",
    "        \n",
    "        self.doc_terms_lists = doc_terms_lists\n",
    "        \n",
    "        corpus_words = list(set(corpus_words))\n",
    "        \n",
    "        if self.add_start_end_tokens:\n",
    "            corpus_words = ['<START>'] + corpus_words + ['<END>']\n",
    "        \n",
    "        corpus_words = sorted(corpus_words)\n",
    "        \n",
    "        for el in corpus_words:\n",
    "            term_dict[el] = k\n",
    "            k += 1\n",
    "            \n",
    "        self.vocabulary = term_dict\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def transform(self, new_corpus = None, y = None):\n",
    "        \n",
    "        \"\"\" Compute the co-occurrence matrix for given corpus and window_size, using term dictionary\n",
    "            obtained with fit method.\n",
    "        \n",
    "            Returns: term-context co-occurrence matrix (shape: target terms by context terms) with\n",
    "            raw counts\n",
    "        \"\"\"\n",
    "        num_terms = len(self.vocabulary)\n",
    "        window = self.window_size\n",
    "        X = np.full((num_terms, num_terms), self.laplace_smoothing)\n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        if type(new_corpus) != list:\n",
    "            new_corpus = self.corpus\n",
    "        \n",
    "        for text in new_corpus:\n",
    "            text = re.sub(r'[_~`@$%^&*[\\]+=\\|}{\\\"\\'<>/]+', '', text)\n",
    "            \n",
    "            words = word_tokenize(text)\n",
    "            \n",
    "            if self.remove_stopwords:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    if word.lower() not in set(stopwords.words('english')):\n",
    "                        clean_words.append(word)\n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lowercase:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    clean_words.append(word.lower())\n",
    "                \n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lemmatize:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    PoS_tag = pos_tag([word])[0][1]\n",
    "                    \n",
    "                    # to change contractions to full word form\n",
    "                    if word in contractions:\n",
    "                        word = contractions[word]\n",
    "\n",
    "                    if PoS_tag[0].upper() in 'JNVR':\n",
    "                        word = lemmatizer.lemmatize(word, convert_pos_wordnet(PoS_tag))\n",
    "                    else:\n",
    "                        word = lemmatizer.lemmatize(word)\n",
    "\n",
    "                    clean_words.append(word)\n",
    "                    \n",
    "                words = clean_words\n",
    "                \n",
    "            if self.add_start_end_tokens:\n",
    "                words = ['<START>'] + words + ['<END>']\n",
    "            \n",
    "            for i in range(len(words)):\n",
    "                target = words[i]\n",
    "                \n",
    "                # check to see if target word is in the dictionary; if not, skip\n",
    "                if target in self.vocabulary:\n",
    "                    \n",
    "                    # grab index from dictionary\n",
    "                    target_dict_index = self.vocabulary[target]\n",
    "                    \n",
    "                    # find left-most and right-most window indices for each target word\n",
    "                    left_end_index = max(i - window, 0)\n",
    "                    right_end_index = min(i + window, len(words) - 1)\n",
    "                    \n",
    "                    # loop over all words within window\n",
    "                    # NOTE: this will include the target word; make sure to skip over it\n",
    "                    for j in range(left_end_index, right_end_index + 1):\n",
    "                        \n",
    "                        # skip \"context word\" where the \"context word\" index is equal to the\n",
    "                        # target word index\n",
    "                        if j != i:\n",
    "                            context_word = words[j]\n",
    "                            \n",
    "                            # check to see if context word is in the fitted dictionary; if\n",
    "                            # not, skip\n",
    "                            if context_word in self.vocabulary:\n",
    "                                X[target_dict_index, self.vocabulary[context_word]] += 1\n",
    "        \n",
    "        # if pmi = True, compute pmi matrix from word-context raw frequencies\n",
    "        # more concise code taken from this StackOverflow post:\n",
    "        # https://stackoverflow.com/questions/58701337/how-to-construct-ppmi-matrix-from-a-text-corpus\n",
    "        if self.pmi:\n",
    "            denom = X.sum()\n",
    "            col_sums = X.sum(axis = 0)\n",
    "            row_sums = X.sum(axis = 1)\n",
    "            \n",
    "            expected = np.outer(row_sums, col_sums)/denom\n",
    "            \n",
    "            X = X/expected\n",
    "            \n",
    "            for i in range(X.shape[0]):\n",
    "                for j in range(X.shape[1]):\n",
    "                \n",
    "                    if X[i,j] > 0:\n",
    "                        X[i,j] = np.log(X[i,j]) - np.log(self.spmi_k)\n",
    "                        \n",
    "                        if self.pmi_positive:\n",
    "                            X[i,j] = max(X[i,j] - np.log(self.sppmi_k), 0)\n",
    "        \n",
    "        # note that X is a dense matrix\n",
    "        self.X = X\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ContextMatrix(lowercase = True, lemmatize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [\n",
    "    \"Coronavirus is a fake liberal hoax.\",\n",
    "    \"Trump won't do anything about coronavirus.\",\n",
    "    \"The liberal fake news media always blame Pres Trump.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ContextMatrix at 0x1a2147bf10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.fit(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.</th>\n",
       "      <th>&lt;END&gt;</th>\n",
       "      <th>&lt;START&gt;</th>\n",
       "      <th>always</th>\n",
       "      <th>anything</th>\n",
       "      <th>blame</th>\n",
       "      <th>coronavirus</th>\n",
       "      <th>fake</th>\n",
       "      <th>hoax</th>\n",
       "      <th>liberal</th>\n",
       "      <th>medium</th>\n",
       "      <th>news</th>\n",
       "      <th>pres</th>\n",
       "      <th>trump</th>\n",
       "      <th>wont</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;END&gt;</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;START&gt;</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>always</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anything</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blame</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coronavirus</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fake</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hoax</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liberal</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medium</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pres</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wont</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             .  <END>  <START>  always  anything  blame  coronavirus  fake  \\\n",
       ".            0      3        0       1         1      1            2     1   \n",
       "<END>        3      0        0       0         1      1            1     1   \n",
       "<START>      0      0        0       0         1      0            2     2   \n",
       "always       1      0        0       0         0      1            0     1   \n",
       "anything     1      1        1       0         0      0            1     0   \n",
       "blame        1      1        0       1         0      0            0     1   \n",
       "coronavirus  2      1        2       0         1      0            0     1   \n",
       "fake         1      1        2       1         0      1            1     0   \n",
       "hoax         1      1        1       0         0      0            1     1   \n",
       "liberal      1      1        2       1         0      0            1     2   \n",
       "medium       0      0        1       1         0      1            0     1   \n",
       "news         0      0        1       1         0      1            0     1   \n",
       "pres         1      1        0       1         0      1            0     0   \n",
       "trump        2      1        1       1         1      1            1     0   \n",
       "wont         1      1        1       0         1      0            1     0   \n",
       "\n",
       "             hoax  liberal  medium  news  pres  trump  wont  \n",
       ".               1        1       0     0     1      2     1  \n",
       "<END>           1        1       0     0     1      1     1  \n",
       "<START>         1        2       1     1     0      1     1  \n",
       "always          0        1       1     1     1      1     0  \n",
       "anything        0        0       0     0     0      1     1  \n",
       "blame           0        0       1     1     1      1     0  \n",
       "coronavirus     1        1       0     0     0      1     1  \n",
       "fake            1        2       1     1     0      0     0  \n",
       "hoax            0        1       0     0     0      0     0  \n",
       "liberal         1        0       1     1     0      0     0  \n",
       "medium          0        1       0     1     1      1     0  \n",
       "news            0        1       1     0     1      0     0  \n",
       "pres            0        0       1     1     0      1     0  \n",
       "trump           0        0       1     0     1      0     1  \n",
       "wont            0        0       0     0     0      1     0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cm.transform(tweets), index = cm.vocabulary, columns = cm.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coronavirus fake liberal hoax.',\n",
       " 'trump wont anything coronavirus.',\n",
       " 'liberal fake news medium always blame pres trump.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.clean_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Coronavirus is a fake liberal hoax.',\n",
       " \"Trump won't do anything about coronavirus.\",\n",
       " 'The liberal fake news media always blame Pres Trump.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train embeddings using tweets as corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('COVID19_Dataset-text_labels_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Is_Unreliable</th>\n",
       "      <th>Category</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 3, 6, 9</td>\n",
       "      <td>We are living in scary times in Canada. Gov’t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 6, 8, 9</td>\n",
       "      <td>Just as bad in Canada. In fact, our government...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 4, 9</td>\n",
       "      <td>It was only a matter of time before the mainst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6, 8</td>\n",
       "      <td>Russia's taking no chances: Foreigners infecte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>6, 8, 9</td>\n",
       "      <td>Although there is now a presumptive confirmed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BREAKING: Harvard classes will move online sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Singularity University is hosting a FREE Virtu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coronavirus: how does it spread and what are t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stanford just cancelled classes for the rest o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tech conferences were cancelled in #Waterloo R...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Is_Unreliable    Category  \\\n",
       "0                1  1, 3, 6, 9   \n",
       "1                1  1, 6, 8, 9   \n",
       "2                1     1, 4, 9   \n",
       "3                1        6, 8   \n",
       "4                1     6, 8, 9   \n",
       "..             ...         ...   \n",
       "555              0         NaN   \n",
       "556              0         NaN   \n",
       "557              0         NaN   \n",
       "558              0         NaN   \n",
       "559              0         NaN   \n",
       "\n",
       "                                                 Tweet  \n",
       "0    We are living in scary times in Canada. Gov’t ...  \n",
       "1    Just as bad in Canada. In fact, our government...  \n",
       "2    It was only a matter of time before the mainst...  \n",
       "3    Russia's taking no chances: Foreigners infecte...  \n",
       "4    Although there is now a presumptive confirmed ...  \n",
       "..                                                 ...  \n",
       "555  BREAKING: Harvard classes will move online sta...  \n",
       "556  Singularity University is hosting a FREE Virtu...  \n",
       "557  Coronavirus: how does it spread and what are t...  \n",
       "558  Stanford just cancelled classes for the rest o...  \n",
       "559  Tech conferences were cancelled in #Waterloo R...  \n",
       "\n",
       "[560 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ContextMatrix(window_size = 15, lowercase = True, lemmatize = True, laplace_smoothing = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ContextMatrix at 0x1a28febb50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.fit(tweets['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_context_matrix = cm.transform(tweets['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>#</th>\n",
       "      <th>(</th>\n",
       "      <th>)</th>\n",
       "      <th>,</th>\n",
       "      <th>-</th>\n",
       "      <th>--</th>\n",
       "      <th>.</th>\n",
       "      <th>...</th>\n",
       "      <th>1</th>\n",
       "      <th>...</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoomer</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>—</th>\n",
       "      <th>‘</th>\n",
       "      <th>’</th>\n",
       "      <th>“</th>\n",
       "      <th>”</th>\n",
       "      <th>❝real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>126</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <td>32</td>\n",
       "      <td>652</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>115</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>217</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>65</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>)</th>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>12</td>\n",
       "      <td>115</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>140</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>159</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>47</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‘</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>’</th>\n",
       "      <td>31</td>\n",
       "      <td>65</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>47</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>88</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>62</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>“</th>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>”</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>❝real</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2325 rows × 2325 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         !    #   (   )    ,   -  --    .  ...  1  ...  zombie  zone  zoomer  \\\n",
       "!      126   32   2   3   12   5   2   39    9  5  ...       2     2       2   \n",
       "#       32  652  11  11  115  21   5  217    3  3  ...       2     3       3   \n",
       "(        2   11   4  34   15   3   2   19    3  2  ...       2     2       2   \n",
       ")        3   11  34   4   15   3   2   22    3  2  ...       2     2       2   \n",
       ",       12  115  15  15  140  14   6  159    5  6  ...       5     3       2   \n",
       "...    ...  ...  ..  ..  ...  ..  ..  ...  ... ..  ...     ...   ...     ...   \n",
       "‘        2    6   2   2    5   2   2    6    2  2  ...       2     2       2   \n",
       "’       31   65   6   6   47   8   2   88    7  2  ...       2     2       2   \n",
       "“       10    7   3   3   17   5   3   19    3  3  ...       2     2       2   \n",
       "”        4    6   2   2   10   4   3   15    3  2  ...       2     2       2   \n",
       "❝real    3    2   2   2    2   2   2    2    2  2  ...       2     2       2   \n",
       "\n",
       "       zuckerberg  —   ‘   ’   “   ”  ❝real  \n",
       "!               2  3   2  31  10   4      3  \n",
       "#               2  4   6  65   7   6      2  \n",
       "(               2  2   2   6   3   2      2  \n",
       ")               2  2   2   6   3   2      2  \n",
       ",               2  4   5  47  17  10      2  \n",
       "...           ... ..  ..  ..  ..  ..    ...  \n",
       "‘               2  2   2  11   2   2      2  \n",
       "’               2  2  11  62  15  14      3  \n",
       "“               2  3   2  15   6  22      2  \n",
       "”               2  3   2  14  22  10      2  \n",
       "❝real           2  2   2   3   2   2      2  \n",
       "\n",
       "[2325 rows x 2325 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(word_context_matrix, index = cm.vocabulary, columns = cm.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2325, 2325)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_context_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica = FastICA(n_components = 2)\n",
    "std_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03405123, -0.04369647],\n",
       "       [-0.78315548,  0.58870785],\n",
       "       [-0.00816671, -0.02716965],\n",
       "       ...,\n",
       "       [-0.00509353, -0.0307597 ],\n",
       "       [-0.002974  , -0.02261885],\n",
       "       [ 0.0014945 ,  0.0020318 ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_std = std_scaler.fit_transform(word_context_matrix)\n",
    "\n",
    "matrix = ica.fit_transform(X_std)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comp 1</th>\n",
       "      <th>Comp 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>-0.034051</td>\n",
       "      <td>-0.043696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <td>-0.783155</td>\n",
       "      <td>0.588708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(</th>\n",
       "      <td>-0.008167</td>\n",
       "      <td>-0.027170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>)</th>\n",
       "      <td>-0.008134</td>\n",
       "      <td>-0.028568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>-0.116608</td>\n",
       "      <td>-0.258123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‘</th>\n",
       "      <td>-0.000735</td>\n",
       "      <td>-0.004928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>’</th>\n",
       "      <td>-0.070475</td>\n",
       "      <td>-0.087377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>“</th>\n",
       "      <td>-0.005094</td>\n",
       "      <td>-0.030760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>”</th>\n",
       "      <td>-0.002974</td>\n",
       "      <td>-0.022619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>❝real</th>\n",
       "      <td>0.001494</td>\n",
       "      <td>0.002032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2325 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Comp 1    Comp 2\n",
       "!     -0.034051 -0.043696\n",
       "#     -0.783155  0.588708\n",
       "(     -0.008167 -0.027170\n",
       ")     -0.008134 -0.028568\n",
       ",     -0.116608 -0.258123\n",
       "...         ...       ...\n",
       "‘     -0.000735 -0.004928\n",
       "’     -0.070475 -0.087377\n",
       "“     -0.005094 -0.030760\n",
       "”     -0.002974 -0.022619\n",
       "❝real  0.001494  0.002032\n",
       "\n",
       "[2325 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(matrix,\n",
    "                  index = cm.vocabulary,\n",
    "                  columns = ['Comp {}'.format(i+1) for i in range(2)])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caitlinmoroney/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 62222 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/caitlinmoroney/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 62222 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/caitlinmoroney/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 12540 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/caitlinmoroney/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 12540 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = [key for key in cm.vocabulary.keys()]\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    \n",
    "    #if re.match('^co[rv]', word):\n",
    "    \n",
    "        x = df['Comp 1'][i]\n",
    "        y = df['Comp 2'][i]\n",
    "\n",
    "        plt.scatter(x, y)\n",
    "        plt.text(x, y, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set word embeddings for text classification\n",
    "ica = FastICA(n_components = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00078142,  0.00181308,  0.00926719, ..., -0.00151034,\n",
       "        -0.0009899 ,  0.00083232],\n",
       "       [ 0.00216537, -0.00332425,  0.00114032, ..., -0.00133744,\n",
       "         0.00225807,  0.00067013],\n",
       "       [ 0.00019539, -0.00281232, -0.00489027, ..., -0.00058207,\n",
       "        -0.00323997, -0.02003836],\n",
       "       ...,\n",
       "       [ 0.00020389,  0.00625625,  0.00512302, ...,  0.0005339 ,\n",
       "         0.00712531,  0.00790296],\n",
       "       [ 0.00147813,  0.05396281,  0.00682261, ...,  0.0039039 ,\n",
       "        -0.01109735, -0.00180813],\n",
       "       [ 0.00310358, -0.00078365, -0.00245433, ..., -0.00827086,\n",
       "        -0.00182696,  0.00215122]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = ica.fit_transform(X_std)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out target\n",
    "y = tweets['Is_Unreliable']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derive text vectors from word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_vectors(word_embeddings,\n",
    "                     word_index_dict,\n",
    "                     text_list,\n",
    "                     remove_stopwords = True,\n",
    "                     lowercase = True,\n",
    "                     lemmatize = True,\n",
    "                     add_start_end_tokens = True):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    for k in range(len(text_list)):\n",
    "        text = text_list[k]\n",
    "        text = re.sub(r'[_~`@$%^&*[\\]+=\\|}{\\\"\\'<>/]+', '', text)\n",
    "        text_vec = np.zeros(word_embeddings.shape[1])\n",
    "        words = word_tokenize(text)\n",
    "        tracker = 0 # to track whether we've encountered a word for which we have an embedding (in each tweet)\n",
    "        \n",
    "        if remove_stopwords:\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                if word.lower() not in set(stopwords.words('english')):\n",
    "                    clean_words.append(word)\n",
    "            words = clean_words\n",
    "\n",
    "        if lowercase:\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                clean_words.append(word.lower())\n",
    "\n",
    "            words = clean_words\n",
    "\n",
    "        if lemmatize:\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                PoS_tag = pos_tag([word])[0][1]\n",
    "\n",
    "                # to change contractions to full word form\n",
    "                if word in contractions:\n",
    "                    word = contractions[word]\n",
    "\n",
    "                if PoS_tag[0].upper() in 'JNVR':\n",
    "                    word = lemmatizer.lemmatize(word, convert_pos_wordnet(PoS_tag))\n",
    "                else:\n",
    "                    word = lemmatizer.lemmatize(word)\n",
    "\n",
    "                clean_words.append(word)\n",
    "\n",
    "            words = clean_words\n",
    "\n",
    "        if add_start_end_tokens:\n",
    "            words = ['<START>'] + words + ['<END>']\n",
    "        \n",
    "        for i in range(len(words)):\n",
    "            word = words[i]\n",
    "            if word in word_index_dict:\n",
    "                word_embed_vec = word_embeddings[word_index_dict[word],:]\n",
    "                if tracker == 0:\n",
    "                    text_matrix = word_embed_vec\n",
    "                else:\n",
    "                    text_matrix = np.vstack((text_matrix, word_embed_vec))\n",
    "                    \n",
    "                # only increment if we have come across a word in the embeddings dictionary\n",
    "                tracker += 1\n",
    "                    \n",
    "        for j in range(len(text_vec)):\n",
    "            text_vec[j] = text_matrix[:,j].mean()\n",
    "            \n",
    "        if k == 0:\n",
    "            full_matrix = text_vec\n",
    "        else:\n",
    "            full_matrix = np.vstack((full_matrix, text_vec))\n",
    "            \n",
    "    return full_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_text_vectors(embeddings, cm.vocabulary, tweets['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(560, 250)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.36563309e-04,  6.07122644e-03, -2.43461627e-03, ...,\n",
       "        -4.50324063e-03, -2.40903296e-03,  1.01876062e-03],\n",
       "       [-7.49162427e-04, -1.94610348e-02, -4.00533647e-03, ...,\n",
       "        -8.94095420e-03, -1.90439945e-03,  1.32525058e-03],\n",
       "       [-1.69660525e-04, -2.66493589e-04, -7.15905653e-04, ...,\n",
       "        -2.03171834e-03, -7.29394861e-03, -9.52104069e-04],\n",
       "       ...,\n",
       "       [ 2.22931587e-03, -7.68175240e-03, -3.35036277e-03, ...,\n",
       "         1.02842674e-03, -2.21341626e-03,  9.68486110e-04],\n",
       "       [ 1.10152172e-03, -5.23994664e-03, -4.94534676e-04, ...,\n",
       "         2.41898127e-04,  1.80856983e-03,  1.78603202e-03],\n",
       "       [ 8.32653320e-05, -1.54935579e-03,  5.36298548e-04, ...,\n",
       "        -3.97421348e-03,  9.31078219e-05, -1.27060275e-03]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification - nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, KFold, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "pipe = Pipeline([\n",
    "    ('classify', SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC hyperparams to optimize\n",
    "kernel = ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "C = [0.001, 0.01, 0.1, 1, 10]\n",
    "# set up parameter grid\n",
    "params = {\n",
    "    'classify__kernel': kernel,\n",
    "    'classify__C': C\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CV scheme for inner and outer loops\n",
    "inner_cv = KFold(n_splits = 3, shuffle = True, random_state = 1)\n",
    "outer_cv = KFold(n_splits = 5, shuffle = True, random_state = 1)\n",
    "\n",
    "# Set up GridSearch for inner loop\n",
    "grid_SVC = GridSearchCV(pipe, params, cv = inner_cv)\n",
    "#grid_SVC.fit(X, y)\n",
    "\n",
    "# Nested CV scores\n",
    "scores = cross_validate(grid_SVC,\n",
    "                        X = X,\n",
    "                        y = y,\n",
    "                        cv = outer_cv,\n",
    "                        scoring = ['roc_auc', 'accuracy', 'f1', 'precision', 'recall'],\n",
    "                        return_estimator = True)\n",
    "auc = scores['test_roc_auc']\n",
    "accuracy = scores['test_accuracy']\n",
    "f1 = scores['test_f1']\n",
    "precision = scores['test_precision']\n",
    "recall = scores['test_recall']\n",
    "estimators = scores['estimator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9180614591512295"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8642857142857144"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8695658356988281"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8300921558849389"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9135396754598359"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classify__C': 10, 'classify__kernel': 'rbf'}\n",
      "\n",
      "\n",
      "{'classify__C': 10, 'classify__kernel': 'rbf'}\n",
      "\n",
      "\n",
      "{'classify__C': 1, 'classify__kernel': 'rbf'}\n",
      "\n",
      "\n",
      "{'classify__C': 10, 'classify__kernel': 'linear'}\n",
      "\n",
      "\n",
      "{'classify__C': 1, 'classify__kernel': 'rbf'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in estimators:\n",
    "    print(i.best_params_)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
