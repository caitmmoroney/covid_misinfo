{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Word Context Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.decomposition import FastICA, TruncatedSVD, PCA, NMF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to convert contractions picked up by word_tokenize() into full words\n",
    "contractions = {\n",
    "    \"n't\": 'not',\n",
    "    \"'ve\": 'have',\n",
    "    \"'s\": 'is', # note that this will include possessive nouns\n",
    "    'gonna': 'going to',\n",
    "    'gotta': 'got to',\n",
    "    \"'d\": 'would',\n",
    "    \"'ll\": 'will',\n",
    "    \"'re\": 'are',\n",
    "    \"'m\": 'am',\n",
    "    'wanna': 'want to'\n",
    "}\n",
    "\n",
    "# to convert nltk_pos tags to wordnet-compatible PoS tags\n",
    "def convert_pos_wordnet(tag):\n",
    "    tag_abbr = tag[0].upper()\n",
    "    tag_dict = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "                \n",
    "    if tag_abbr in tag_dict:\n",
    "        return tag_dict[tag_abbr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextMatrix(TransformerMixin):\n",
    "    \n",
    "    # initialize class & private variables\n",
    "    def __init__(self,\n",
    "                 window_size = 4,\n",
    "                 remove_stopwords = True,\n",
    "                 add_start_end_tokens = True,\n",
    "                 lowercase = False,\n",
    "                 lemmatize = False,\n",
    "                 pmi = False,\n",
    "                 spmi_k = 1,\n",
    "                 laplace_smoothing = 0,\n",
    "                 pmi_positive = False,\n",
    "                 sppmi_k = 1):\n",
    "        \n",
    "        \"\"\" Params:\n",
    "                window_size: size of +/- context window (default = 4)\n",
    "                remove_stopwords: boolean, whether or not to remove NLTK English stopwords\n",
    "                add_start_end_tokens: boolean, whether or not to append <START> and <END> to the\n",
    "                beginning/end of each document in the corpus (default = True)\n",
    "                lowercase: boolean, whether or not to convert words to all lowercase\n",
    "                lemmatize: boolean, whether or not to lemmatize input text\n",
    "                pmi: boolean, whether or not to compute pointwise mutual information\n",
    "                pmi_positive: boolean, whether or not to compute positive PMI\n",
    "        \"\"\"\n",
    "        self.window_size = window_size\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.add_start_end_tokens = add_start_end_tokens\n",
    "        self.lowercase = lowercase\n",
    "        self.lemmatize = lemmatize\n",
    "        self.pmi = pmi\n",
    "        self.spmi_k = spmi_k\n",
    "        self.laplace_smoothing = laplace_smoothing\n",
    "        self.pmi_positive = pmi_positive\n",
    "        self.sppmi_k = sppmi_k\n",
    "        self.corpus = None\n",
    "        self.clean_corpus = None\n",
    "        self.vocabulary = None\n",
    "        self.X = None\n",
    "        self.doc_terms_lists = None\n",
    "    \n",
    "    def fit(self, corpus, y = None):\n",
    "        \n",
    "        \"\"\" Learn the dictionary of all unique tokens for given corpus.\n",
    "        \n",
    "            Params:\n",
    "                corpus: list of strings\n",
    "            \n",
    "            Returns: self\n",
    "        \"\"\"\n",
    "        self.corpus = corpus\n",
    "        \n",
    "        term_dict = dict()\n",
    "        k = 0\n",
    "        corpus_words = []\n",
    "        clean_corpus = []\n",
    "        doc_terms_lists = []\n",
    "        detokenizer = TreebankWordDetokenizer()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        for text in corpus:\n",
    "            text = re.sub(r'[_~`@$%^&*[\\]+=\\|}{\\\"\\'<>/]+', '', text)\n",
    "            \n",
    "            words = word_tokenize(text)\n",
    "            \n",
    "            if self.remove_stopwords:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    if word.lower() not in set(stopwords.words('english')):\n",
    "                        clean_words.append(word)\n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lowercase:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    clean_words.append(word.lower())\n",
    "                \n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lemmatize:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    PoS_tag = pos_tag([word])[0][1]\n",
    "                    \n",
    "                    # to change contractions to full word form\n",
    "                    if word in contractions:\n",
    "                        word = contractions[word]\n",
    "\n",
    "                    if PoS_tag[0].upper() in 'JNVR':\n",
    "                        word = lemmatizer.lemmatize(word, convert_pos_wordnet(PoS_tag))\n",
    "                    else:\n",
    "                        word = lemmatizer.lemmatize(word)\n",
    "\n",
    "                    clean_words.append(word)\n",
    "                    \n",
    "                words = clean_words\n",
    "            \n",
    "            # detokenize trick taken from this StackOverflow post:\n",
    "            # https://stackoverflow.com/questions/21948019/python-untokenize-a-sentence\n",
    "            # and NLTK treebank documentation:\n",
    "            # https://www.nltk.org/_modules/nltk/tokenize/treebank.html\n",
    "            text = detokenizer.detokenize(words)\n",
    "            clean_corpus.append(text)\n",
    "            \n",
    "            [corpus_words.append(word) for word in words]\n",
    "            \n",
    "            if self.add_start_end_tokens:\n",
    "                words = ['<START>'] + words + ['<END>']\n",
    "            \n",
    "            doc_terms_lists.append(words)\n",
    "            \n",
    "        self.clean_corpus = clean_corpus\n",
    "        \n",
    "        self.doc_terms_lists = doc_terms_lists\n",
    "        \n",
    "        corpus_words = list(set(corpus_words))\n",
    "        \n",
    "        if self.add_start_end_tokens:\n",
    "            corpus_words = ['<START>'] + corpus_words + ['<END>']\n",
    "        \n",
    "        corpus_words = sorted(corpus_words)\n",
    "        \n",
    "        for el in corpus_words:\n",
    "            term_dict[el] = k\n",
    "            k += 1\n",
    "            \n",
    "        self.vocabulary = term_dict\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def transform(self, new_corpus = None, y = None):\n",
    "        \n",
    "        \"\"\" Compute the co-occurrence matrix for given corpus and window_size, using term dictionary\n",
    "            obtained with fit method.\n",
    "        \n",
    "            Returns: term-context co-occurrence matrix (shape: target terms by context terms) with\n",
    "            raw counts\n",
    "        \"\"\"\n",
    "        num_terms = len(self.vocabulary)\n",
    "        window = self.window_size\n",
    "        X = np.full((num_terms, num_terms), self.laplace_smoothing)\n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        if type(new_corpus) != list:\n",
    "            new_corpus = self.corpus\n",
    "        \n",
    "        for text in new_corpus:\n",
    "            text = re.sub(r'[_~`@$%^&*[\\]+=\\|}{\\\"\\'<>/]+', '', text)\n",
    "            \n",
    "            words = word_tokenize(text)\n",
    "            \n",
    "            if self.remove_stopwords:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    if word.lower() not in set(stopwords.words('english')):\n",
    "                        clean_words.append(word)\n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lowercase:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    clean_words.append(word.lower())\n",
    "                \n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lemmatize:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    PoS_tag = pos_tag([word])[0][1]\n",
    "                    \n",
    "                    # to change contractions to full word form\n",
    "                    if word in contractions:\n",
    "                        word = contractions[word]\n",
    "\n",
    "                    if PoS_tag[0].upper() in 'JNVR':\n",
    "                        word = lemmatizer.lemmatize(word, convert_pos_wordnet(PoS_tag))\n",
    "                    else:\n",
    "                        word = lemmatizer.lemmatize(word)\n",
    "\n",
    "                    clean_words.append(word)\n",
    "                    \n",
    "                words = clean_words\n",
    "                \n",
    "            if self.add_start_end_tokens:\n",
    "                words = ['<START>'] + words + ['<END>']\n",
    "            \n",
    "            for i in range(len(words)):\n",
    "                target = words[i]\n",
    "                \n",
    "                # check to see if target word is in the dictionary; if not, skip\n",
    "                if target in self.vocabulary:\n",
    "                    \n",
    "                    # grab index from dictionary\n",
    "                    target_dict_index = self.vocabulary[target]\n",
    "                    \n",
    "                    # find left-most and right-most window indices for each target word\n",
    "                    left_end_index = max(i - window, 0)\n",
    "                    right_end_index = min(i + window, len(words) - 1)\n",
    "                    \n",
    "                    # loop over all words within window\n",
    "                    # NOTE: this will include the target word; make sure to skip over it\n",
    "                    for j in range(left_end_index, right_end_index + 1):\n",
    "                        \n",
    "                        # skip \"context word\" where the \"context word\" index is equal to the\n",
    "                        # target word index\n",
    "                        if j != i:\n",
    "                            context_word = words[j]\n",
    "                            \n",
    "                            # check to see if context word is in the fitted dictionary; if\n",
    "                            # not, skip\n",
    "                            if context_word in self.vocabulary:\n",
    "                                X[target_dict_index, self.vocabulary[context_word]] += 1\n",
    "        \n",
    "        # if pmi = True, compute pmi matrix from word-context raw frequencies\n",
    "        # more concise code taken from this StackOverflow post:\n",
    "        # https://stackoverflow.com/questions/58701337/how-to-construct-ppmi-matrix-from-a-text-corpus\n",
    "        if self.pmi:\n",
    "            denom = X.sum()\n",
    "            col_sums = X.sum(axis = 0)\n",
    "            row_sums = X.sum(axis = 1)\n",
    "            \n",
    "            expected = np.outer(row_sums, col_sums)/denom\n",
    "            \n",
    "            X = X/expected\n",
    "            \n",
    "            for i in range(X.shape[0]):\n",
    "                for j in range(X.shape[1]):\n",
    "                \n",
    "                    if X[i,j] > 0:\n",
    "                        X[i,j] = np.log(X[i,j]) - np.log(self.spmi_k)\n",
    "                        \n",
    "                        if self.pmi_positive:\n",
    "                            X[i,j] = max(X[i,j] - np.log(self.sppmi_k), 0)\n",
    "        \n",
    "        # note that X is a dense matrix\n",
    "        self.X = X\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ContextMatrix(lowercase = True, lemmatize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [\n",
    "    \"Coronavirus is a fake liberal hoax.\",\n",
    "    \"Trump won't do anything about coronavirus.\",\n",
    "    \"The liberal fake news media always blame Pres Trump.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ContextMatrix at 0x1a20aace90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.fit(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.</th>\n",
       "      <th>&lt;END&gt;</th>\n",
       "      <th>&lt;START&gt;</th>\n",
       "      <th>always</th>\n",
       "      <th>anything</th>\n",
       "      <th>blame</th>\n",
       "      <th>coronavirus</th>\n",
       "      <th>fake</th>\n",
       "      <th>hoax</th>\n",
       "      <th>liberal</th>\n",
       "      <th>medium</th>\n",
       "      <th>news</th>\n",
       "      <th>pres</th>\n",
       "      <th>trump</th>\n",
       "      <th>wont</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;END&gt;</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;START&gt;</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>always</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anything</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blame</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coronavirus</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fake</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hoax</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liberal</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medium</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pres</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wont</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             .  <END>  <START>  always  anything  blame  coronavirus  fake  \\\n",
       ".            0      3        0       1         1      1            2     1   \n",
       "<END>        3      0        0       0         1      1            1     1   \n",
       "<START>      0      0        0       0         1      0            2     2   \n",
       "always       1      0        0       0         0      1            0     1   \n",
       "anything     1      1        1       0         0      0            1     0   \n",
       "blame        1      1        0       1         0      0            0     1   \n",
       "coronavirus  2      1        2       0         1      0            0     1   \n",
       "fake         1      1        2       1         0      1            1     0   \n",
       "hoax         1      1        1       0         0      0            1     1   \n",
       "liberal      1      1        2       1         0      0            1     2   \n",
       "medium       0      0        1       1         0      1            0     1   \n",
       "news         0      0        1       1         0      1            0     1   \n",
       "pres         1      1        0       1         0      1            0     0   \n",
       "trump        2      1        1       1         1      1            1     0   \n",
       "wont         1      1        1       0         1      0            1     0   \n",
       "\n",
       "             hoax  liberal  medium  news  pres  trump  wont  \n",
       ".               1        1       0     0     1      2     1  \n",
       "<END>           1        1       0     0     1      1     1  \n",
       "<START>         1        2       1     1     0      1     1  \n",
       "always          0        1       1     1     1      1     0  \n",
       "anything        0        0       0     0     0      1     1  \n",
       "blame           0        0       1     1     1      1     0  \n",
       "coronavirus     1        1       0     0     0      1     1  \n",
       "fake            1        2       1     1     0      0     0  \n",
       "hoax            0        1       0     0     0      0     0  \n",
       "liberal         1        0       1     1     0      0     0  \n",
       "medium          0        1       0     1     1      1     0  \n",
       "news            0        1       1     0     1      0     0  \n",
       "pres            0        0       1     1     0      1     0  \n",
       "trump           0        0       1     0     1      0     1  \n",
       "wont            0        0       0     0     0      1     0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cm.transform(tweets), index = cm.vocabulary, columns = cm.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coronavirus fake liberal hoax.',\n",
       " 'trump wont anything coronavirus.',\n",
       " 'liberal fake news medium always blame pres trump.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.clean_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Coronavirus is a fake liberal hoax.',\n",
       " \"Trump won't do anything about coronavirus.\",\n",
       " 'The liberal fake news media always blame Pres Trump.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train embeddings using tweets as corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('COVID19_Dataset-text_labels_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Is_Unreliable</th>\n",
       "      <th>Category</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 3, 6, 9</td>\n",
       "      <td>We are living in scary times in Canada. Gov’t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 6, 8, 9</td>\n",
       "      <td>Just as bad in Canada. In fact, our government...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 4, 9</td>\n",
       "      <td>It was only a matter of time before the mainst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6, 8</td>\n",
       "      <td>Russia's taking no chances: Foreigners infecte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>6, 8, 9</td>\n",
       "      <td>Although there is now a presumptive confirmed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BREAKING: Harvard classes will move online sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Singularity University is hosting a FREE Virtu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coronavirus: how does it spread and what are t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stanford just cancelled classes for the rest o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tech conferences were cancelled in #Waterloo R...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Is_Unreliable    Category  \\\n",
       "0                1  1, 3, 6, 9   \n",
       "1                1  1, 6, 8, 9   \n",
       "2                1     1, 4, 9   \n",
       "3                1        6, 8   \n",
       "4                1     6, 8, 9   \n",
       "..             ...         ...   \n",
       "555              0         NaN   \n",
       "556              0         NaN   \n",
       "557              0         NaN   \n",
       "558              0         NaN   \n",
       "559              0         NaN   \n",
       "\n",
       "                                                 Tweet  \n",
       "0    We are living in scary times in Canada. Gov’t ...  \n",
       "1    Just as bad in Canada. In fact, our government...  \n",
       "2    It was only a matter of time before the mainst...  \n",
       "3    Russia's taking no chances: Foreigners infecte...  \n",
       "4    Although there is now a presumptive confirmed ...  \n",
       "..                                                 ...  \n",
       "555  BREAKING: Harvard classes will move online sta...  \n",
       "556  Singularity University is hosting a FREE Virtu...  \n",
       "557  Coronavirus: how does it spread and what are t...  \n",
       "558  Stanford just cancelled classes for the rest o...  \n",
       "559  Tech conferences were cancelled in #Waterloo R...  \n",
       "\n",
       "[560 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ContextMatrix(window_size = 15,\n",
    "                   lowercase = True,\n",
    "                   lemmatize = True,\n",
    "                   pmi = True,\n",
    "                   pmi_positive = True,\n",
    "                   sppmi_k = 5,\n",
    "                   laplace_smoothing = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ContextMatrix at 0x1a28625ad0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.fit(tweets['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_context_matrix = cm.transform(tweets['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>#</th>\n",
       "      <th>(</th>\n",
       "      <th>)</th>\n",
       "      <th>,</th>\n",
       "      <th>-</th>\n",
       "      <th>--</th>\n",
       "      <th>.</th>\n",
       "      <th>...</th>\n",
       "      <th>1</th>\n",
       "      <th>...</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoomer</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>—</th>\n",
       "      <th>‘</th>\n",
       "      <th>’</th>\n",
       "      <th>“</th>\n",
       "      <th>”</th>\n",
       "      <th>❝real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>2.424239</td>\n",
       "      <td>0.173120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.377325</td>\n",
       "      <td>0.038504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.798712</td>\n",
       "      <td>0.025324</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <td>0.173120</td>\n",
       "      <td>2.360850</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.863298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.258224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.699605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.510181</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.510181</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.863298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.306332</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.190300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.614122</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‘</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058733</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>’</th>\n",
       "      <td>0.798712</td>\n",
       "      <td>0.699605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.614122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.007236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058733</td>\n",
       "      <td>1.309977</td>\n",
       "      <td>0.268746</td>\n",
       "      <td>0.238795</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>“</th>\n",
       "      <td>0.025324</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.268746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.148953</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>”</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.238795</td>\n",
       "      <td>1.148953</td>\n",
       "      <td>0.345813</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>❝real</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2325 rows × 2325 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              !         #         (         )         ,    -   --         .  \\\n",
       "!      2.424239  0.173120  0.000000  0.000000  0.000000  0.0  0.0  0.377325   \n",
       "#      0.173120  2.360850  0.000000  0.000000  0.863298  0.0  0.0  1.258224   \n",
       "(      0.000000  0.000000  0.000000  1.510181  0.000000  0.0  0.0  0.000000   \n",
       ")      0.000000  0.000000  1.510181  0.000000  0.000000  0.0  0.0  0.000000   \n",
       ",      0.000000  0.863298  0.000000  0.000000  1.306332  0.0  0.0  1.190300   \n",
       "...         ...       ...       ...       ...       ...  ...  ...       ...   \n",
       "‘      0.000000  0.000000  0.000000  0.000000  0.000000  0.0  0.0  0.000000   \n",
       "’      0.798712  0.699605  0.000000  0.000000  0.614122  0.0  0.0  1.007236   \n",
       "“      0.025324  0.000000  0.000000  0.000000  0.000000  0.0  0.0  0.000000   \n",
       "”      0.000000  0.000000  0.000000  0.000000  0.000000  0.0  0.0  0.000000   \n",
       "❝real  0.000000  0.000000  0.000000  0.000000  0.000000  0.0  0.0  0.000000   \n",
       "\n",
       "            ...    1  ...  zombie  zone  zoomer  zuckerberg    —         ‘  \\\n",
       "!      0.038504  0.0  ...     0.0   0.0     0.0         0.0  0.0  0.000000   \n",
       "#      0.000000  0.0  ...     0.0   0.0     0.0         0.0  0.0  0.000000   \n",
       "(      0.000000  0.0  ...     0.0   0.0     0.0         0.0  0.0  0.000000   \n",
       ")      0.000000  0.0  ...     0.0   0.0     0.0         0.0  0.0  0.000000   \n",
       ",      0.000000  0.0  ...     0.0   0.0     0.0         0.0  0.0  0.000000   \n",
       "...         ...  ...  ...     ...   ...     ...         ...  ...       ...   \n",
       "‘      0.000000  0.0  ...     0.0   0.0     0.0         0.0  0.0  0.000000   \n",
       "’      0.000000  0.0  ...     0.0   0.0     0.0         0.0  0.0  0.058733   \n",
       "“      0.000000  0.0  ...     0.0   0.0     0.0         0.0  0.0  0.000000   \n",
       "”      0.000000  0.0  ...     0.0   0.0     0.0         0.0  0.0  0.000000   \n",
       "❝real  0.000000  0.0  ...     0.0   0.0     0.0         0.0  0.0  0.000000   \n",
       "\n",
       "              ’         “         ”  ❝real  \n",
       "!      0.798712  0.025324  0.000000    0.0  \n",
       "#      0.699605  0.000000  0.000000    0.0  \n",
       "(      0.000000  0.000000  0.000000    0.0  \n",
       ")      0.000000  0.000000  0.000000    0.0  \n",
       ",      0.614122  0.000000  0.000000    0.0  \n",
       "...         ...       ...       ...    ...  \n",
       "‘      0.058733  0.000000  0.000000    0.0  \n",
       "’      1.309977  0.268746  0.238795    0.0  \n",
       "“      0.268746  0.000000  1.148953    0.0  \n",
       "”      0.238795  1.148953  0.345813    0.0  \n",
       "❝real  0.000000  0.000000  0.000000    0.0  \n",
       "\n",
       "[2325 rows x 2325 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(word_context_matrix, index = cm.vocabulary, columns = cm.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2325, 2325)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_context_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica = FastICA(n_components = 2)\n",
    "std_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.10325107,  0.21470284],\n",
       "       [-0.44883685, -0.10749049],\n",
       "       [-0.02881714, -0.00573572],\n",
       "       ...,\n",
       "       [-0.00492577,  0.00278753],\n",
       "       [-0.00540991,  0.00331637],\n",
       "       [ 0.00179428, -0.00055114]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_std = std_scaler.fit_transform(word_context_matrix)\n",
    "\n",
    "matrix = ica.fit_transform(X_std)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comp 1</th>\n",
       "      <th>Comp 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>-0.103251</td>\n",
       "      <td>0.214703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <td>-0.448837</td>\n",
       "      <td>-0.107490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(</th>\n",
       "      <td>-0.028817</td>\n",
       "      <td>-0.005736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>)</th>\n",
       "      <td>-0.028623</td>\n",
       "      <td>-0.005543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>-0.311643</td>\n",
       "      <td>0.030535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‘</th>\n",
       "      <td>0.000864</td>\n",
       "      <td>-0.000191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>’</th>\n",
       "      <td>-0.188424</td>\n",
       "      <td>0.055169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>“</th>\n",
       "      <td>-0.004926</td>\n",
       "      <td>0.002788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>”</th>\n",
       "      <td>-0.005410</td>\n",
       "      <td>0.003316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>❝real</th>\n",
       "      <td>0.001794</td>\n",
       "      <td>-0.000551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2325 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Comp 1    Comp 2\n",
       "!     -0.103251  0.214703\n",
       "#     -0.448837 -0.107490\n",
       "(     -0.028817 -0.005736\n",
       ")     -0.028623 -0.005543\n",
       ",     -0.311643  0.030535\n",
       "...         ...       ...\n",
       "‘      0.000864 -0.000191\n",
       "’     -0.188424  0.055169\n",
       "“     -0.004926  0.002788\n",
       "”     -0.005410  0.003316\n",
       "❝real  0.001794 -0.000551\n",
       "\n",
       "[2325 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(matrix,\n",
    "                  index = cm.vocabulary,\n",
    "                  columns = ['Comp {}'.format(i+1) for i in range(2)])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caitlinmoroney/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 62222 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/caitlinmoroney/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 62222 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/caitlinmoroney/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 12540 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/caitlinmoroney/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 12540 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = [key for key in cm.vocabulary.keys()]\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    \n",
    "    #if re.match('^co[rv]', word):\n",
    "    \n",
    "        x = df['Comp 1'][i]\n",
    "        y = df['Comp 2'][i]\n",
    "\n",
    "        plt.scatter(x, y)\n",
    "        plt.text(x, y, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set word embeddings for text classification\n",
    "ica = FastICA(n_components = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caitlinmoroney/opt/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.56250000e-01,  2.18750000e-01, -7.81250000e-02, ...,\n",
       "        -3.12500000e-02,  6.25000000e-02, -7.81250000e-02],\n",
       "       [ 2.25000000e+00,  3.64453125e+00, -2.66015625e+00, ...,\n",
       "        -3.14062500e+00, -2.50000000e+00,  4.39062500e+00],\n",
       "       [ 3.67462158e-01,  6.59576416e-01, -1.81101990e+00, ...,\n",
       "        -1.25239944e+00,  4.25506592e-01,  1.19458008e+00],\n",
       "       ...,\n",
       "       [-2.42233276e-02,  1.39312744e-01, -1.61834717e-01, ...,\n",
       "        -2.85224915e-02, -2.49847412e-01,  2.32543945e-02],\n",
       "       [-1.69425964e-01,  1.12548828e-01, -2.37579346e-02, ...,\n",
       "        -1.46686554e-01, -2.04132080e-01,  2.07458496e-01],\n",
       "       [-1.23977661e-02, -1.11389160e-02, -6.42395020e-03, ...,\n",
       "        -4.15802002e-03,  1.01318359e-02, -3.05175781e-03]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = ica.fit_transform(X_std)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out target\n",
    "y = tweets['Is_Unreliable']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derive text vectors from word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_vectors(word_embeddings,\n",
    "                     word_index_dict,\n",
    "                     text_list,\n",
    "                     remove_stopwords = True,\n",
    "                     lowercase = True,\n",
    "                     lemmatize = True,\n",
    "                     add_start_end_tokens = True):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    for k in range(len(text_list)):\n",
    "        text = text_list[k]\n",
    "        text = re.sub(r'[_~`@$%^&*[\\]+=\\|}{\\\"\\'<>/]+', '', text)\n",
    "        text_vec = np.zeros(word_embeddings.shape[1])\n",
    "        words = word_tokenize(text)\n",
    "        tracker = 0 # to track whether we've encountered a word for which we have an embedding (in each tweet)\n",
    "        \n",
    "        if remove_stopwords:\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                if word.lower() not in set(stopwords.words('english')):\n",
    "                    clean_words.append(word)\n",
    "            words = clean_words\n",
    "\n",
    "        if lowercase:\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                clean_words.append(word.lower())\n",
    "\n",
    "            words = clean_words\n",
    "\n",
    "        if lemmatize:\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                PoS_tag = pos_tag([word])[0][1]\n",
    "\n",
    "                # to change contractions to full word form\n",
    "                if word in contractions:\n",
    "                    word = contractions[word]\n",
    "\n",
    "                if PoS_tag[0].upper() in 'JNVR':\n",
    "                    word = lemmatizer.lemmatize(word, convert_pos_wordnet(PoS_tag))\n",
    "                else:\n",
    "                    word = lemmatizer.lemmatize(word)\n",
    "\n",
    "                clean_words.append(word)\n",
    "\n",
    "            words = clean_words\n",
    "\n",
    "        if add_start_end_tokens:\n",
    "            words = ['<START>'] + words + ['<END>']\n",
    "        \n",
    "        for i in range(len(words)):\n",
    "            word = words[i]\n",
    "            if word in word_index_dict:\n",
    "                word_embed_vec = word_embeddings[word_index_dict[word],:]\n",
    "                if tracker == 0:\n",
    "                    text_matrix = word_embed_vec\n",
    "                else:\n",
    "                    text_matrix = np.vstack((text_matrix, word_embed_vec))\n",
    "                    \n",
    "                # only increment if we have come across a word in the embeddings dictionary\n",
    "                tracker += 1\n",
    "                    \n",
    "        for j in range(len(text_vec)):\n",
    "            text_vec[j] = text_matrix[:,j].mean()\n",
    "            \n",
    "        if k == 0:\n",
    "            full_matrix = text_vec\n",
    "        else:\n",
    "            full_matrix = np.vstack((full_matrix, text_vec))\n",
    "            \n",
    "    return full_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_text_vectors(embeddings, cm.vocabulary, tweets['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(560, 250)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.31696261,  0.11251714,  0.04767785, ...,  0.04734098,\n",
       "        -0.08614643,  0.06234506],\n",
       "       [ 0.1600102 ,  0.19140189, -0.11492593, ..., -0.08335604,\n",
       "        -0.08824158,  0.20905413],\n",
       "       [ 0.15967669,  0.18835885, -0.10315922, ..., -0.08061572,\n",
       "        -0.09204756,  0.207707  ],\n",
       "       ...,\n",
       "       [ 0.43635698,  0.31329762, -0.03909163, ..., -0.15361543,\n",
       "        -0.38915738,  0.25505759],\n",
       "       [ 0.46350215,  0.22672917, -0.03525837, ..., -0.07102321,\n",
       "        -0.12973022,  0.29912391],\n",
       "       [ 0.2385114 ,  0.15232442, -0.16638997, ..., -0.11135406,\n",
       "         0.05596212,  0.2577179 ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification - nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, KFold, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "pipe = Pipeline([\n",
    "    ('classify', SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC hyperparams to optimize\n",
    "kernel = ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "C = [0.001, 0.01, 0.1, 1, 10]\n",
    "# set up parameter grid\n",
    "params = {\n",
    "    'classify__kernel': kernel,\n",
    "    'classify__C': C\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CV scheme for inner and outer loops\n",
    "inner_cv = KFold(n_splits = 3, shuffle = True, random_state = 1)\n",
    "outer_cv = KFold(n_splits = 5, shuffle = True, random_state = 1)\n",
    "\n",
    "# Set up GridSearch for inner loop\n",
    "grid_SVC = GridSearchCV(pipe, params, cv = inner_cv)\n",
    "#grid_SVC.fit(X, y)\n",
    "\n",
    "# Nested CV scores\n",
    "scores = cross_validate(grid_SVC,\n",
    "                        X = X,\n",
    "                        y = y,\n",
    "                        cv = outer_cv,\n",
    "                        scoring = ['roc_auc', 'accuracy', 'f1', 'precision', 'recall'],\n",
    "                        return_estimator = True)\n",
    "auc = scores['test_roc_auc']\n",
    "accuracy = scores['test_accuracy']\n",
    "f1 = scores['test_f1']\n",
    "precision = scores['test_precision']\n",
    "recall = scores['test_recall']\n",
    "estimators = scores['estimator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9018474456646395"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8160714285714284"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8318795807246981"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7680839524410938"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9136144386389704"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classify__C': 10, 'classify__kernel': 'linear'}\n",
      "\n",
      "\n",
      "{'classify__C': 10, 'classify__kernel': 'linear'}\n",
      "\n",
      "\n",
      "{'classify__C': 10, 'classify__kernel': 'linear'}\n",
      "\n",
      "\n",
      "{'classify__C': 10, 'classify__kernel': 'linear'}\n",
      "\n",
      "\n",
      "{'classify__C': 10, 'classify__kernel': 'linear'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in estimators:\n",
    "    print(i.best_params_)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
