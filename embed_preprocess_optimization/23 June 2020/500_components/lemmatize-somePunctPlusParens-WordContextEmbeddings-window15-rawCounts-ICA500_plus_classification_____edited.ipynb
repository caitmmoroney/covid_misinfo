{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Word Context Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.decomposition import FastICA, TruncatedSVD, PCA, NMF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to convert contractions picked up by word_tokenize() into full words\n",
    "contractions = {\n",
    "    \"n't\": 'not',\n",
    "    \"'ve\": 'have',\n",
    "    \"'s\": 'is', # note that this will include possessive nouns\n",
    "    'gonna': 'going to',\n",
    "    'gotta': 'got to',\n",
    "    \"'d\": 'would',\n",
    "    \"'ll\": 'will',\n",
    "    \"'re\": 'are',\n",
    "    \"'m\": 'am',\n",
    "    'wanna': 'want to'\n",
    "}\n",
    "\n",
    "# to convert nltk_pos tags to wordnet-compatible PoS tags\n",
    "def convert_pos_wordnet(tag):\n",
    "    tag_abbr = tag[0].upper()\n",
    "    tag_dict = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "                \n",
    "    if tag_abbr in tag_dict:\n",
    "        return tag_dict[tag_abbr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextMatrix(TransformerMixin):\n",
    "    \n",
    "    # initialize class & private variables\n",
    "    def __init__(self,\n",
    "                 window_size = 4,\n",
    "                 remove_stopwords = True,\n",
    "                 add_start_end_tokens = True,\n",
    "                 lowercase = False,\n",
    "                 lemmatize = False,\n",
    "                 pmi = False,\n",
    "                 spmi_k = 1,\n",
    "                 laplace_smoothing = 0,\n",
    "                 pmi_positive = False,\n",
    "                 sppmi_k = 1):\n",
    "        \n",
    "        \"\"\" Params:\n",
    "                window_size: size of +/- context window (default = 4)\n",
    "                remove_stopwords: boolean, whether or not to remove NLTK English stopwords\n",
    "                add_start_end_tokens: boolean, whether or not to append <START> and <END> to the\n",
    "                beginning/end of each document in the corpus (default = True)\n",
    "                lowercase: boolean, whether or not to convert words to all lowercase\n",
    "                lemmatize: boolean, whether or not to lemmatize input text\n",
    "                pmi: boolean, whether or not to compute pointwise mutual information\n",
    "                pmi_positive: boolean, whether or not to compute positive PMI\n",
    "        \"\"\"\n",
    "        self.window_size = window_size\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.add_start_end_tokens = add_start_end_tokens\n",
    "        self.lowercase = lowercase\n",
    "        self.lemmatize = lemmatize\n",
    "        self.pmi = pmi\n",
    "        self.spmi_k = spmi_k\n",
    "        self.laplace_smoothing = laplace_smoothing\n",
    "        self.pmi_positive = pmi_positive\n",
    "        self.sppmi_k = sppmi_k\n",
    "        self.corpus = None\n",
    "        self.clean_corpus = None\n",
    "        self.vocabulary = None\n",
    "        self.X = None\n",
    "        self.doc_terms_lists = None\n",
    "    \n",
    "    def fit(self, corpus, y = None):\n",
    "        \n",
    "        \"\"\" Learn the dictionary of all unique tokens for given corpus.\n",
    "        \n",
    "            Params:\n",
    "                corpus: list of strings\n",
    "            \n",
    "            Returns: self\n",
    "        \"\"\"\n",
    "        self.corpus = corpus\n",
    "        \n",
    "        term_dict = dict()\n",
    "        k = 0\n",
    "        corpus_words = []\n",
    "        clean_corpus = []\n",
    "        doc_terms_lists = []\n",
    "        detokenizer = TreebankWordDetokenizer()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        for text in corpus:\n",
    "            text = re.sub(r'[_~`@$%^&*[\\]+=\\|}{\\\"\\'<>/]+', '', text)\n",
    "            \n",
    "            words = word_tokenize(text)\n",
    "            \n",
    "            if self.remove_stopwords:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    if word.lower() not in set(stopwords.words('english')):\n",
    "                        clean_words.append(word)\n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lowercase:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    clean_words.append(word.lower())\n",
    "                \n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lemmatize:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    PoS_tag = pos_tag([word])[0][1]\n",
    "                    \n",
    "                    # to change contractions to full word form\n",
    "                    if word in contractions:\n",
    "                        word = contractions[word]\n",
    "\n",
    "                    if PoS_tag[0].upper() in 'JNVR':\n",
    "                        word = lemmatizer.lemmatize(word, convert_pos_wordnet(PoS_tag))\n",
    "                    else:\n",
    "                        word = lemmatizer.lemmatize(word)\n",
    "\n",
    "                    clean_words.append(word)\n",
    "                    \n",
    "                words = clean_words\n",
    "            \n",
    "            # detokenize trick taken from this StackOverflow post:\n",
    "            # https://stackoverflow.com/questions/21948019/python-untokenize-a-sentence\n",
    "            # and NLTK treebank documentation:\n",
    "            # https://www.nltk.org/_modules/nltk/tokenize/treebank.html\n",
    "            text = detokenizer.detokenize(words)\n",
    "            clean_corpus.append(text)\n",
    "            \n",
    "            [corpus_words.append(word) for word in words]\n",
    "            \n",
    "            if self.add_start_end_tokens:\n",
    "                words = ['<START>'] + words + ['<END>']\n",
    "            \n",
    "            doc_terms_lists.append(words)\n",
    "            \n",
    "        self.clean_corpus = clean_corpus\n",
    "        \n",
    "        self.doc_terms_lists = doc_terms_lists\n",
    "        \n",
    "        corpus_words = list(set(corpus_words))\n",
    "        \n",
    "        if self.add_start_end_tokens:\n",
    "            corpus_words = ['<START>'] + corpus_words + ['<END>']\n",
    "        \n",
    "        corpus_words = sorted(corpus_words)\n",
    "        \n",
    "        for el in corpus_words:\n",
    "            term_dict[el] = k\n",
    "            k += 1\n",
    "            \n",
    "        self.vocabulary = term_dict\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def transform(self, new_corpus = None, y = None):\n",
    "        \n",
    "        \"\"\" Compute the co-occurrence matrix for given corpus and window_size, using term dictionary\n",
    "            obtained with fit method.\n",
    "        \n",
    "            Returns: term-context co-occurrence matrix (shape: target terms by context terms) with\n",
    "            raw counts\n",
    "        \"\"\"\n",
    "        num_terms = len(self.vocabulary)\n",
    "        window = self.window_size\n",
    "        X = np.full((num_terms, num_terms), self.laplace_smoothing)\n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        if type(new_corpus) != list:\n",
    "            new_corpus = self.corpus\n",
    "        \n",
    "        for text in new_corpus:\n",
    "            text = re.sub(r'[_~`@$%^&*[\\]+=\\|}{\\\"\\'<>/]+', '', text)\n",
    "            \n",
    "            words = word_tokenize(text)\n",
    "            \n",
    "            if self.remove_stopwords:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    if word.lower() not in set(stopwords.words('english')):\n",
    "                        clean_words.append(word)\n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lowercase:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    clean_words.append(word.lower())\n",
    "                \n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lemmatize:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    PoS_tag = pos_tag([word])[0][1]\n",
    "                    \n",
    "                    # to change contractions to full word form\n",
    "                    if word in contractions:\n",
    "                        word = contractions[word]\n",
    "\n",
    "                    if PoS_tag[0].upper() in 'JNVR':\n",
    "                        word = lemmatizer.lemmatize(word, convert_pos_wordnet(PoS_tag))\n",
    "                    else:\n",
    "                        word = lemmatizer.lemmatize(word)\n",
    "\n",
    "                    clean_words.append(word)\n",
    "                    \n",
    "                words = clean_words\n",
    "                \n",
    "            if self.add_start_end_tokens:\n",
    "                words = ['<START>'] + words + ['<END>']\n",
    "            \n",
    "            for i in range(len(words)):\n",
    "                target = words[i]\n",
    "                \n",
    "                # check to see if target word is in the dictionary; if not, skip\n",
    "                if target in self.vocabulary:\n",
    "                    \n",
    "                    # grab index from dictionary\n",
    "                    target_dict_index = self.vocabulary[target]\n",
    "                    \n",
    "                    # find left-most and right-most window indices for each target word\n",
    "                    left_end_index = max(i - window, 0)\n",
    "                    right_end_index = min(i + window, len(words) - 1)\n",
    "                    \n",
    "                    # loop over all words within window\n",
    "                    # NOTE: this will include the target word; make sure to skip over it\n",
    "                    for j in range(left_end_index, right_end_index + 1):\n",
    "                        \n",
    "                        # skip \"context word\" where the \"context word\" index is equal to the\n",
    "                        # target word index\n",
    "                        if j != i:\n",
    "                            context_word = words[j]\n",
    "                            \n",
    "                            # check to see if context word is in the fitted dictionary; if\n",
    "                            # not, skip\n",
    "                            if context_word in self.vocabulary:\n",
    "                                X[target_dict_index, self.vocabulary[context_word]] += 1\n",
    "        \n",
    "        # if pmi = True, compute pmi matrix from word-context raw frequencies\n",
    "        # more concise code taken from this StackOverflow post:\n",
    "        # https://stackoverflow.com/questions/58701337/how-to-construct-ppmi-matrix-from-a-text-corpus\n",
    "        if self.pmi:\n",
    "            denom = X.sum()\n",
    "            col_sums = X.sum(axis = 0)\n",
    "            row_sums = X.sum(axis = 1)\n",
    "            \n",
    "            expected = np.outer(row_sums, col_sums)/denom\n",
    "            \n",
    "            X = X/expected\n",
    "            \n",
    "            for i in range(X.shape[0]):\n",
    "                for j in range(X.shape[1]):\n",
    "                \n",
    "                    if X[i,j] > 0:\n",
    "                        X[i,j] = np.log(X[i,j]) - np.log(self.spmi_k)\n",
    "                        \n",
    "                        if self.pmi_positive:\n",
    "                            X[i,j] = max(X[i,j] - np.log(self.sppmi_k), 0)\n",
    "        \n",
    "        # note that X is a dense matrix\n",
    "        self.X = X\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ContextMatrix(lowercase = True, lemmatize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [\n",
    "    \"Coronavirus is a fake liberal hoax.\",\n",
    "    \"Trump won't do anything about coronavirus.\",\n",
    "    \"The liberal fake news media always blame Pres Trump.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ContextMatrix at 0x1a1d1e6fd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.fit(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.</th>\n",
       "      <th>&lt;END&gt;</th>\n",
       "      <th>&lt;START&gt;</th>\n",
       "      <th>always</th>\n",
       "      <th>anything</th>\n",
       "      <th>blame</th>\n",
       "      <th>coronavirus</th>\n",
       "      <th>fake</th>\n",
       "      <th>hoax</th>\n",
       "      <th>liberal</th>\n",
       "      <th>medium</th>\n",
       "      <th>news</th>\n",
       "      <th>pres</th>\n",
       "      <th>trump</th>\n",
       "      <th>wont</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;END&gt;</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;START&gt;</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>always</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anything</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blame</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coronavirus</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fake</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hoax</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liberal</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medium</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pres</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wont</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             .  <END>  <START>  always  anything  blame  coronavirus  fake  \\\n",
       ".            0      3        0       1         1      1            2     1   \n",
       "<END>        3      0        0       0         1      1            1     1   \n",
       "<START>      0      0        0       0         1      0            2     2   \n",
       "always       1      0        0       0         0      1            0     1   \n",
       "anything     1      1        1       0         0      0            1     0   \n",
       "blame        1      1        0       1         0      0            0     1   \n",
       "coronavirus  2      1        2       0         1      0            0     1   \n",
       "fake         1      1        2       1         0      1            1     0   \n",
       "hoax         1      1        1       0         0      0            1     1   \n",
       "liberal      1      1        2       1         0      0            1     2   \n",
       "medium       0      0        1       1         0      1            0     1   \n",
       "news         0      0        1       1         0      1            0     1   \n",
       "pres         1      1        0       1         0      1            0     0   \n",
       "trump        2      1        1       1         1      1            1     0   \n",
       "wont         1      1        1       0         1      0            1     0   \n",
       "\n",
       "             hoax  liberal  medium  news  pres  trump  wont  \n",
       ".               1        1       0     0     1      2     1  \n",
       "<END>           1        1       0     0     1      1     1  \n",
       "<START>         1        2       1     1     0      1     1  \n",
       "always          0        1       1     1     1      1     0  \n",
       "anything        0        0       0     0     0      1     1  \n",
       "blame           0        0       1     1     1      1     0  \n",
       "coronavirus     1        1       0     0     0      1     1  \n",
       "fake            1        2       1     1     0      0     0  \n",
       "hoax            0        1       0     0     0      0     0  \n",
       "liberal         1        0       1     1     0      0     0  \n",
       "medium          0        1       0     1     1      1     0  \n",
       "news            0        1       1     0     1      0     0  \n",
       "pres            0        0       1     1     0      1     0  \n",
       "trump           0        0       1     0     1      0     1  \n",
       "wont            0        0       0     0     0      1     0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cm.transform(tweets), index = cm.vocabulary, columns = cm.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coronavirus fake liberal hoax.',\n",
       " 'trump wont anything coronavirus.',\n",
       " 'liberal fake news medium always blame pres trump.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.clean_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Coronavirus is a fake liberal hoax.',\n",
       " \"Trump won't do anything about coronavirus.\",\n",
       " 'The liberal fake news media always blame Pres Trump.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train embeddings using tweets as corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('COVID19_Dataset-text_labels_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Is_Unreliable</th>\n",
       "      <th>Category</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 3, 6, 9</td>\n",
       "      <td>We are living in scary times in Canada. Gov’t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 6, 8, 9</td>\n",
       "      <td>Just as bad in Canada. In fact, our government...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 4, 9</td>\n",
       "      <td>It was only a matter of time before the mainst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6, 8</td>\n",
       "      <td>Russia's taking no chances: Foreigners infecte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>6, 8, 9</td>\n",
       "      <td>Although there is now a presumptive confirmed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BREAKING: Harvard classes will move online sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Singularity University is hosting a FREE Virtu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coronavirus: how does it spread and what are t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stanford just cancelled classes for the rest o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tech conferences were cancelled in #Waterloo R...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Is_Unreliable    Category  \\\n",
       "0                1  1, 3, 6, 9   \n",
       "1                1  1, 6, 8, 9   \n",
       "2                1     1, 4, 9   \n",
       "3                1        6, 8   \n",
       "4                1     6, 8, 9   \n",
       "..             ...         ...   \n",
       "555              0         NaN   \n",
       "556              0         NaN   \n",
       "557              0         NaN   \n",
       "558              0         NaN   \n",
       "559              0         NaN   \n",
       "\n",
       "                                                 Tweet  \n",
       "0    We are living in scary times in Canada. Gov’t ...  \n",
       "1    Just as bad in Canada. In fact, our government...  \n",
       "2    It was only a matter of time before the mainst...  \n",
       "3    Russia's taking no chances: Foreigners infecte...  \n",
       "4    Although there is now a presumptive confirmed ...  \n",
       "..                                                 ...  \n",
       "555  BREAKING: Harvard classes will move online sta...  \n",
       "556  Singularity University is hosting a FREE Virtu...  \n",
       "557  Coronavirus: how does it spread and what are t...  \n",
       "558  Stanford just cancelled classes for the rest o...  \n",
       "559  Tech conferences were cancelled in #Waterloo R...  \n",
       "\n",
       "[560 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ContextMatrix(window_size = 15, lowercase = True, lemmatize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ContextMatrix at 0x1a24d609d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.fit(tweets['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_context_matrix = cm.transform(tweets['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>#</th>\n",
       "      <th>(</th>\n",
       "      <th>)</th>\n",
       "      <th>,</th>\n",
       "      <th>-</th>\n",
       "      <th>--</th>\n",
       "      <th>.</th>\n",
       "      <th>...</th>\n",
       "      <th>1</th>\n",
       "      <th>...</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoomer</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>—</th>\n",
       "      <th>‘</th>\n",
       "      <th>’</th>\n",
       "      <th>“</th>\n",
       "      <th>”</th>\n",
       "      <th>❝real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>124</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <td>30</td>\n",
       "      <td>650</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>113</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>215</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>63</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>)</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>10</td>\n",
       "      <td>113</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>138</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>157</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‘</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>’</th>\n",
       "      <td>29</td>\n",
       "      <td>63</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>60</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>“</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>”</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>❝real</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2325 rows × 2325 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         !    #   (   )    ,   -  --    .  ...  1  ...  zombie  zone  zoomer  \\\n",
       "!      124   30   0   1   10   3   0   37    7  3  ...       0     0       0   \n",
       "#       30  650   9   9  113  19   3  215    1  1  ...       0     1       1   \n",
       "(        0    9   2  32   13   1   0   17    1  0  ...       0     0       0   \n",
       ")        1    9  32   2   13   1   0   20    1  0  ...       0     0       0   \n",
       ",       10  113  13  13  138  12   4  157    3  4  ...       3     1       0   \n",
       "...    ...  ...  ..  ..  ...  ..  ..  ...  ... ..  ...     ...   ...     ...   \n",
       "‘        0    4   0   0    3   0   0    4    0  0  ...       0     0       0   \n",
       "’       29   63   4   4   45   6   0   86    5  0  ...       0     0       0   \n",
       "“        8    5   1   1   15   3   1   17    1  1  ...       0     0       0   \n",
       "”        2    4   0   0    8   2   1   13    1  0  ...       0     0       0   \n",
       "❝real    1    0   0   0    0   0   0    0    0  0  ...       0     0       0   \n",
       "\n",
       "       zuckerberg  —  ‘   ’   “   ”  ❝real  \n",
       "!               0  1  0  29   8   2      1  \n",
       "#               0  2  4  63   5   4      0  \n",
       "(               0  0  0   4   1   0      0  \n",
       ")               0  0  0   4   1   0      0  \n",
       ",               0  2  3  45  15   8      0  \n",
       "...           ... .. ..  ..  ..  ..    ...  \n",
       "‘               0  0  0   9   0   0      0  \n",
       "’               0  0  9  60  13  12      1  \n",
       "“               0  1  0  13   4  20      0  \n",
       "”               0  1  0  12  20   8      0  \n",
       "❝real           0  0  0   1   0   0      0  \n",
       "\n",
       "[2325 rows x 2325 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(word_context_matrix, index = cm.vocabulary, columns = cm.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2325, 2325)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_context_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica = FastICA(n_components = 2)\n",
    "std_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03588795, -0.04220098],\n",
       "       [ 0.73522062, -0.64758016],\n",
       "       [-0.02495049, -0.01350401],\n",
       "       ...,\n",
       "       [-0.02908891, -0.01122225],\n",
       "       [-0.02154661, -0.00749671],\n",
       "       [ 0.00168672,  0.00187529]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_std = std_scaler.fit_transform(word_context_matrix)\n",
    "\n",
    "matrix = ica.fit_transform(X_std)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comp 1</th>\n",
       "      <th>Comp 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>-0.035888</td>\n",
       "      <td>-0.042201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <td>0.735221</td>\n",
       "      <td>-0.647580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(</th>\n",
       "      <td>-0.024950</td>\n",
       "      <td>-0.013504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>)</th>\n",
       "      <td>-0.026326</td>\n",
       "      <td>-0.013755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>-0.229131</td>\n",
       "      <td>-0.166505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‘</th>\n",
       "      <td>-0.004676</td>\n",
       "      <td>-0.001719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>’</th>\n",
       "      <td>-0.071279</td>\n",
       "      <td>-0.086722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>“</th>\n",
       "      <td>-0.029089</td>\n",
       "      <td>-0.011222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>”</th>\n",
       "      <td>-0.021547</td>\n",
       "      <td>-0.007497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>❝real</th>\n",
       "      <td>0.001687</td>\n",
       "      <td>0.001875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2325 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Comp 1    Comp 2\n",
       "!     -0.035888 -0.042201\n",
       "#      0.735221 -0.647580\n",
       "(     -0.024950 -0.013504\n",
       ")     -0.026326 -0.013755\n",
       ",     -0.229131 -0.166505\n",
       "...         ...       ...\n",
       "‘     -0.004676 -0.001719\n",
       "’     -0.071279 -0.086722\n",
       "“     -0.029089 -0.011222\n",
       "”     -0.021547 -0.007497\n",
       "❝real  0.001687  0.001875\n",
       "\n",
       "[2325 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(matrix,\n",
    "                  index = cm.vocabulary,\n",
    "                  columns = ['Comp {}'.format(i+1) for i in range(2)])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caitlinmoroney/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 62222 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/caitlinmoroney/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 62222 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/caitlinmoroney/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 12540 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/caitlinmoroney/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 12540 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXhUVZrH8e9bVVkJBEJICFsIm0AgCSQgqzSkQRQUcRBUpKFRsWkVxdEWWlppe6a1hcGRVkdRmkURRWxF1JaBKG6ACgIRAdmVJWwJRENIyPLOHylqCCQksbJBvZ/nuU/d5dQ951Qqv7p16tYtUVWMMcZc/hw13QBjjDHVwwLfGGN8hAW+Mcb4CAt8Y4zxERb4xhjjI1w13YDShIeHa8uWLWu6GcYYc0nZsGHDcVVtVNK2Whv4LVu2ZP369TXdDGOMuaSIyA+lbbMhHWOM8REW+MYY4yMs8I0xxkdY4BtjjI+wwDeVRkRsOmdq3rx5seVmzZp55l0uF3Xq1EFEuPHGGwG4//77adasGQkJCURFReHv789jjz3GhAkTSEhI8Ezh4eHcf//9pf4devXqVebfqmXLlhw/fvyC9atXr2bNmjW//ElgajULfFMp/P39a7oJtU5aWlqx5YMHD3rmCwoKyM7OBuDtt98mKCiIZ555htjYWIKCgujVqxd5eXkcOHCAV155hcTERDZt2sSGDRsIDQ2la9eupda7Zs0aIiIiSEhIAOCFF15g4cKFF21rdnY2Q4YMYcSIEQwfPpwpU6Z4tuXm5jJq1CjatGnDlVdeyb59+wBYuXIliYmJdO7cmcTERD766CPPfTZs2EDnzp1p06YNkyZNQlUpKCgo3wNnqo6q1sopMTFRTe134sQJHTFihAYFBSlgUxVODodDHQ6HAioi6u/vr4GBgep0OrV169Y6ceJEjYyMLHafjh07akxMjIaFhWlwcLAGBwdrp06dNDg4WKdMmaJdunTRTp066TfffKOvvfaaRkZGapMmTbROnTr61FNP6dixY7VPnz7apEkTjYmJ0T/96U/asmVLbd++vV533XV68OBBVVUdOXKk+vn5aceOHfXRRx/Vbt266Zo1azQ6OlrbtGmjHTt21MWLF+uuXbv06quv1q5du2qfPn1027ZtNfwMvvwA67WUXK3xYC9tssC/NERERKiI1HgY2nTh5HQ61el0el4gnE6njh49WuvXr+95AZkwYYI+99xzevvtt6uq6mOPPaYzZszQSZMm6Zw5c3Ts2LHauHFj/eKLL/Sdd97RunXramhoqObn52vXrl1148aNqqp6/PhxDQsL01OnTmnPnj01OjpaVVWjo6P1lltu0QkTJqiq6oABA3THjh2qqrpu3Trt379/9T9pL3MXC3wb0jHlcvLkSZ5//vli67p06cLRo0eLjhxMrVNQUFBsGKWgoIBFixZx8uRJXC4Xw4cPZ/v27WRlZXmGaQBOnz7N8uXLSU5OBsDpdNKiRQs6d+5MZGQkDRs25MSJE8TGxnruN3XqVPLy8ujRowfbtm0jJCTEs7/hw4dz8OBBsrKyWLNmDTfddBMJCQncddddFwx7mapVKYEvIoNF5HsR2SUiU0rYHiAib7i3fykiLSujXlN9zg38yWOGsPo/YtixI7WGW2VKIyIA1KlTh6ZNm9KkSRPCw8MJCQlhzJgxNG3alBdeeIHf/OY3bNq0ifz8fAAKCwtZtGgRkyZNolWrVsX25XA4CAgI8KxzOBzk5+ezYsUK5s2bx+rVq0lNTaV3794UFhZ62hIYGIiIUFhYSP369dm0aZNn2rZtW3U+LD7P68AXESfwHHAN0BG4RUQ6nlfsduCEqrYBngb+5m29pnpNmTKF3bt306FNBC8s/RcDHt1HdnZh2Xc0leps+J4/f5bDUfQvffZdV3Z2NidPnkRVSU9P59SpUyxbtozs7Gx27dp1wf0//PBD6tWrV+wsoLCwMPbv3+9ZzszMJCwsDID09HQmTJhAdHQ0CQkJHDlyhC+//JITJ054yh86dIgmTZpQr149YmJiePPNNz1t3Lx5szcPh6mgyjjC7w7sUtU9qnoGeB0Ydl6ZYcAC9/xSIFlKeraaWuvJJ5+kRcO6/D20Lk0K/VAFcdiIYHU7d/js7Py5/0rnHlkD+Pn5cerUKQ4fPoy/vz+qSlZWFidPnmTLli28+uqrJCYmAjBt2jTq169PTk4OCQkJfPbZZwB069aNBQuK/n0zMzMZMGAAIsKZM2f461//yqxZs+jTpw+xsbGMHz+evn37EhgYyLp16wBYsmQJw4YVRcKiRYuYO3cu8fHxxMbGsmzZsip6pEyJShvcL+8EjABePmd5DPDseWW2AM3OWd4NhJewrwnAemB9ixYtqugjDVMR369L0/lTP9fnBv5B2wQE6NYr2uuI0NCiDwJrwQeTNl04RUREKKAul0sXLlyo8fHxKiKeD187duyoAQEBGhgYqHfeeaeqqu7fv18Bbd++vcbHx2t8fLy+9NJLqqp6+vRpHTFihLZu3Vq7deumu3fvVlXVv/zlLxocHOwpHx8fr0eOHFFV1a+//lpjY2O1VatWevfdd2thYWHNPIF9EFV5lg5wUwmB//fzynxXQuA3vNh+7SydmpWVlaX9eiVr04attGF4pN7XPFLDnE7tFBiodd2nBtpUNAUHB2tgYGCxdeHh4cWWQ0JCtE6dOupyudTlcmmjRo20f//+evfddyuggYGBunz5cg0KCtLIyEidNm2aRkZGaqdOnTQ7O1tvvPFG7dy5s44ZM0ZbtGiho0aNUlXVsWPHavv27fXaa6/V8PBwbd68uXbo0EHnzZunqqr9+vXT+Pj4YuvM5e1igV8Zl0c+ADQ/Z7kZcKiUMgdExAWEAhmVULepIh9++CF6KpgRd93FJ61f55r/yOR1SWdJdEteyUjniWPHcAH5Nd3QauBwOCgsLMTPz4/OnTuTmppKQUEB3377LV26dGHIkCEsWbKEtm3bsnfvXgoLC8nIKHp6BwQEcObMGXJycnC5XDRt2pSGDRuSnp5Obm4un3/+OU2aNKF9+/YMHTqU7t2707NnT1JSUggPD2fu3LkEBQUxcuRInnjiCb755hs6derE3//+dwDmz59/0bavXr26ih8dc0kp7ZWgvBNF19TfA8QA/sBmIPa8MncDL7jnbwaWlLVfO8KvfkvT0jXxiy3a+KONGrv4PW1Yt7FGDWqm0f8erT1C62iQiApo4CVw3v3ZIYzQ0FCdN2+eBgQEqNPp1Jdeekmjo6M1ICBA+/fvr7GxsdqwYUO9/vrrtXfv3tq5c2edPn26Nm7cWFu0aKFPPPGE5/GJjY3VvXv36t69ezU2NrZK/gb9+vXTr7/+ukr2bXwDVXkevqrmA/cAK4Bt7jD/TkQeF5Hr3cXmAg1FZBfwAHDBqZumZr11OIMHv9/Pgdw8FDge2ZTf3fYizmhIW5TGrsB8ghwOVrVqzT0Nwz33u3rAOHAF1Fi7z/3A8o9//CMNGzYEoGnTpgQEBNChQwcA7rjjDvr06eO53MAbb7zhud+gQYNwOp00btyYmJgYXn31VT799FN++9vf4nJd+Ca4bt26/Pzzz1XZLWOqRKX84pWqfgB8cN66R8+Zz6ForN/UUk/sSeN0oXqWnT+8z6boH2naoCUFpws4/MZhtEC57+hBtmfleMqt+GQRFORVenvq1avHTz/9dMH68PBw0tPTCQ0Nxel0oqqe4ZNnn32W6Oho0tPTcTqdPPLIIzzwwAPs3r37onXNnj2biRMn8vHHH/Pzzz8TEhLCihUrSi3fsGFDevfuTadOnbjmmmuYMWOGd509hw3BmKokWku/JZmUlKT2E4fVJ+rjTZx9JgRkfYH+73+R9up+AusEU+CXT9igBhyccxAEzhZ0BAQDQuGZXNCyR/ODQuqSn5tDSEgIJ06cICQkhDp16tCjRw/P6Xnt2rVjx44dOJ1OnE4n+fn5FBYW0qtXL9LS0jhw4AAdO3akX79+FBQU8Oyzz7Js2TImT55M06ZN6dGjB19//bUFp/FZIrJBVZNK2lZrf9PWVK+mAX4cyC06Uq+T+SYF0X446zhp+Z8tyDuRR8HpAhxBDoI7BpOzORsXDkDJyTtTQti7XxXECVoAUlS2Xp1gsrSQu+66i1mzZnHNNdewfPlyz/nesbGxHDlyBBGhadOmBAUFsWvXLlwuF/v27eNf//oXBw4cYNasWTzzzDOe2oYNG+Y5z9sYUzr75owBYGqrKIIc7q/QF6Rz5M0jnDl6hl1/2sXeJ/eye/puCk8XcuqbLFzBLs4UFFDP34FDz/mijzgQv0Cc9RoVLWohOJ2ghaDKkSNHOHXqFC+++CJnzpzhrbfeIicnh/z8fJxOJw6Hg8TERESE/Px8AgMDCQkJwel0curUKbZu3UpiYmKx674YY8rPAt8A8G+Nw5h5RXOaBfhR6GxI5E2R+Ef40+YvbWgyrolnGOea+g0IdoWgqiS0akyAy4HL4SDQP5C4Fj0IDQimeYMCcDhwtIhG3NfJj4qKQkTw8/MjKysLgEmTJuHv70+TJk083/Z0uVxERETQvXt3srKy2Lt3L506daJPnz7k5OR4hnmMMRVngW88/q1xGMtb7eKZ5ln8Keo0Ea5CugYXDfP41XfhEEg5cYKMjAwU+OHoKZwBheQXFuJw5fLtj2vIPJXBvh/TQQsp+HEfevo0oaGhNG7cGFUlLy+P/Px8RIT58+fTuHFjrrjiCg4fPuxpR9euXdm5cycHDx6kd+/eHDhwwH6FyZhKYIFvPNIOL2P79kdwFJxABJwCNzfII+b4aTQ9n2iXP5936crEiRNpFBbOyjFv8NbkBwkKchAd7c/D98cUXWPHX6jbtS6O4KKnl38dfxwOBw6Hg1atWuFyuVBV7rjjDoKCgoCis2+uuuoqAAYPHkxhYSHBwcHk5uaSl5dHfHx8jT0uxlwu7ENb47Fn90wKC08DEBTkIDtb8XdA/YOnaeRw4nAIqXFxRDZsxOlT2YxcfB/p2ScpPOMiO60hI4Jn8V/hI3EEOAhpH0K9rvXIWJVBcKdgjqceJygoiEOHDhEWFsaRI0e48cYbmTFjBnfeeSd9+/bl3//93z1tuffee0ttp43hG/PL2BG+8cjJ/f8fowgNdRLbKYA7bt/PF9+cwulw0rNLF948kcW13QbSMLgBezJ+pHFIOD1aJJBXmM/8iGX4NfSj0XWNKDxTSNpraZw5doZTZ04xdepUkpKS+Oyzz1i3bh0dOnRgwYIFxMXFkZGRwcSJE2uw58b4Bgt84xEYEFVs+ZFHInl5bnMem96KUyF1qNc7mZ96TqR1n46ERjRgzpgnWHTzf1GvbgiuSD82NtpJ09ubUr9HfRoNaUSddnVo+tumtOjWgn/84x/06NGDkydPcvjwYQoLC3nhhRdITU3l3nvvZeTIkSW26dlnn6VNmzaICMePH/esP3HiBMOHDycuLo7u3buzZcuWKn1sjLkcWOAbj1atH8ThCCq2rqDAiRb2oG/fvry5cA6H/nEPDzzwAI//7T+YvOJJbv3iEa4Y1JWYJm2YeuVUMldmsvORnez60y4c/g7Cu4Tz+G8f59Zbb+X9999n8uTJTJw48YLrtpemd+/erFq1iujo6GLr//rXv5KQkEBqaioLFy7kvvvuK9f+Hn30UVatWnXB+tWrVzN06FAAtm/fTs+ePQkICGDmzJnFyj3zzDN06tSJ2NhY/vu//7tcdRpTW9gYvvGIalz05aVNmx7Dz+9ncnPrsG9vAseOtSIhAVz+gezYvpWNGzdSUFDAxx9/zPjx41mxYgWNGjWis6szf5/xd8YOHEubv7Uhqm4Ud3W4i9/1+x179uxh48aNDB06lBEjRvDhhx/Svn17wsPD6dq1a6lt6tKlS4nrt27dytSpUwFo3749+/bt48iRI0RGRl60j48//niZj0NYWBizZ8/mnXfeKbZ+y5YtvPTSS3z11Vf4+/szePBghgwZQtu2bcvcpzG1gR3hm2KiGg+jaZOFfLluPF9/dSPHjhX9rmlGRgbrv1rHRx99xObNm3nmmWe45557+M1vfkNqaiqjR49m0qRJjIwfyaCeg5jdcjb/O+J/yd+Sz9VXX42fn5+njpycHO68807Pt2zPPSWzLAsXLiQuLo7169dz55138sMPP9CtWzf27NnDddddx48//khmZiYtW7b0vIvIzs6mefPm5OXlMW7cOJYuXQrgedHp06cP//znPz11RERE0K1bt2JtBti2bRs9evQgODgYl8tFv379ePvtt3/xY21MdbPANxeIi4vjuuuuIzQ0lM5s4wH5B212vsTotqcIP/QRUHQUvHbtWm699VYAxowZw+effw7AqFGjPFejfP311xk1alSx/W/fvp2YmBjatm2LiHDbbbeVq13bt2/nP//zP/noo4/YuXMnXbp0IS4uDj8/P5KSkhgyZAiTJk0iNDSU+Ph4PvnkEwCWL19eKS86nTp14tNPPyU9PZ3s7Gw++OCDYr/1akxtZ4Fv6Nu3L2vXri22Li4ujsnJTfk3v0+op5mAIrk/wfJJkLqkxP2cvVTx9ddfz7/+9S8yMjLYsGEDAwYMKLXs+a6++moSEhK44447ANjx5WEW/PELfk7P4ZlprzKg52DCw8OpV68er732Gn5+fqxevZpjx45xzz33VOmLTocOHXj44YcZOHAggwcPJj4+vsTLJxtTW1ng+7i8vKJv0vbo0ePCjSmPQ17RefnJMS6WfJdPeuYpSHmcjIwMevXqxeuvvw4U/Th1nz59AAgJCaF79+7cd999DB06FKfTWWy37du3Z+/evZ7LFi9evNizbcWKFWzatImXX36ZHV8e5uNF28nKyAUgNzufH77LYMeXhzl58iRnzpwBYO7cuVx11VXUq1evUl50Lub222/nm2++4dNPPyUsLMzG780lxQ5PDG+//XbJ4Zd5wDMbG+Hkkb7+9JufjdOxjS6bH2D27NmMHz+eGTNm0KhRI+bNm+cpP2rUKG666aYSL1McGBjInDlzGDJkCOHh4fTp06fE0yrXLtvNqg1LWbX5DX7KzuCjb5dSUJDHysW30HVUA0aPHk1OTg5z585l5cqVv/hFp3Xr1sVedC7m6NGjRERE8OOPP/LPf/7zgndGxtRmdj18U7qnO0FmCWPUoc1hctWf9/7c7z66YN2671eQsnkJjZrXo0uXLkyfPp3x48dz/Phxz4tOixYtAFi6dKnnRadfv34AjBs3rtiZQvfff3+xF5333nuPw4cPk5SUxE8//YTD4SAkJIStW7dSr149+vbtS3p6On5+fsyaNYvk5OQqfxyMqYiLXQ/fAt8XpS4pGq7JPAChzSD5UYgr4YtPqUuKxuzdwzoA+AXBdbNLLl/JFvzxC89wzrlCwgIY+9feVV6/MZeiiwW+jeH7mrMhnrkfUMjcz7U3jeHQqhcuLBs3sijcQ5sDUnRbTWEP0HNYa1z+xZ+iLn8HPYe1rpb6jbnc2BG+r6nhYZqK2vHlYdYu201WRi4hYQH0HNaadlc2rulmGVNr2U8cGg/N3E9J56aUtr6mtbuysQW8MZXEqyEdEQkTkZUistN926CUch+KyEkRec+b+oz3cgP9KrTeGHP58HYMfwqQoqptgRT3cklmAGO8rMtUgl3RgRSc91e/5rVs1tSzwDfmcudt4A8DFrjnFwA3lFRIVVOAn72sy1SCzBYxbGsbwukABwqcDnAw488RBMS3qemmGWOqmLdj+JGqmgagqmkiEuHNzkRkAjAB8JxLbSpXq9YPsj3vEY5EBnrWORxBtG/9YA22yhhTHcoMfBFZBZT0qdkjld0YVZ0DzIGis3Qqe//m/y+BvGf3THJy0wgMiKJV6wc9640xl68yA19Vf13aNhE5IiJR7qP7KOBopbbOVImoxsMs4I3xQd6O4b8LjHXPjwWWebk/Y4wxVcTbwH8SGCgiO4GB7mVEJElEXj5bSEQ+A94EkkXkgIhc7WW9xhhjKsirD21VNR244OpRqroeuOOc5b7e1GOMMcZ7di0dY4zxERb4xhjjIyzwjTHGR1jgG2OMj7DAN8YYH2GBb4wxPsIC3xhjfIQFvjHG+AgLfGOM8REW+MYY4yMs8I0xxkfYj5h7aceXh1m7bDdZGbmEhAXQc1hr+9FtY0ytZIHvhR1fHubjRdvJP1MIQFZGLh8v2g5goW+MqXVsSMcLa5ft9oQ9wOzlD3L8xBHWLttdg60yxpiSWeB7ISsj1zNfqIUc++kgwQH1iq03xpjawgLfCyFhAZ75wyd+ICGmL/6ugGLrjTGmtrDA90LPYa1x+Rc9hE3CYvi3Xr/H5e+g57DWNdwyY4y5kH1o64WzH8zaWTrGmEuBBb6X2l3Z2ALeGHNJsCEdY4zxERb4xhjjI7wKfBEJE5GVIrLTfdughDIJIrJWRL4TkVQRGeVNncYYY34Zb4/wpwApqtoWSHEvny8b+I2qxgKDgf8Wkfpe1muMMaaCvA38YcAC9/wC4IbzC6jqDlXd6Z4/BBwFGnlZrzHGmAryNvAjVTUNwH0bcbHCItId8AdKvPaAiEwQkfUisv7YsWNeNs0YY8y5yjwtU0RWASWdd/hIRSoSkSjgFWCsqhaWVEZV5wBzAJKSkrQi+zfGGHNxZQa+qv66tG0ickREolQ1zR3oR0spVw94H5imqut+cWuNMcb8Yt4O6bwLjHXPjwWWnV9ARPyBt4GFqvqml/UZY4z5hbwN/CeBgSKyExjoXkZEkkTkZXeZkcBVwDgR2eSeErys1xhjTAWJau0cKk9KStL169fXdDOMMeaSIiIbVDWppG32TVtjjPERFvjGGOMjLPCNMcZHWOAbY4yPsMA3xhgfYYFvjDE+wgLfGGN8hAW+Mcb4CAt8Y4zxERb4xhjjI8q8WqYpLjU1lZSUFDIzMwkNDSU5OZm4uLiabpYxxpTJAr8CUlNTWb58OXl5eQBkZmayfPlyAAt9Y0ytZ0M6FZCSkuIJ+7Py8vJISUmpoRYZY0z5WeBXQGZmZrHlRYsW8fPPP1+w3hhjaiML/AoIDQ0ttjx69Gjq1q17wXpjjKmNLPArIDk5GT8/v2Lr/Pz8SE5OrqEWGWNM+dmHthVw9oNZO0vHGHMpssCvoLi4OAt4Y8wlyYZ0jDHGR1jgG2OMj7DAN8YYH+FV4ItImIisFJGd7tsGJZSJFpENIrJJRL4Tkd95U6cxxphfxtsj/ClAiqq2BVLcy+dLA3qpagJwJTBFRJp4Wa8xxpgK8jbwhwEL3PMLgBvOL6CqZ1Q1170YUAl1GmOM+QW8Dd9IVU0DcN9GlFRIRJqLSCqwH/ibqh4qpdwEEVkvIuuPHTvmZdOMMcacq8zz8EVkFdC4hE2PlLcSVd0PxLmHct4RkaWqeqSEcnOAOQBJSUla3v0bY4wpW5mBr6q/Lm2biBwRkShVTRORKOBoGfs6JCLfAX2BpRVurTHGmF/M2yGdd4Gx7vmxwLLzC4hIMxEJcs83AHoD33tZrzHGmAryNvCfBAaKyE5goHsZEUkSkZfdZToAX4rIZuATYKaqfutlvcYYYyrIq2vpqGo6cMGlIlV1PXCHe34lYBefMcaYGmanSBpjjI+wwDfGGB9hgW+MMT7CAr8W6tWrV003wRhzGbLAr4D8/PxqqWfNmjUXrCsoKKiWuo0xly+fDfyFCxcSFxdHfHw8Y8aM4YcffvD8XGFycjI//vgjAOPGjeOBBx6gf//+PPzww2RkZHDDDTcQFxdHjx49SE1NBWD69OmMHz+eX/3qV7Rq1YrZs2d76rrhhhtITEwkNjaWOXPmAPA///M//OEPf/CUmT9/Pvfeey8AISEhAKxevZr+/ftz66230rlzZ/bt20enTp0895k5cybTp08HYPbs2XTs2JG4uDhuvvnmqnvgjDGXLlWtlVNiYqJWlS1btmi7du302LFjqqqanp6uQ4cO1fnz56uq6ty5c3XYsGGqqjp27FgdMmSI5ufnq6rqPffco9OnT1dV1ZSUFI2Pj1dV1ccee0x79uypOTk5euzYMQ0LC9MzZ8549q+qmp2drbGxsXr8+HE9evSotm7d2tOmwYMH62effaaqqnXq1FFV1Y8//liDg4N1z549qqq6d+9ejY2N9dxnxowZ+thjj6mqalRUlObk5Kiq6okTJyrz4TLGXEKA9VpKrvrUEX5qaipPP/00Dz74IC1atODQoaJruIWFhbF27VpuvfVWAMaMGcPnn3/uud9NN92E0+kE4PPPP2fMmDEADBgwgPT0dDIzMwEYMmQIAQEBhIeHExERwZEjRZcLmj17NvHx8fTo0YP9+/ezc+dOGjVqRKtWrVi3bh3p6el8//339O7d+4I2d+/enZiYmDL7FhcXx+jRo3n11Vdxueynio0xF/KZwE9NTWX58uWecM7NzWX58uWeIZnziYhnvk6dOp75ohfQkssGBAR41jmdTvLz81m9ejWrVq1i7dq1bN68mS5dupCTkwPAqFGjWLJkCW+99RbDhw8vVmdJdbtcLgoLCz3LZ/cD8P7773P33XezYcMGEhMTq+3zBmPMpcNnAj8lJYW8vDwAYmJi+O6778jMzCQlJYWMjAx69erF66+/DsCiRYvo06dPifu56qqrWLRoEVA0xh4eHk69evVKrTczM5MGDRoQHBzM9u3bWbdunWfbjTfeyDvvvMPixYsZNWpUmX2IjIzk6NGjpKenk5uby3vvvQdAYWEh+/fvp3///jz11FOcPHmSrKys8j0wxhif4TPv/c8e2QNERETQt29f5s+fj8PhYPPmzcyePZvx48czY8YMGjVqxLx580rcz/Tp0/ntb39LXFwcwcHBLFiwoMRyZw0ePJgXXniBuLg4rrjiCnr06OHZ1qBBAzp27MjWrVvp3r17mX3w8/Pj0Ucf5corryQmJob27dsDRWfw3HbbbWRmZqKqTJ48mfr165fnYTHG+BApaYiiNkhKStL169dX2v6efvrpYqF/VmhoKMuWLSMtLY2goCAA2rRpw9KlS5k+fTpPPfUU+/btIyKi6LddQkJCPEfPTqeTzp07k5eXh8vlYuzYsdx///04HD7zxskYU8uIyAZVTSppm88kU3JyMn5+fp7lgoICVJXk5KJrv/3ud79DRCTgBnUAAA6OSURBVFBVtm7dyosvvsinn35Kfn4+nTt3xul0kpCQwOnTpz2nXAYFBaGqxMXFsXLlSj744AP+/Oc/M27cOFq2bEl8fDzx8fGkpKQAMHz4cBISEmjTpg2hoaEkJCSQkJBQ4nn3xhhT2XzmCB+KPrh97bXX+OSTT9ixYwezZ8/mhhtu4JprrmHbtm1s3LiRZs2akZuby759+1i8eDFQdI788ePHycrKKnaEHxwcTOvWrcnIyGDHjh0cOXKEbt26cd111xEbG8vzzz9PUlISX331FT/88IOnHatXr2bmzJmeMXiAEydO0KBBg0rtrzHG9/j8Ef6pU6eYN28ev//97/nss8+49tprufXWW5k2bRo7duygoKCAEydOcO2115KQkMC0adO44oorgKIhnPHjx3PmzJkL9pufn8+YMWMYNGgQ7777Lq1ataKwsJDTp08TExNDamoqAwYM4MCBA/Tp04d58+Zx6tSpEtt477330r9/fxYtWlTs7BtjjKksl23gv7PxIL2f/IiYKe8T2jCCJ59+lmuvvRYR4cMPPyQ+Pp7U1FS6dOmCn58fQ4cO5fDhw3To0IGEhIRipz9OmjSJ/Px8fvrpp2J15OfnM2rUKG655RbPu4Fz3zHVrVuXqKgoRo0axZw5c3jppZeIiooqsb2vvvoqM2fOZM2aNcTGxnLvvfeyefPmKnhkjDG+6rIM/Hc2HmTqP78l8aeVfOY/ifdGwJ7vvuHxP0+nS5cuvPbaa9xxxx3UrVvXc59p06aRkpJC9+7dmTlzJuPHj/dsq1+/Pi6Xi+eff96z7uuvv0ZEiI6OJjk5mW+++YaNGzfidDoJCgrioYceolWrVowePZr69etz44030rx5c5YuLf2nfBMTE3nuuef47rvvaNOmDd27d2fWrFlV8yAZY3zOZRn4M1Z8z8CCT3jS72WaOY4zuI2L928NZmCMsOS1V7jyyivp3bs3+/btK3a/zp07M3nyZFauXMlbb71VbJufnx8vvvii5wtNixcvprCwkJYtW9K6dWsyMzMZPXo099xzDwAPPfQQMTEx1K9fn+XLl/PFF1/wxhtvMGjQoFLbnZ+fz7vvvsstt9zCSy+9xOOPP85tt91WuQ+OMcZnXZaBf+jkaf7gWkKw/P+4+6DWLpbfHMjWyY2ZOnUqx44d47bbbmPfvn0UFBR4zqBJSEhg0KBBREdHF9uniDB8+HByc3MpLCzkzTffRESoX78+derUISIigry8PB577DEAHA4HTzzxBAcOHCA8PJyyPoCeNWsW7dq146233mLy5Mls2bKFhx9+2HM6qDHGeOuy/OJVk/pBNDl9vMRtDfMPc99993Hffffx1Vdf4XQ6+eCDDxg1ahS7d+8mKCiIoKAg5syZQ1JS8Q+6Z82axaxZs1i9ejVNmzZl//79nm0FBQU0a9bMc/2c8PBwz5eppk2bxlNPPcXVV19dapvj4uLYtGnTRb+1a4wx3rgsT8t8Z+NBur1zFU2lhNAPbQ6Tt3jZOmOMqZ2q7LRMEQkTkZUistN9W+qJ5CJST0QOisiz3tRZHjd0acqhxD9wmoDiG/yCIPnRqq7eGGNqJW/H8KcAKaraFkhxL5fmL8AnXtZXbt2uv4ugG58tOqJHim6vmw1xI6urCcYYU6t4O4Y/DPiVe34BsBp4+PxCIpIIRAIfAiW+1agScSMt4I0xxs3bI/xIVU0DcN9ecEqJiDiA/wIeKmtnIjJBRNaLyPpjx4552TRjjDHnKvMIX0RWAY1L2PRIOev4PfCBqu4v6Qc+zqWqc4A5UPShbTn3b4wxphzKDHxV/XVp20TkiIhEqWqaiEQBR0so1hPoKyK/B0IAfxHJUtWLjfcbY4ypZN6O4b8LjAWedN8uO7+Aqo4+Oy8i44AkC3tjjKl+3o7hPwkMFJGdwED3MiKSJCIve9s4Y4wxleey/OKVMcb4Kp+/Hr4xxhgLfGOM8RkW+MYY4yMs8I0xxkdY4BtjjI+wwDfGGB9hgW+MMT7CAt8YY3yEBb4xxvgIC3xjjPERFvjGGOMjLPCNMcZHWOAbY4yPsMA3xhgfYYFvjDE+wgLfGGN8hAW+Mcb4CAt8Y4zxERb4xhjjIyzwjTHGR1jgG2OMj/Aq8EUkTERWishO922DUsoViMgm9/SuN3UaY4z5Zbw9wp8CpKhqWyDFvVyS06qa4J6u97JOY4wxv4C3gT8MWOCeXwDc4OX+jDHGVBFvAz9SVdMA3LcRpZQLFJH1IrJOREp9URCRCe5y648dO+Zl04wxxpzLVVYBEVkFNC5h0yMVqKeFqh4SkVbARyLyraruPr+Qqs4B5gAkJSVpBfZvjDGmDGUGvqr+urRtInJERKJUNU1EooCjpezjkPt2j4isBroAFwS+McaYquPtkM67wFj3/Fhg2fkFRKSBiAS458OB3sBWL+s1xhhTQd4G/pPAQBHZCQx0LyMiSSLysrtMB2C9iGwGPgaeVFULfGOMqWZlDulcjKqmA8klrF8P3OGeXwN09qYeY4wx3rNv2hpjjI+wwDfGGB9hgW+MMT7CAt8YY3yEBb4xxvgIC3xjjPERFvjGGOMjLPCNMcZHWOAbY4yPsMA3xhgfYYFvjDE+wgLfGGN8hAW+Mcb4CAt8Y4zxERb4xhjjIyzwjTHGR1jgG2OMj7DAN8YYH2GBb4wxPsIC3xhjfIRXgS8iYSKyUkR2um8blFKuhYj8r4hsE5GtItLSm3qNMcZUnLdH+FOAFFVtC6S4l0uyEJihqh2A7sBRL+s1xhhTQd4G/jBggXt+AXDD+QVEpCPgUtWVAKqaparZXtZrjDGmgrwN/EhVTQNw30aUUKYdcFJE/ikiG0Vkhog4vazXGGNMBbnKKiAiq4DGJWx6pAJ19AW6AD8CbwDjgLkl1DUBmADQokWLcu7eGGNMeZQZ+Kr669K2icgREYlS1TQRiaLksfkDwEZV3eO+zztAD0oIfFWdA8wBSEpK0vJ1wRhjTHl4O6TzLjDWPT8WWFZCma+BBiLSyL08ANjqZb3GGGMqyNvAfxIYKCI7gYHuZUQkSUReBlDVAuBBIEVEvgUEeMnLeo0xxlRQmUM6F6Oq6UByCevXA3ecs7wSiPOmLmOMMd6xb9oaY4yPsMA3xhgf4dWQjjHGmKo3depUrr76ak6ePMn27duZMqW0ixpcnB3hG2NMLffll19y5ZVX8sknn9C3b99fvB87wjfGmFrqoYceYsWKFezdu5eePXuye/duUlJSGDFiBI8++miF9yeqtfP7TUlJSbp+/fqaboYxxtSor776ildeeYVZs2bxq1/9ii+++OKi5UVkg6omlbTNjvCNMaa2SF0CKY9D5gEIbQbJj7Jx4wkSEhLYvn07HTt29Gr3FvjGGFMbpC6B5ZMg7zQAm77fx7inRnMgJ4jwyCZkZ2ejqiQkJLB27VqCgoIqXIV9aGuMMbVByuOesAdIaOxk013BtKufz9atWxkwYAArVqxg06ZNvyjswQLfGGNqh8wDF6w6dqqQBn55OByOShnSscA3xpjaILTZBasa1XHw/sQrAFi3bp3XVVjgG2NMbZD8KPidN1TjF1S0vpJY4BtjTG0QNxKumw2hzQEpur1udtH6SmJn6RhjTG0RN7JSA/58doRvjDE+wgLfGGN8hAW+Mcb4CAt8Y4zxERb4xhjjI2rt1TJF5BjwQ023oxKFA8druhFVzBf6CL7RT+vjpStaVRuVtKHWBv7lRkTWl3bJ0suFL/QRfKOf1sfLkw3pGGOMj7DAN8YYH2GBX33m1HQDqoEv9BF8o5/Wx8uQjeEbY4yPsCN8Y4zxERb4xhjjIyzwq4iIhInIShHZ6b5tUEKZBBFZKyLfiUiqiIyqibZWlIgMFpHvRWSXiEwpYXuAiLzh3v6liLSs/lZ6pxx9fEBEtrr/bikiEl0T7fRWWf08p9wIEVERueROYyxPH0VkpPvv+Z2IvFbdbaw2qmpTFUzAU8AU9/wU4G8llGkHtHXPNwHSgPo13fYy+uUEdgOtAH9gM9DxvDK/B15wz98MvFHT7a6CPvYHgt3zEy+1Ppa3n+5ydYFPgXVAUk23uwr+lm2BjUAD93JETbe7qiY7wq86w4AF7vkFwA3nF1DVHaq60z1/CDgKlPgNuVqkO7BLVfeo6hngdYr6eq5z+74USBYRqcY2eqvMPqrqx6qa7V5cB1z4+3S1X3n+lgB/oegAJqc6G1dJytPHO4HnVPUEgKoereY2VhsL/KoTqappAO7biIsVFpHuFB2B7K6GtnmjKbD/nOUD7nUlllHVfCATaFgtrasc5enjuW4H/lWlLaoaZfZTRLoAzVX1vepsWCUqz9+yHdBORL4QkXUiMrjaWlfN7BevvCAiq4DGJWx6pIL7iQJeAcaqamFltK0KlXSkfv65veUpU5uVu/0ichuQBPSr0hZVjYv2U0QcwNPAuOpqUBUoz9/SRdGwzq8oeqf2mYh0UtWTVdy2ameB7wVV/XVp20TkiIhEqWqaO9BLfJsoIvWA94Fpqur9z9JXvQNA83OWmwGHSilzQERcQCiQUT3NqxTl6SMi8muKXtz7qWpuNbWtMpXVz7pAJ2C1e0SuMfCuiFyvquurrZXeKe/zdZ2q5gF7ReR7il4Avq6eJlYfG9KpOu8CY93zY4Fl5xcQEX/gbWChqr5ZjW3zxtdAWxGJcbf/Zor6eq5z+z4C+Ejdn4ZdIsrso3uo40Xg+kt4zPei/VTVTFUNV9WWqtqSos8qLqWwh/I9X9+h6EN4RCScoiGePdXaympigV91ngQGishOYKB7GRFJEpGX3WVGAlcB40Rkk3tKqJnmlo97TP4eYAWwDViiqt+JyOMicr272FygoYjsAh6g6CylS0Y5+zgDCAHedP/dzg+RWq+c/byklbOPK4B0EdkKfAw8pKrpNdPiqmWXVjDGGB9hR/jGGOMjLPCNMcZHWOAbY4yPsMA3xhgfYYFvjDE+wgLfGGN8hAW+Mcb4iP8DMLRVW+PWbtQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = [key for key in cm.vocabulary.keys()]\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    \n",
    "    #if re.match('^co[rv]', word):\n",
    "    \n",
    "        x = df['Comp 1'][i]\n",
    "        y = df['Comp 2'][i]\n",
    "\n",
    "        plt.scatter(x, y)\n",
    "        plt.text(x, y, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set word embeddings for text classification\n",
    "ica = FastICA(n_components = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.99182570e-05, -1.09463962e-04, -7.73219957e-04, ...,\n",
       "         1.48702659e-03, -1.81285647e-03,  1.26224956e-03],\n",
       "       [ 2.32312147e-03, -1.44037186e-03,  4.69187119e-04, ...,\n",
       "         5.28556885e-04, -6.20829888e-04, -5.78904213e-04],\n",
       "       [-1.52774760e-03, -1.25965695e-03, -7.39504369e-04, ...,\n",
       "        -1.92556059e-03, -5.04695894e-03,  6.60500938e-03],\n",
       "       ...,\n",
       "       [-7.64287845e-04,  3.22916214e-03, -1.48470944e-04, ...,\n",
       "         2.68479892e-03, -6.85885228e-04, -1.15466562e-02],\n",
       "       [ 2.16015969e-03, -3.62573571e-03,  5.90032136e-03, ...,\n",
       "        -1.39294189e-03, -1.93007051e-03,  4.61185532e-03],\n",
       "       [ 3.98456572e-03, -1.24162671e-03,  2.59703729e-03, ...,\n",
       "         6.69273646e-03, -7.25654308e-03, -6.96256955e-03]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = ica.fit_transform(X_std)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out target\n",
    "y = tweets['Is_Unreliable']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derive text vectors from word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_vectors(word_embeddings,\n",
    "                     word_index_dict,\n",
    "                     text_list,\n",
    "                     remove_stopwords = True,\n",
    "                     lowercase = True,\n",
    "                     lemmatize = True,\n",
    "                     add_start_end_tokens = True):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    for k in range(len(text_list)):\n",
    "        text = text_list[k]\n",
    "        text = re.sub(r'[_~`@$%^&*[\\]+=\\|}{\\\"\\'<>/]+', '', text)\n",
    "        text_vec = np.zeros(word_embeddings.shape[1])\n",
    "        words = word_tokenize(text)\n",
    "        tracker = 0 # to track whether we've encountered a word for which we have an embedding (in each tweet)\n",
    "        \n",
    "        if remove_stopwords:\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                if word.lower() not in set(stopwords.words('english')):\n",
    "                    clean_words.append(word)\n",
    "            words = clean_words\n",
    "\n",
    "        if lowercase:\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                clean_words.append(word.lower())\n",
    "\n",
    "            words = clean_words\n",
    "\n",
    "        if lemmatize:\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                PoS_tag = pos_tag([word])[0][1]\n",
    "\n",
    "                # to change contractions to full word form\n",
    "                if word in contractions:\n",
    "                    word = contractions[word]\n",
    "\n",
    "                if PoS_tag[0].upper() in 'JNVR':\n",
    "                    word = lemmatizer.lemmatize(word, convert_pos_wordnet(PoS_tag))\n",
    "                else:\n",
    "                    word = lemmatizer.lemmatize(word)\n",
    "\n",
    "                clean_words.append(word)\n",
    "\n",
    "            words = clean_words\n",
    "\n",
    "        if add_start_end_tokens:\n",
    "            words = ['<START>'] + words + ['<END>']\n",
    "        \n",
    "        for i in range(len(words)):\n",
    "            word = words[i]\n",
    "            if word in word_index_dict:\n",
    "                word_embed_vec = word_embeddings[word_index_dict[word],:]\n",
    "                if tracker == 0:\n",
    "                    text_matrix = word_embed_vec\n",
    "                else:\n",
    "                    text_matrix = np.vstack((text_matrix, word_embed_vec))\n",
    "                    \n",
    "                # only increment if we have come across a word in the embeddings dictionary\n",
    "                tracker += 1\n",
    "                    \n",
    "        for j in range(len(text_vec)):\n",
    "            text_vec[j] = text_matrix[:,j].mean()\n",
    "            \n",
    "        if k == 0:\n",
    "            full_matrix = text_vec\n",
    "        else:\n",
    "            full_matrix = np.vstack((full_matrix, text_vec))\n",
    "            \n",
    "    return full_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_text_vectors(embeddings, cm.vocabulary, tweets['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(560, 500)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.75886925e-05, -6.86344542e-04,  4.36490190e-05, ...,\n",
       "         1.95034402e-03, -6.76130908e-04,  1.53121777e-03],\n",
       "       [-1.69146007e-02, -3.32848763e-04,  4.47220149e-03, ...,\n",
       "        -1.54179797e-03,  2.04563322e-04,  5.81564422e-05],\n",
       "       [-1.39796812e-03, -1.11950818e-03, -8.95217323e-04, ...,\n",
       "         6.61087423e-04, -9.34907108e-04,  1.30108110e-03],\n",
       "       ...,\n",
       "       [ 1.23049558e-04, -7.02400422e-04,  6.05188018e-04, ...,\n",
       "        -3.27204540e-04,  1.59288666e-03,  4.69577155e-04],\n",
       "       [-3.02721970e-03, -2.90740681e-04, -5.76557374e-04, ...,\n",
       "         4.95322424e-04,  3.94471599e-05, -2.34930365e-04],\n",
       "       [-3.25031342e-03,  4.64609750e-04, -3.09609989e-05, ...,\n",
       "         2.27898915e-04,  2.16300665e-04, -1.67063693e-03]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification - nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, KFold, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "pipe = Pipeline([\n",
    "    ('classify', SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC hyperparams to optimize\n",
    "kernel = ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "C = [0.001, 0.01, 0.1, 1, 10]\n",
    "# set up parameter grid\n",
    "params = {\n",
    "    'classify__kernel': kernel,\n",
    "    'classify__C': C\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CV scheme for inner and outer loops\n",
    "inner_cv = KFold(n_splits = 3, shuffle = True, random_state = 1)\n",
    "outer_cv = KFold(n_splits = 5, shuffle = True, random_state = 1)\n",
    "\n",
    "# Set up GridSearch for inner loop\n",
    "grid_SVC = GridSearchCV(pipe, params, cv = inner_cv)\n",
    "#grid_SVC.fit(X, y)\n",
    "\n",
    "# Nested CV scores\n",
    "scores = cross_validate(grid_SVC,\n",
    "                        X = X,\n",
    "                        y = y,\n",
    "                        cv = outer_cv,\n",
    "                        scoring = ['roc_auc', 'accuracy', 'f1', 'precision', 'recall'],\n",
    "                        return_estimator = True)\n",
    "auc = scores['test_roc_auc']\n",
    "accuracy = scores['test_accuracy']\n",
    "f1 = scores['test_f1']\n",
    "precision = scores['test_precision']\n",
    "recall = scores['test_recall']\n",
    "estimators = scores['estimator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9258391842763981"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8696428571428572"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8772189721867141"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8280464480874317"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9340659912493097"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classify__C': 10, 'classify__kernel': 'rbf'}\n",
      "\n",
      "\n",
      "{'classify__C': 10, 'classify__kernel': 'rbf'}\n",
      "\n",
      "\n",
      "{'classify__C': 1, 'classify__kernel': 'rbf'}\n",
      "\n",
      "\n",
      "{'classify__C': 10, 'classify__kernel': 'linear'}\n",
      "\n",
      "\n",
      "{'classify__C': 1, 'classify__kernel': 'rbf'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in estimators:\n",
    "    print(i.best_params_)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
