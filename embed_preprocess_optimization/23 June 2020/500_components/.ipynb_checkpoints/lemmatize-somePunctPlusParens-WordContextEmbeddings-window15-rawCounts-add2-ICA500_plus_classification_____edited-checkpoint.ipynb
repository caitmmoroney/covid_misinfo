{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Word Context Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.decomposition import FastICA, TruncatedSVD, PCA, NMF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to convert contractions picked up by word_tokenize() into full words\n",
    "contractions = {\n",
    "    \"n't\": 'not',\n",
    "    \"'ve\": 'have',\n",
    "    \"'s\": 'is', # note that this will include possessive nouns\n",
    "    'gonna': 'going to',\n",
    "    'gotta': 'got to',\n",
    "    \"'d\": 'would',\n",
    "    \"'ll\": 'will',\n",
    "    \"'re\": 'are',\n",
    "    \"'m\": 'am',\n",
    "    'wanna': 'want to'\n",
    "}\n",
    "\n",
    "# to convert nltk_pos tags to wordnet-compatible PoS tags\n",
    "def convert_pos_wordnet(tag):\n",
    "    tag_abbr = tag[0].upper()\n",
    "    tag_dict = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "                \n",
    "    if tag_abbr in tag_dict:\n",
    "        return tag_dict[tag_abbr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextMatrix(TransformerMixin):\n",
    "    \n",
    "    # initialize class & private variables\n",
    "    def __init__(self,\n",
    "                 window_size = 4,\n",
    "                 remove_stopwords = True,\n",
    "                 add_start_end_tokens = True,\n",
    "                 lowercase = False,\n",
    "                 lemmatize = False,\n",
    "                 pmi = False,\n",
    "                 spmi_k = 1,\n",
    "                 laplace_smoothing = 0,\n",
    "                 pmi_positive = False,\n",
    "                 sppmi_k = 1):\n",
    "        \n",
    "        \"\"\" Params:\n",
    "                window_size: size of +/- context window (default = 4)\n",
    "                remove_stopwords: boolean, whether or not to remove NLTK English stopwords\n",
    "                add_start_end_tokens: boolean, whether or not to append <START> and <END> to the\n",
    "                beginning/end of each document in the corpus (default = True)\n",
    "                lowercase: boolean, whether or not to convert words to all lowercase\n",
    "                lemmatize: boolean, whether or not to lemmatize input text\n",
    "                pmi: boolean, whether or not to compute pointwise mutual information\n",
    "                pmi_positive: boolean, whether or not to compute positive PMI\n",
    "        \"\"\"\n",
    "        self.window_size = window_size\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.add_start_end_tokens = add_start_end_tokens\n",
    "        self.lowercase = lowercase\n",
    "        self.lemmatize = lemmatize\n",
    "        self.pmi = pmi\n",
    "        self.spmi_k = spmi_k\n",
    "        self.laplace_smoothing = laplace_smoothing\n",
    "        self.pmi_positive = pmi_positive\n",
    "        self.sppmi_k = sppmi_k\n",
    "        self.corpus = None\n",
    "        self.clean_corpus = None\n",
    "        self.vocabulary = None\n",
    "        self.X = None\n",
    "        self.doc_terms_lists = None\n",
    "    \n",
    "    def fit(self, corpus, y = None):\n",
    "        \n",
    "        \"\"\" Learn the dictionary of all unique tokens for given corpus.\n",
    "        \n",
    "            Params:\n",
    "                corpus: list of strings\n",
    "            \n",
    "            Returns: self\n",
    "        \"\"\"\n",
    "        self.corpus = corpus\n",
    "        \n",
    "        term_dict = dict()\n",
    "        k = 0\n",
    "        corpus_words = []\n",
    "        clean_corpus = []\n",
    "        doc_terms_lists = []\n",
    "        detokenizer = TreebankWordDetokenizer()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        for text in corpus:\n",
    "            text = re.sub(r'[_~`@$%^&*[\\]+=\\|}{\\\"\\'<>/]+', '', text)\n",
    "            \n",
    "            words = word_tokenize(text)\n",
    "            \n",
    "            if self.remove_stopwords:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    if word.lower() not in set(stopwords.words('english')):\n",
    "                        clean_words.append(word)\n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lowercase:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    clean_words.append(word.lower())\n",
    "                \n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lemmatize:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    PoS_tag = pos_tag([word])[0][1]\n",
    "                    \n",
    "                    # to change contractions to full word form\n",
    "                    if word in contractions:\n",
    "                        word = contractions[word]\n",
    "\n",
    "                    if PoS_tag[0].upper() in 'JNVR':\n",
    "                        word = lemmatizer.lemmatize(word, convert_pos_wordnet(PoS_tag))\n",
    "                    else:\n",
    "                        word = lemmatizer.lemmatize(word)\n",
    "\n",
    "                    clean_words.append(word)\n",
    "                    \n",
    "                words = clean_words\n",
    "            \n",
    "            # detokenize trick taken from this StackOverflow post:\n",
    "            # https://stackoverflow.com/questions/21948019/python-untokenize-a-sentence\n",
    "            # and NLTK treebank documentation:\n",
    "            # https://www.nltk.org/_modules/nltk/tokenize/treebank.html\n",
    "            text = detokenizer.detokenize(words)\n",
    "            clean_corpus.append(text)\n",
    "            \n",
    "            [corpus_words.append(word) for word in words]\n",
    "            \n",
    "            if self.add_start_end_tokens:\n",
    "                words = ['<START>'] + words + ['<END>']\n",
    "            \n",
    "            doc_terms_lists.append(words)\n",
    "            \n",
    "        self.clean_corpus = clean_corpus\n",
    "        \n",
    "        self.doc_terms_lists = doc_terms_lists\n",
    "        \n",
    "        corpus_words = list(set(corpus_words))\n",
    "        \n",
    "        if self.add_start_end_tokens:\n",
    "            corpus_words = ['<START>'] + corpus_words + ['<END>']\n",
    "        \n",
    "        corpus_words = sorted(corpus_words)\n",
    "        \n",
    "        for el in corpus_words:\n",
    "            term_dict[el] = k\n",
    "            k += 1\n",
    "            \n",
    "        self.vocabulary = term_dict\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def transform(self, new_corpus = None, y = None):\n",
    "        \n",
    "        \"\"\" Compute the co-occurrence matrix for given corpus and window_size, using term dictionary\n",
    "            obtained with fit method.\n",
    "        \n",
    "            Returns: term-context co-occurrence matrix (shape: target terms by context terms) with\n",
    "            raw counts\n",
    "        \"\"\"\n",
    "        num_terms = len(self.vocabulary)\n",
    "        window = self.window_size\n",
    "        X = np.full((num_terms, num_terms), self.laplace_smoothing)\n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        if type(new_corpus) != list:\n",
    "            new_corpus = self.corpus\n",
    "        \n",
    "        for text in new_corpus:\n",
    "            text = re.sub(r'[_~`@$%^&*[\\]+=\\|}{\\\"\\'<>/]+', '', text)\n",
    "            \n",
    "            words = word_tokenize(text)\n",
    "            \n",
    "            if self.remove_stopwords:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    if word.lower() not in set(stopwords.words('english')):\n",
    "                        clean_words.append(word)\n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lowercase:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    clean_words.append(word.lower())\n",
    "                \n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lemmatize:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    PoS_tag = pos_tag([word])[0][1]\n",
    "                    \n",
    "                    # to change contractions to full word form\n",
    "                    if word in contractions:\n",
    "                        word = contractions[word]\n",
    "\n",
    "                    if PoS_tag[0].upper() in 'JNVR':\n",
    "                        word = lemmatizer.lemmatize(word, convert_pos_wordnet(PoS_tag))\n",
    "                    else:\n",
    "                        word = lemmatizer.lemmatize(word)\n",
    "\n",
    "                    clean_words.append(word)\n",
    "                    \n",
    "                words = clean_words\n",
    "                \n",
    "            if self.add_start_end_tokens:\n",
    "                words = ['<START>'] + words + ['<END>']\n",
    "            \n",
    "            for i in range(len(words)):\n",
    "                target = words[i]\n",
    "                \n",
    "                # check to see if target word is in the dictionary; if not, skip\n",
    "                if target in self.vocabulary:\n",
    "                    \n",
    "                    # grab index from dictionary\n",
    "                    target_dict_index = self.vocabulary[target]\n",
    "                    \n",
    "                    # find left-most and right-most window indices for each target word\n",
    "                    left_end_index = max(i - window, 0)\n",
    "                    right_end_index = min(i + window, len(words) - 1)\n",
    "                    \n",
    "                    # loop over all words within window\n",
    "                    # NOTE: this will include the target word; make sure to skip over it\n",
    "                    for j in range(left_end_index, right_end_index + 1):\n",
    "                        \n",
    "                        # skip \"context word\" where the \"context word\" index is equal to the\n",
    "                        # target word index\n",
    "                        if j != i:\n",
    "                            context_word = words[j]\n",
    "                            \n",
    "                            # check to see if context word is in the fitted dictionary; if\n",
    "                            # not, skip\n",
    "                            if context_word in self.vocabulary:\n",
    "                                X[target_dict_index, self.vocabulary[context_word]] += 1\n",
    "        \n",
    "        # if pmi = True, compute pmi matrix from word-context raw frequencies\n",
    "        # more concise code taken from this StackOverflow post:\n",
    "        # https://stackoverflow.com/questions/58701337/how-to-construct-ppmi-matrix-from-a-text-corpus\n",
    "        if self.pmi:\n",
    "            denom = X.sum()\n",
    "            col_sums = X.sum(axis = 0)\n",
    "            row_sums = X.sum(axis = 1)\n",
    "            \n",
    "            expected = np.outer(row_sums, col_sums)/denom\n",
    "            \n",
    "            X = X/expected\n",
    "            \n",
    "            for i in range(X.shape[0]):\n",
    "                for j in range(X.shape[1]):\n",
    "                \n",
    "                    if X[i,j] > 0:\n",
    "                        X[i,j] = np.log(X[i,j]) - np.log(self.spmi_k)\n",
    "                        \n",
    "                        if self.pmi_positive:\n",
    "                            X[i,j] = max(X[i,j] - np.log(self.sppmi_k), 0)\n",
    "        \n",
    "        # note that X is a dense matrix\n",
    "        self.X = X\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ContextMatrix(lowercase = True, lemmatize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [\n",
    "    \"Coronavirus is a fake liberal hoax.\",\n",
    "    \"Trump won't do anything about coronavirus.\",\n",
    "    \"The liberal fake news media always blame Pres Trump.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ContextMatrix at 0x1a20b66d50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.fit(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.</th>\n",
       "      <th>&lt;END&gt;</th>\n",
       "      <th>&lt;START&gt;</th>\n",
       "      <th>always</th>\n",
       "      <th>anything</th>\n",
       "      <th>blame</th>\n",
       "      <th>coronavirus</th>\n",
       "      <th>fake</th>\n",
       "      <th>hoax</th>\n",
       "      <th>liberal</th>\n",
       "      <th>medium</th>\n",
       "      <th>news</th>\n",
       "      <th>pres</th>\n",
       "      <th>trump</th>\n",
       "      <th>wont</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;END&gt;</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;START&gt;</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>always</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anything</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blame</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coronavirus</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fake</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hoax</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liberal</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medium</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pres</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wont</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             .  <END>  <START>  always  anything  blame  coronavirus  fake  \\\n",
       ".            0      3        0       1         1      1            2     1   \n",
       "<END>        3      0        0       0         1      1            1     1   \n",
       "<START>      0      0        0       0         1      0            2     2   \n",
       "always       1      0        0       0         0      1            0     1   \n",
       "anything     1      1        1       0         0      0            1     0   \n",
       "blame        1      1        0       1         0      0            0     1   \n",
       "coronavirus  2      1        2       0         1      0            0     1   \n",
       "fake         1      1        2       1         0      1            1     0   \n",
       "hoax         1      1        1       0         0      0            1     1   \n",
       "liberal      1      1        2       1         0      0            1     2   \n",
       "medium       0      0        1       1         0      1            0     1   \n",
       "news         0      0        1       1         0      1            0     1   \n",
       "pres         1      1        0       1         0      1            0     0   \n",
       "trump        2      1        1       1         1      1            1     0   \n",
       "wont         1      1        1       0         1      0            1     0   \n",
       "\n",
       "             hoax  liberal  medium  news  pres  trump  wont  \n",
       ".               1        1       0     0     1      2     1  \n",
       "<END>           1        1       0     0     1      1     1  \n",
       "<START>         1        2       1     1     0      1     1  \n",
       "always          0        1       1     1     1      1     0  \n",
       "anything        0        0       0     0     0      1     1  \n",
       "blame           0        0       1     1     1      1     0  \n",
       "coronavirus     1        1       0     0     0      1     1  \n",
       "fake            1        2       1     1     0      0     0  \n",
       "hoax            0        1       0     0     0      0     0  \n",
       "liberal         1        0       1     1     0      0     0  \n",
       "medium          0        1       0     1     1      1     0  \n",
       "news            0        1       1     0     1      0     0  \n",
       "pres            0        0       1     1     0      1     0  \n",
       "trump           0        0       1     0     1      0     1  \n",
       "wont            0        0       0     0     0      1     0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cm.transform(tweets), index = cm.vocabulary, columns = cm.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coronavirus fake liberal hoax.',\n",
       " 'trump wont anything coronavirus.',\n",
       " 'liberal fake news medium always blame pres trump.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.clean_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Coronavirus is a fake liberal hoax.',\n",
       " \"Trump won't do anything about coronavirus.\",\n",
       " 'The liberal fake news media always blame Pres Trump.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train embeddings using tweets as corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('COVID19_Dataset-text_labels_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Is_Unreliable</th>\n",
       "      <th>Category</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 3, 6, 9</td>\n",
       "      <td>We are living in scary times in Canada. Gov’t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 6, 8, 9</td>\n",
       "      <td>Just as bad in Canada. In fact, our government...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 4, 9</td>\n",
       "      <td>It was only a matter of time before the mainst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6, 8</td>\n",
       "      <td>Russia's taking no chances: Foreigners infecte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>6, 8, 9</td>\n",
       "      <td>Although there is now a presumptive confirmed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BREAKING: Harvard classes will move online sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Singularity University is hosting a FREE Virtu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coronavirus: how does it spread and what are t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stanford just cancelled classes for the rest o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tech conferences were cancelled in #Waterloo R...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Is_Unreliable    Category  \\\n",
       "0                1  1, 3, 6, 9   \n",
       "1                1  1, 6, 8, 9   \n",
       "2                1     1, 4, 9   \n",
       "3                1        6, 8   \n",
       "4                1     6, 8, 9   \n",
       "..             ...         ...   \n",
       "555              0         NaN   \n",
       "556              0         NaN   \n",
       "557              0         NaN   \n",
       "558              0         NaN   \n",
       "559              0         NaN   \n",
       "\n",
       "                                                 Tweet  \n",
       "0    We are living in scary times in Canada. Gov’t ...  \n",
       "1    Just as bad in Canada. In fact, our government...  \n",
       "2    It was only a matter of time before the mainst...  \n",
       "3    Russia's taking no chances: Foreigners infecte...  \n",
       "4    Although there is now a presumptive confirmed ...  \n",
       "..                                                 ...  \n",
       "555  BREAKING: Harvard classes will move online sta...  \n",
       "556  Singularity University is hosting a FREE Virtu...  \n",
       "557  Coronavirus: how does it spread and what are t...  \n",
       "558  Stanford just cancelled classes for the rest o...  \n",
       "559  Tech conferences were cancelled in #Waterloo R...  \n",
       "\n",
       "[560 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ContextMatrix(window_size = 15, lowercase = True, lemmatize = True, laplace_smoothing = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ContextMatrix at 0x1a286d6d90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.fit(tweets['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_context_matrix = cm.transform(tweets['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>#</th>\n",
       "      <th>(</th>\n",
       "      <th>)</th>\n",
       "      <th>,</th>\n",
       "      <th>-</th>\n",
       "      <th>--</th>\n",
       "      <th>.</th>\n",
       "      <th>...</th>\n",
       "      <th>1</th>\n",
       "      <th>...</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoomer</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>—</th>\n",
       "      <th>‘</th>\n",
       "      <th>’</th>\n",
       "      <th>“</th>\n",
       "      <th>”</th>\n",
       "      <th>❝real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>126</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <td>32</td>\n",
       "      <td>652</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>115</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>217</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>65</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>)</th>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>12</td>\n",
       "      <td>115</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>140</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>159</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>47</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‘</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>’</th>\n",
       "      <td>31</td>\n",
       "      <td>65</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>47</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>88</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>62</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>“</th>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>”</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>❝real</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2325 rows × 2325 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         !    #   (   )    ,   -  --    .  ...  1  ...  zombie  zone  zoomer  \\\n",
       "!      126   32   2   3   12   5   2   39    9  5  ...       2     2       2   \n",
       "#       32  652  11  11  115  21   5  217    3  3  ...       2     3       3   \n",
       "(        2   11   4  34   15   3   2   19    3  2  ...       2     2       2   \n",
       ")        3   11  34   4   15   3   2   22    3  2  ...       2     2       2   \n",
       ",       12  115  15  15  140  14   6  159    5  6  ...       5     3       2   \n",
       "...    ...  ...  ..  ..  ...  ..  ..  ...  ... ..  ...     ...   ...     ...   \n",
       "‘        2    6   2   2    5   2   2    6    2  2  ...       2     2       2   \n",
       "’       31   65   6   6   47   8   2   88    7  2  ...       2     2       2   \n",
       "“       10    7   3   3   17   5   3   19    3  3  ...       2     2       2   \n",
       "”        4    6   2   2   10   4   3   15    3  2  ...       2     2       2   \n",
       "❝real    3    2   2   2    2   2   2    2    2  2  ...       2     2       2   \n",
       "\n",
       "       zuckerberg  —   ‘   ’   “   ”  ❝real  \n",
       "!               2  3   2  31  10   4      3  \n",
       "#               2  4   6  65   7   6      2  \n",
       "(               2  2   2   6   3   2      2  \n",
       ")               2  2   2   6   3   2      2  \n",
       ",               2  4   5  47  17  10      2  \n",
       "...           ... ..  ..  ..  ..  ..    ...  \n",
       "‘               2  2   2  11   2   2      2  \n",
       "’               2  2  11  62  15  14      3  \n",
       "“               2  3   2  15   6  22      2  \n",
       "”               2  3   2  14  22  10      2  \n",
       "❝real           2  2   2   3   2   2      2  \n",
       "\n",
       "[2325 rows x 2325 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(word_context_matrix, index = cm.vocabulary, columns = cm.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2325, 2325)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_context_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica = FastICA(n_components = 2)\n",
    "std_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.53922663e-02,  7.51312563e-04],\n",
       "       [-3.73450316e-03,  9.79742561e-01],\n",
       "       [-2.65874962e-02, -9.89900119e-03],\n",
       "       ...,\n",
       "       [-2.75929799e-02, -1.45165719e-02],\n",
       "       [-1.98243231e-02, -1.12895179e-02],\n",
       "       [ 2.52199006e-03,  3.58539452e-05]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_std = std_scaler.fit_transform(word_context_matrix)\n",
    "\n",
    "matrix = ica.fit_transform(X_std)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comp 1</th>\n",
       "      <th>Comp 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>-0.055392</td>\n",
       "      <td>0.000751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <td>-0.003735</td>\n",
       "      <td>0.979743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(</th>\n",
       "      <td>-0.026587</td>\n",
       "      <td>-0.009899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>)</th>\n",
       "      <td>-0.027682</td>\n",
       "      <td>-0.010770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>-0.276158</td>\n",
       "      <td>-0.062943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‘</th>\n",
       "      <td>-0.004372</td>\n",
       "      <td>-0.002390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>’</th>\n",
       "      <td>-0.112205</td>\n",
       "      <td>0.003403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>“</th>\n",
       "      <td>-0.027593</td>\n",
       "      <td>-0.014517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>”</th>\n",
       "      <td>-0.019824</td>\n",
       "      <td>-0.011290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>❝real</th>\n",
       "      <td>0.002522</td>\n",
       "      <td>0.000036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2325 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Comp 1    Comp 2\n",
       "!     -0.055392  0.000751\n",
       "#     -0.003735  0.979743\n",
       "(     -0.026587 -0.009899\n",
       ")     -0.027682 -0.010770\n",
       ",     -0.276158 -0.062943\n",
       "...         ...       ...\n",
       "‘     -0.004372 -0.002390\n",
       "’     -0.112205  0.003403\n",
       "“     -0.027593 -0.014517\n",
       "”     -0.019824 -0.011290\n",
       "❝real  0.002522  0.000036\n",
       "\n",
       "[2325 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(matrix,\n",
    "                  index = cm.vocabulary,\n",
    "                  columns = ['Comp {}'.format(i+1) for i in range(2)])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caitlinmoroney/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 62222 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/caitlinmoroney/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 62222 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/caitlinmoroney/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 12540 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/caitlinmoroney/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 12540 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = [key for key in cm.vocabulary.keys()]\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    \n",
    "    #if re.match('^co[rv]', word):\n",
    "    \n",
    "        x = df['Comp 1'][i]\n",
    "        y = df['Comp 2'][i]\n",
    "\n",
    "        plt.scatter(x, y)\n",
    "        plt.text(x, y, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set word embeddings for text classification\n",
    "ica = FastICA(n_components = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.00439132e-04,  2.17406723e-03,  5.97972010e-04, ...,\n",
       "         1.18864933e-04, -1.13189427e-03,  6.74735531e-03],\n",
       "       [-7.16422144e-04, -3.34855198e-03, -1.47280485e-03, ...,\n",
       "        -1.00383439e-03,  4.92412073e-03, -9.78325049e-04],\n",
       "       [ 6.85566722e-04,  4.43612668e-04, -1.92795912e-03, ...,\n",
       "        -6.08943825e-04, -2.18513606e-03,  4.85497636e-03],\n",
       "       ...,\n",
       "       [ 8.65834230e-03, -1.77062919e-02,  2.33462143e-03, ...,\n",
       "         4.21473884e-03,  7.02204834e-03,  3.21881509e-01],\n",
       "       [-9.73821871e-04,  1.65858443e-03, -3.19110106e-03, ...,\n",
       "         4.62085061e-03, -2.04548759e-03, -1.48913593e-02],\n",
       "       [-2.36372094e-03,  4.34593280e-03, -2.53292473e-03, ...,\n",
       "         8.10431739e-03, -3.92977623e-03,  8.97561912e-04]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = ica.fit_transform(X_std)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out target\n",
    "y = tweets['Is_Unreliable']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derive text vectors from word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_vectors(word_embeddings,\n",
    "                     word_index_dict,\n",
    "                     text_list,\n",
    "                     remove_stopwords = True,\n",
    "                     lowercase = True,\n",
    "                     lemmatize = True,\n",
    "                     add_start_end_tokens = True):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    for k in range(len(text_list)):\n",
    "        text = text_list[k]\n",
    "        text = re.sub(r'[_~`@$%^&*[\\]+=\\|}{\\\"\\'<>/]+', '', text)\n",
    "        text_vec = np.zeros(word_embeddings.shape[1])\n",
    "        words = word_tokenize(text)\n",
    "        tracker = 0 # to track whether we've encountered a word for which we have an embedding (in each tweet)\n",
    "        \n",
    "        if remove_stopwords:\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                if word.lower() not in set(stopwords.words('english')):\n",
    "                    clean_words.append(word)\n",
    "            words = clean_words\n",
    "\n",
    "        if lowercase:\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                clean_words.append(word.lower())\n",
    "\n",
    "            words = clean_words\n",
    "\n",
    "        if lemmatize:\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                PoS_tag = pos_tag([word])[0][1]\n",
    "\n",
    "                # to change contractions to full word form\n",
    "                if word in contractions:\n",
    "                    word = contractions[word]\n",
    "\n",
    "                if PoS_tag[0].upper() in 'JNVR':\n",
    "                    word = lemmatizer.lemmatize(word, convert_pos_wordnet(PoS_tag))\n",
    "                else:\n",
    "                    word = lemmatizer.lemmatize(word)\n",
    "\n",
    "                clean_words.append(word)\n",
    "\n",
    "            words = clean_words\n",
    "\n",
    "        if add_start_end_tokens:\n",
    "            words = ['<START>'] + words + ['<END>']\n",
    "        \n",
    "        for i in range(len(words)):\n",
    "            word = words[i]\n",
    "            if word in word_index_dict:\n",
    "                word_embed_vec = word_embeddings[word_index_dict[word],:]\n",
    "                if tracker == 0:\n",
    "                    text_matrix = word_embed_vec\n",
    "                else:\n",
    "                    text_matrix = np.vstack((text_matrix, word_embed_vec))\n",
    "                    \n",
    "                # only increment if we have come across a word in the embeddings dictionary\n",
    "                tracker += 1\n",
    "                    \n",
    "        for j in range(len(text_vec)):\n",
    "            text_vec[j] = text_matrix[:,j].mean()\n",
    "            \n",
    "        if k == 0:\n",
    "            full_matrix = text_vec\n",
    "        else:\n",
    "            full_matrix = np.vstack((full_matrix, text_vec))\n",
    "            \n",
    "    return full_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_text_vectors(embeddings, cm.vocabulary, tweets['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(560, 500)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.73866180e-04, -1.76913356e-03, -9.50603856e-04, ...,\n",
       "        -5.81647850e-04,  1.53920536e-03, -2.17291937e-03],\n",
       "       [-4.83242332e-03, -2.78047746e-03, -4.62378150e-03, ...,\n",
       "         3.65290798e-04, -5.18771896e-03,  1.85530905e-02],\n",
       "       [ 2.93713568e-04,  1.08276515e-03,  7.09844816e-05, ...,\n",
       "        -2.34956563e-04,  6.57529434e-04, -2.23295282e-03],\n",
       "       ...,\n",
       "       [ 2.42104333e-03,  3.54548094e-03, -1.19359338e-03, ...,\n",
       "        -1.52754568e-03, -3.05753283e-03,  9.48417731e-05],\n",
       "       [ 2.40166030e-04,  1.38628170e-03, -6.89001293e-04, ...,\n",
       "        -9.53972072e-04, -2.97909387e-03, -1.35248212e-03],\n",
       "       [ 1.11878949e-03, -6.46676997e-03, -5.29116171e-05, ...,\n",
       "        -2.83275595e-05, -7.88530934e-04, -1.63258832e-03]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification - nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, KFold, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "pipe = Pipeline([\n",
    "    ('classify', SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC hyperparams to optimize\n",
    "kernel = ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "C = [0.001, 0.01, 0.1, 1, 10]\n",
    "# set up parameter grid\n",
    "params = {\n",
    "    'classify__kernel': kernel,\n",
    "    'classify__C': C\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CV scheme for inner and outer loops\n",
    "inner_cv = KFold(n_splits = 3, shuffle = True, random_state = 1)\n",
    "outer_cv = KFold(n_splits = 5, shuffle = True, random_state = 1)\n",
    "\n",
    "# Set up GridSearch for inner loop\n",
    "grid_SVC = GridSearchCV(pipe, params, cv = inner_cv)\n",
    "#grid_SVC.fit(X, y)\n",
    "\n",
    "# Nested CV scores\n",
    "scores = cross_validate(grid_SVC,\n",
    "                        X = X,\n",
    "                        y = y,\n",
    "                        cv = outer_cv,\n",
    "                        scoring = ['roc_auc', 'accuracy', 'f1', 'precision', 'recall'],\n",
    "                        return_estimator = True)\n",
    "auc = scores['test_roc_auc']\n",
    "accuracy = scores['test_accuracy']\n",
    "f1 = scores['test_f1']\n",
    "precision = scores['test_precision']\n",
    "recall = scores['test_recall']\n",
    "estimators = scores['estimator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9259042884430647"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8696428571428572"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8772189721867141"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8280464480874317"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9340659912493097"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classify__C': 10, 'classify__kernel': 'rbf'}\n",
      "\n",
      "\n",
      "{'classify__C': 10, 'classify__kernel': 'rbf'}\n",
      "\n",
      "\n",
      "{'classify__C': 1, 'classify__kernel': 'rbf'}\n",
      "\n",
      "\n",
      "{'classify__C': 10, 'classify__kernel': 'linear'}\n",
      "\n",
      "\n",
      "{'classify__C': 1, 'classify__kernel': 'rbf'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in estimators:\n",
    "    print(i.best_params_)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
