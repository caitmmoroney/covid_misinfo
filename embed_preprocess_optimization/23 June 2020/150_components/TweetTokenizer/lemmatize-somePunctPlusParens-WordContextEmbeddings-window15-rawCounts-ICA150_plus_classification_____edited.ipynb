{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Word Context Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.decomposition import FastICA, TruncatedSVD, PCA, NMF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to convert contractions picked up by word_tokenize() into full words\n",
    "contractions = {\n",
    "    \"n't\": 'not',\n",
    "    \"'ve\": 'have',\n",
    "    \"'s\": 'is', # note that this will include possessive nouns\n",
    "    'gonna': 'going to',\n",
    "    'gotta': 'got to',\n",
    "    \"'d\": 'would',\n",
    "    \"'ll\": 'will',\n",
    "    \"'re\": 'are',\n",
    "    \"'m\": 'am',\n",
    "    'wanna': 'want to'\n",
    "}\n",
    "\n",
    "# to convert nltk_pos tags to wordnet-compatible PoS tags\n",
    "def convert_pos_wordnet(tag):\n",
    "    tag_abbr = tag[0].upper()\n",
    "    tag_dict = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "                \n",
    "    if tag_abbr in tag_dict:\n",
    "        return tag_dict[tag_abbr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextMatrix(TransformerMixin):\n",
    "    \n",
    "    # initialize class & private variables\n",
    "    def __init__(self,\n",
    "                 window_size = 4,\n",
    "                 remove_stopwords = True,\n",
    "                 add_start_end_tokens = True,\n",
    "                 lowercase = False,\n",
    "                 lemmatize = False,\n",
    "                 pmi = False,\n",
    "                 spmi_k = 1,\n",
    "                 laplace_smoothing = 0,\n",
    "                 pmi_positive = False,\n",
    "                 sppmi_k = 1):\n",
    "        \n",
    "        \"\"\" Params:\n",
    "                window_size: size of +/- context window (default = 4)\n",
    "                remove_stopwords: boolean, whether or not to remove NLTK English stopwords\n",
    "                add_start_end_tokens: boolean, whether or not to append <START> and <END> to the\n",
    "                beginning/end of each document in the corpus (default = True)\n",
    "                lowercase: boolean, whether or not to convert words to all lowercase\n",
    "                lemmatize: boolean, whether or not to lemmatize input text\n",
    "                pmi: boolean, whether or not to compute pointwise mutual information\n",
    "                pmi_positive: boolean, whether or not to compute positive PMI\n",
    "        \"\"\"\n",
    "        self.window_size = window_size\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.add_start_end_tokens = add_start_end_tokens\n",
    "        self.lowercase = lowercase\n",
    "        self.lemmatize = lemmatize\n",
    "        self.pmi = pmi\n",
    "        self.spmi_k = spmi_k\n",
    "        self.laplace_smoothing = laplace_smoothing\n",
    "        self.pmi_positive = pmi_positive\n",
    "        self.sppmi_k = sppmi_k\n",
    "        self.corpus = None\n",
    "        self.clean_corpus = None\n",
    "        self.vocabulary = None\n",
    "        self.X = None\n",
    "        self.doc_terms_lists = None\n",
    "    \n",
    "    def fit(self, corpus, y = None):\n",
    "        \n",
    "        \"\"\" Learn the dictionary of all unique tokens for given corpus.\n",
    "        \n",
    "            Params:\n",
    "                corpus: list of strings\n",
    "            \n",
    "            Returns: self\n",
    "        \"\"\"\n",
    "        self.corpus = corpus\n",
    "        \n",
    "        term_dict = dict()\n",
    "        k = 0\n",
    "        corpus_words = []\n",
    "        clean_corpus = []\n",
    "        doc_terms_lists = []\n",
    "        detokenizer = TreebankWordDetokenizer()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        for text in corpus:\n",
    "            text = re.sub(r'[_~`@$%^&*[\\]+=\\|}{\\\"\\'<>/]+', '', text)\n",
    "            \n",
    "            words = word_tokenize(text)\n",
    "            \n",
    "            if self.remove_stopwords:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    if word.lower() not in set(stopwords.words('english')):\n",
    "                        clean_words.append(word)\n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lowercase:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    clean_words.append(word.lower())\n",
    "                \n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lemmatize:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    PoS_tag = pos_tag([word])[0][1]\n",
    "                    \n",
    "                    # to change contractions to full word form\n",
    "                    if word in contractions:\n",
    "                        word = contractions[word]\n",
    "\n",
    "                    if PoS_tag[0].upper() in 'JNVR':\n",
    "                        word = lemmatizer.lemmatize(word, convert_pos_wordnet(PoS_tag))\n",
    "                    else:\n",
    "                        word = lemmatizer.lemmatize(word)\n",
    "\n",
    "                    clean_words.append(word)\n",
    "                    \n",
    "                words = clean_words\n",
    "            \n",
    "            # detokenize trick taken from this StackOverflow post:\n",
    "            # https://stackoverflow.com/questions/21948019/python-untokenize-a-sentence\n",
    "            # and NLTK treebank documentation:\n",
    "            # https://www.nltk.org/_modules/nltk/tokenize/treebank.html\n",
    "            text = detokenizer.detokenize(words)\n",
    "            clean_corpus.append(text)\n",
    "            \n",
    "            [corpus_words.append(word) for word in words]\n",
    "            \n",
    "            if self.add_start_end_tokens:\n",
    "                words = ['<START>'] + words + ['<END>']\n",
    "            \n",
    "            doc_terms_lists.append(words)\n",
    "            \n",
    "        self.clean_corpus = clean_corpus\n",
    "        \n",
    "        self.doc_terms_lists = doc_terms_lists\n",
    "        \n",
    "        corpus_words = list(set(corpus_words))\n",
    "        \n",
    "        if self.add_start_end_tokens:\n",
    "            corpus_words = ['<START>'] + corpus_words + ['<END>']\n",
    "        \n",
    "        corpus_words = sorted(corpus_words)\n",
    "        \n",
    "        for el in corpus_words:\n",
    "            term_dict[el] = k\n",
    "            k += 1\n",
    "            \n",
    "        self.vocabulary = term_dict\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def transform(self, new_corpus = None, y = None):\n",
    "        \n",
    "        \"\"\" Compute the co-occurrence matrix for given corpus and window_size, using term dictionary\n",
    "            obtained with fit method.\n",
    "        \n",
    "            Returns: term-context co-occurrence matrix (shape: target terms by context terms) with\n",
    "            raw counts\n",
    "        \"\"\"\n",
    "        num_terms = len(self.vocabulary)\n",
    "        window = self.window_size\n",
    "        X = np.full((num_terms, num_terms), self.laplace_smoothing)\n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        if type(new_corpus) != list:\n",
    "            new_corpus = self.corpus\n",
    "        \n",
    "        for text in new_corpus:\n",
    "            text = re.sub(r'[_~`@$%^&*[\\]+=\\|}{\\\"\\'<>/]+', '', text)\n",
    "            \n",
    "            words = word_tokenize(text)\n",
    "            \n",
    "            if self.remove_stopwords:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    if word.lower() not in set(stopwords.words('english')):\n",
    "                        clean_words.append(word)\n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lowercase:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    clean_words.append(word.lower())\n",
    "                \n",
    "                words = clean_words\n",
    "                \n",
    "            if self.lemmatize:\n",
    "                clean_words = []\n",
    "                for word in words:\n",
    "                    PoS_tag = pos_tag([word])[0][1]\n",
    "                    \n",
    "                    # to change contractions to full word form\n",
    "                    if word in contractions:\n",
    "                        word = contractions[word]\n",
    "\n",
    "                    if PoS_tag[0].upper() in 'JNVR':\n",
    "                        word = lemmatizer.lemmatize(word, convert_pos_wordnet(PoS_tag))\n",
    "                    else:\n",
    "                        word = lemmatizer.lemmatize(word)\n",
    "\n",
    "                    clean_words.append(word)\n",
    "                    \n",
    "                words = clean_words\n",
    "                \n",
    "            if self.add_start_end_tokens:\n",
    "                words = ['<START>'] + words + ['<END>']\n",
    "            \n",
    "            for i in range(len(words)):\n",
    "                target = words[i]\n",
    "                \n",
    "                # check to see if target word is in the dictionary; if not, skip\n",
    "                if target in self.vocabulary:\n",
    "                    \n",
    "                    # grab index from dictionary\n",
    "                    target_dict_index = self.vocabulary[target]\n",
    "                    \n",
    "                    # find left-most and right-most window indices for each target word\n",
    "                    left_end_index = max(i - window, 0)\n",
    "                    right_end_index = min(i + window, len(words) - 1)\n",
    "                    \n",
    "                    # loop over all words within window\n",
    "                    # NOTE: this will include the target word; make sure to skip over it\n",
    "                    for j in range(left_end_index, right_end_index + 1):\n",
    "                        \n",
    "                        # skip \"context word\" where the \"context word\" index is equal to the\n",
    "                        # target word index\n",
    "                        if j != i:\n",
    "                            context_word = words[j]\n",
    "                            \n",
    "                            # check to see if context word is in the fitted dictionary; if\n",
    "                            # not, skip\n",
    "                            if context_word in self.vocabulary:\n",
    "                                X[target_dict_index, self.vocabulary[context_word]] += 1\n",
    "        \n",
    "        # if pmi = True, compute pmi matrix from word-context raw frequencies\n",
    "        # more concise code taken from this StackOverflow post:\n",
    "        # https://stackoverflow.com/questions/58701337/how-to-construct-ppmi-matrix-from-a-text-corpus\n",
    "        if self.pmi:\n",
    "            denom = X.sum()\n",
    "            col_sums = X.sum(axis = 0)\n",
    "            row_sums = X.sum(axis = 1)\n",
    "            \n",
    "            expected = np.outer(row_sums, col_sums)/denom\n",
    "            \n",
    "            X = X/expected\n",
    "            \n",
    "            for i in range(X.shape[0]):\n",
    "                for j in range(X.shape[1]):\n",
    "                \n",
    "                    if X[i,j] > 0:\n",
    "                        X[i,j] = np.log(X[i,j]) - np.log(self.spmi_k)\n",
    "                        \n",
    "                        if self.pmi_positive:\n",
    "                            X[i,j] = max(X[i,j] - np.log(self.sppmi_k), 0)\n",
    "        \n",
    "        # note that X is a dense matrix\n",
    "        self.X = X\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ContextMatrix(lowercase = True, lemmatize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [\n",
    "    \"Coronavirus is a fake liberal hoax.\",\n",
    "    \"Trump won't do anything about coronavirus.\",\n",
    "    \"The liberal fake news media always blame Pres Trump.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ContextMatrix at 0x1a2533bc90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.fit(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.</th>\n",
       "      <th>&lt;END&gt;</th>\n",
       "      <th>&lt;START&gt;</th>\n",
       "      <th>always</th>\n",
       "      <th>anything</th>\n",
       "      <th>blame</th>\n",
       "      <th>coronavirus</th>\n",
       "      <th>fake</th>\n",
       "      <th>hoax</th>\n",
       "      <th>liberal</th>\n",
       "      <th>medium</th>\n",
       "      <th>news</th>\n",
       "      <th>pres</th>\n",
       "      <th>trump</th>\n",
       "      <th>wont</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;END&gt;</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;START&gt;</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>always</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anything</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blame</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coronavirus</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fake</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hoax</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liberal</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medium</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pres</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wont</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             .  <END>  <START>  always  anything  blame  coronavirus  fake  \\\n",
       ".            0      3        0       1         1      1            2     1   \n",
       "<END>        3      0        0       0         1      1            1     1   \n",
       "<START>      0      0        0       0         1      0            2     2   \n",
       "always       1      0        0       0         0      1            0     1   \n",
       "anything     1      1        1       0         0      0            1     0   \n",
       "blame        1      1        0       1         0      0            0     1   \n",
       "coronavirus  2      1        2       0         1      0            0     1   \n",
       "fake         1      1        2       1         0      1            1     0   \n",
       "hoax         1      1        1       0         0      0            1     1   \n",
       "liberal      1      1        2       1         0      0            1     2   \n",
       "medium       0      0        1       1         0      1            0     1   \n",
       "news         0      0        1       1         0      1            0     1   \n",
       "pres         1      1        0       1         0      1            0     0   \n",
       "trump        2      1        1       1         1      1            1     0   \n",
       "wont         1      1        1       0         1      0            1     0   \n",
       "\n",
       "             hoax  liberal  medium  news  pres  trump  wont  \n",
       ".               1        1       0     0     1      2     1  \n",
       "<END>           1        1       0     0     1      1     1  \n",
       "<START>         1        2       1     1     0      1     1  \n",
       "always          0        1       1     1     1      1     0  \n",
       "anything        0        0       0     0     0      1     1  \n",
       "blame           0        0       1     1     1      1     0  \n",
       "coronavirus     1        1       0     0     0      1     1  \n",
       "fake            1        2       1     1     0      0     0  \n",
       "hoax            0        1       0     0     0      0     0  \n",
       "liberal         1        0       1     1     0      0     0  \n",
       "medium          0        1       0     1     1      1     0  \n",
       "news            0        1       1     0     1      0     0  \n",
       "pres            0        0       1     1     0      1     0  \n",
       "trump           0        0       1     0     1      0     1  \n",
       "wont            0        0       0     0     0      1     0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cm.transform(tweets), index = cm.vocabulary, columns = cm.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coronavirus fake liberal hoax.',\n",
       " 'trump wont anything coronavirus.',\n",
       " 'liberal fake news medium always blame pres trump.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.clean_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Coronavirus is a fake liberal hoax.',\n",
       " \"Trump won't do anything about coronavirus.\",\n",
       " 'The liberal fake news media always blame Pres Trump.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train embeddings using tweets as corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('COVID19_Dataset-text_labels_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Is_Unreliable</th>\n",
       "      <th>Category</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 3, 6, 9</td>\n",
       "      <td>We are living in scary times in Canada. Gov’t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 6, 8, 9</td>\n",
       "      <td>Just as bad in Canada. In fact, our government...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 4, 9</td>\n",
       "      <td>It was only a matter of time before the mainst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6, 8</td>\n",
       "      <td>Russia's taking no chances: Foreigners infecte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>6, 8, 9</td>\n",
       "      <td>Although there is now a presumptive confirmed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BREAKING: Harvard classes will move online sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Singularity University is hosting a FREE Virtu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coronavirus: how does it spread and what are t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stanford just cancelled classes for the rest o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tech conferences were cancelled in #Waterloo R...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Is_Unreliable    Category  \\\n",
       "0                1  1, 3, 6, 9   \n",
       "1                1  1, 6, 8, 9   \n",
       "2                1     1, 4, 9   \n",
       "3                1        6, 8   \n",
       "4                1     6, 8, 9   \n",
       "..             ...         ...   \n",
       "555              0         NaN   \n",
       "556              0         NaN   \n",
       "557              0         NaN   \n",
       "558              0         NaN   \n",
       "559              0         NaN   \n",
       "\n",
       "                                                 Tweet  \n",
       "0    We are living in scary times in Canada. Gov’t ...  \n",
       "1    Just as bad in Canada. In fact, our government...  \n",
       "2    It was only a matter of time before the mainst...  \n",
       "3    Russia's taking no chances: Foreigners infecte...  \n",
       "4    Although there is now a presumptive confirmed ...  \n",
       "..                                                 ...  \n",
       "555  BREAKING: Harvard classes will move online sta...  \n",
       "556  Singularity University is hosting a FREE Virtu...  \n",
       "557  Coronavirus: how does it spread and what are t...  \n",
       "558  Stanford just cancelled classes for the rest o...  \n",
       "559  Tech conferences were cancelled in #Waterloo R...  \n",
       "\n",
       "[560 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ContextMatrix(window_size = 15, lowercase = True, lemmatize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ContextMatrix at 0x1a2cec4f10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.fit(tweets['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_context_matrix = cm.transform(tweets['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>#</th>\n",
       "      <th>(</th>\n",
       "      <th>)</th>\n",
       "      <th>,</th>\n",
       "      <th>-</th>\n",
       "      <th>--</th>\n",
       "      <th>.</th>\n",
       "      <th>...</th>\n",
       "      <th>1</th>\n",
       "      <th>...</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoomer</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>—</th>\n",
       "      <th>‘</th>\n",
       "      <th>’</th>\n",
       "      <th>“</th>\n",
       "      <th>”</th>\n",
       "      <th>❝real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>124</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <td>30</td>\n",
       "      <td>650</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>113</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>215</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>63</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>)</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>10</td>\n",
       "      <td>113</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>138</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>157</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‘</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>’</th>\n",
       "      <td>29</td>\n",
       "      <td>63</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>60</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>“</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>”</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>❝real</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2325 rows × 2325 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         !    #   (   )    ,   -  --    .  ...  1  ...  zombie  zone  zoomer  \\\n",
       "!      124   30   0   1   10   3   0   37    7  3  ...       0     0       0   \n",
       "#       30  650   9   9  113  19   3  215    1  1  ...       0     1       1   \n",
       "(        0    9   2  32   13   1   0   17    1  0  ...       0     0       0   \n",
       ")        1    9  32   2   13   1   0   20    1  0  ...       0     0       0   \n",
       ",       10  113  13  13  138  12   4  157    3  4  ...       3     1       0   \n",
       "...    ...  ...  ..  ..  ...  ..  ..  ...  ... ..  ...     ...   ...     ...   \n",
       "‘        0    4   0   0    3   0   0    4    0  0  ...       0     0       0   \n",
       "’       29   63   4   4   45   6   0   86    5  0  ...       0     0       0   \n",
       "“        8    5   1   1   15   3   1   17    1  1  ...       0     0       0   \n",
       "”        2    4   0   0    8   2   1   13    1  0  ...       0     0       0   \n",
       "❝real    1    0   0   0    0   0   0    0    0  0  ...       0     0       0   \n",
       "\n",
       "       zuckerberg  —  ‘   ’   “   ”  ❝real  \n",
       "!               0  1  0  29   8   2      1  \n",
       "#               0  2  4  63   5   4      0  \n",
       "(               0  0  0   4   1   0      0  \n",
       ")               0  0  0   4   1   0      0  \n",
       ",               0  2  3  45  15   8      0  \n",
       "...           ... .. ..  ..  ..  ..    ...  \n",
       "‘               0  0  0   9   0   0      0  \n",
       "’               0  0  9  60  13  12      1  \n",
       "“               0  1  0  13   4  20      0  \n",
       "”               0  1  0  12  20   8      0  \n",
       "❝real           0  0  0   1   0   0      0  \n",
       "\n",
       "[2325 rows x 2325 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(word_context_matrix, index = cm.vocabulary, columns = cm.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2325, 2325)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_context_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica = FastICA(n_components = 2)\n",
    "std_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.36532191e-04, -5.53937042e-02],\n",
       "       [-9.79732719e-01, -5.76462756e-03],\n",
       "       [ 9.95407211e-03, -2.65669273e-02],\n",
       "       ...,\n",
       "       [ 1.45737163e-02, -2.75628408e-02],\n",
       "       [ 1.13305718e-02, -1.98008874e-02],\n",
       "       [-4.10797041e-05,  2.52191035e-03]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_std = std_scaler.fit_transform(word_context_matrix)\n",
    "\n",
    "matrix = ica.fit_transform(X_std)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comp 1</th>\n",
       "      <th>Comp 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>-0.000637</td>\n",
       "      <td>-0.055394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <td>-0.979733</td>\n",
       "      <td>-0.005765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(</th>\n",
       "      <td>0.009954</td>\n",
       "      <td>-0.026567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>)</th>\n",
       "      <td>0.010827</td>\n",
       "      <td>-0.027660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>0.063515</td>\n",
       "      <td>-0.276027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‘</th>\n",
       "      <td>0.002399</td>\n",
       "      <td>-0.004367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>’</th>\n",
       "      <td>-0.003171</td>\n",
       "      <td>-0.112212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>“</th>\n",
       "      <td>0.014574</td>\n",
       "      <td>-0.027563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>”</th>\n",
       "      <td>0.011331</td>\n",
       "      <td>-0.019801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>❝real</th>\n",
       "      <td>-0.000041</td>\n",
       "      <td>0.002522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2325 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Comp 1    Comp 2\n",
       "!     -0.000637 -0.055394\n",
       "#     -0.979733 -0.005765\n",
       "(      0.009954 -0.026567\n",
       ")      0.010827 -0.027660\n",
       ",      0.063515 -0.276027\n",
       "...         ...       ...\n",
       "‘      0.002399 -0.004367\n",
       "’     -0.003171 -0.112212\n",
       "“      0.014574 -0.027563\n",
       "”      0.011331 -0.019801\n",
       "❝real -0.000041  0.002522\n",
       "\n",
       "[2325 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(matrix,\n",
    "                  index = cm.vocabulary,\n",
    "                  columns = ['Comp {}'.format(i+1) for i in range(2)])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caitlinmoroney/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 62222 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/caitlinmoroney/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 62222 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/caitlinmoroney/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 12540 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/caitlinmoroney/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 12540 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeYAAAD4CAYAAAA5OEWQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXhV1b3/8fc3c0IgIUwyhQAyicRQUgQUSg0OFQraitQBsVxLHVF7tWpplUul0uK9FKxt5YdX4EJRoSrSOoLiUFEJGiMgCiggZSYQxpDp+/vjnMQAQQInJIfweT3Pec7e+6yz91pH3J+stSdzd0RERCQ8RNR2BUREROQbCmYREZEwomAWEREJIwpmERGRMKJgFhERCSNRtV2BY2ncuLGnpaXVdjVERE4ry5Yt2+HuTWq7HnLywjaY09LSyM7Oru1qiIicVsxsfW3XQUKjoWwREZEwomAWEREJIwpmERGRMKJgFhERCSMKZhGRE7B7927+/Oc/A7B48WIGDRpEWloaO3bsIDExsdLv/PWvf2XmzJkATJ8+nY8++oirrroKgJycHF566aVjbm/s2LG0b9+e7OxsHnzwQXr27Mnu3buruVUSTixcH2KRmZnpOitbRMLNunXrGDRoEAkJCSxdurS2q1MbHCgFIo9YXkqgs+eAHWcdJcHv7wYOBr8XDcQAicBSIAt4B0gCCoAmwCx3v6viisxsOnAtsMrd04+1QTNbB2S6+w4zSwDmAu2BWOBddx8eLBcLzAR6ADuBYe6+zswuBiYE61gI3OvubwS/0wOYDsQDLwF3egjhqh6ziEgVzZw5kw4dOrBixYozNZQhELoVQ7n0iM+LvuW7B4Lvf66wLAqYBDwPbAoua+Tue909A3gbeAhYDzx3jPWuBf5dXkGzqlwK/Ki7dwZmAReY2Q+Cy/8D2OXuZwfr9fvg8h3AD929GzAC+L8K6/oLMAroEHxdFqzHkX+8VIl6zCIiwIMPPki/fv0YMGDAYcsXL17Mo48+yu9//3u+//3vs3379lqqoVTCge1A00qWHyDQgy3rnRvwAYEecjQwlEBP/H0CvfQ84Drg6WC5hkAbIIXAHwS9gQ/c/UYzMwI9/RXB7zZy98YAZrYdWAccAv5EoPf/OIEe/wHgZ+6+6tsaFbY3GBERqUnjxo075mcFO7cx997JJBTFEGFGaZh2aM4QFYfKHUgOTpcGlzuwH9gAdAb2AvcRCMcOwE1AC+Aed78pOBT+M6C/u38ZHOaOBC4CBhPoyT8V/N5SM8sAzgbed/f+ZtYTeNXM0t09FygGEt39uwBmtgi42d1Xm9n5BEYLLvq2Bp4RQ9kPPPAAixcv5oUXXmDChAm1XR0ROQVmzpxJeno65513HsOHD2f9+vVkZWWRnp5OVlYWGzZsID8/n7S0NEpLA6OvBw4coHXr1hQVFXHjjTcyb948AF555RU6d+7MhRdeyNTfTcK3RfJZvRVsjtqGo1CuZRWPX0dUmI+o8PkuAuEZSSC4HyPQS14L9AOWAWnBIe8fEzjG/GWF9S4MHiP+lEDQrnT3UgI95L4EhrcXmtlHwGygHnBOhe//G8DMEoE+wFwzywGeAJofr4HVEsxmdpmZfW5ma8zs/ko+jzWzZ4Kff2BmadWx3ar64IMPOP/883nrrbfo27dvTW5aRGrAihUrGD9+PG+88QaffPIJkydP5vbbb+eGG24gNzeX6667jtGjR5OUlMR5553HW2+9BcCCBQu49NJLiY6OLl9XQUEBP/vZz1iwYAFPPjqXXRv2sD1+E/U7rqPBgSLUWa5Rlf3ae4+Y31GhbFn59QROLCshMHw8jEAP+m/Bz0sIjBhPJTCE/U6F9R0A6genjUBO5gXn44AHCPTAbyRwglo/AsPaccEyUXxzvDsC2O3uGRVeXY7X6JCDOXhw+3HgBwT+YrjGzM45otixDqafUvfeey/p6eksXbqU3r17M23aNG655ZZvHbISkdPDFx9sYcav/sXjN7/BI3dP46Lel9G4cWMAUlJSWLJkCddeey0Aw4cP59133wVg2LBhPPPMMwA8/fTTDBs27LD1rlq1irZt29KhQwfef/FL2l14GQW2ncdi9nPfBTE12ELh8CHrMhUP8pcCDYLThyp8p+whHhEETkb7kMDZ1NcBbwU/a0vgjO+5fBPEAF8D/YPTlwP73d3NLBkYQKCHvJrAcHl+sA4JQPvgsed6wMsA7r4H+MrMhgJYwHnHa3R19Jh7Amvc/Ut3LyRw4HzIEWWGADOC0/OArGADTqmJEycybdo0brzxRpYuXUp6ejq5ubk8+OCDp3rTInIKffHBFt6cvYp9eYF98aH9RaxfkccXH2w55nfKdjmDBw/m5ZdfJi8vj2XLlnHRRUcf7isruyTReTY1ltTCYnY3jubH58RwVuRJnWgroamYF+0qTEfwTSDH8c0x5s4EQtcJhO4mAkPIW919PoGTxVIJdCb/A/ilmW0ws74EQjfRzNYEPyv7R3V7cF1XEciz1sAq4H8J9Lh/CqwhMPS9sEIdrwP+w8w+ITAUfmQ+HqU6Tv5qSeAvjDIbgfOPVcbdi80sH2jEN0MQ1Sf3WVg0DvI3QlIrPs7rTUZGL1atWsU55xzZkReR09GS+WspLvzmKp1OLb/D1Nce5PU5H9Hx/MvJy8ujT58+PP300wwfPpzZs2dz4YUXApCYmEjPnj258847GTRoEJFHBG3nzp356quvWLt2LYszEtj76CsQDas61uc3j+ZTGkFgIFRCUXbGNATCMw+4i8CJUU4gZDcRODt6OdDU3e+sbEVmluju+8ysEYGe8cXBk7jK5i9w98P+YnP3lzj2tdbvVLbQ3R8GHq56E8u/9xXBy6eqqjqCubLGHXlcoCplMLNRBK4FIzU19cRrkvssLBgNRQfJ2VLCjX/9jI17V9K46UscKInE3cnIyGDJkiXEx8cff30iEpbKesplmqekcWn36xg/8zamvtmA7t27M2XKFEaOHMnEiRNp0qQJTz31VHn5YcOGMXToUBYvXnzUuuPi4pg6dSoDBw7ky+h4OqalcyAhitJIo8OAK3hv2txT3bxwVfFs6EICQ8NlVhLoQdavUHYr0IxAZ60Bgbwpu3RpXfC9wN27V1jPbAAzG0bgWG634Hdv/JZ6/SM4zBwD/Bb434rzR4by6SDk65jNrDcw1t0vDc4/AODuj1Qo82qwzJLgWXBbgCbfdmeUk7qOedK5kP/1YYv6PLmfd+/qxMhPv8svf/lL9ZpF6oAZv/rXUeEMkJgSy4jfXVBt2+nyzqfMXJRPft8RmMF/vDaZ+B0fs+rJM+ZwWBGBAC077HmQwHDtOOAfwBfAMwQuJYojcNZxDHAzgR7vJwTOK/p18PtzgB8C7wJjgdvc/ckaaMdppTqOMS8FOphZWzOLAX4CvHhEmRcJ3CkFAuPzb4Ryu7Jjyt942Oz2/aU0jDci9v5bQ9kidUjvIe2Jijl89xUVE0HvIe2rbRt/35LHvuISmhU4eQVJADSK28Xe6BbVto1aVMo3d+gq4Jszmp1Ar7aYQG82hsBZx58BOQR6sB8BuHsBgeOqAwj0iucA97n7rQTOgM5w94EEzo4us8zdB7n7BHePUyhXrlru/GVmlwN/JPCX1f+6+3gzGwdku/uLZhZH4PZl3QkcS/jJEdeMHaW6eswAJLWGu5ef2LpEJKx98cEWlsxfy768QySmxNJ7SHs6nn9Wta0/870VrFv9BS1+/SAbSrbRuvFe9h6qR0HP+9g6695q284pULZTf4/APjcuuKzsml8ncJnQBmA8gRBe7O4Xm9ly4DcE7lj1NoEgftndw7rBdU3duiVnhWPM5aLj4YdTIP3q6q2giNRpzd/MoWjDV/TLg41bY9i79BG+WvIhja+dxL+fvD1QKDoGigprt6IEziKPjo6msLAQvjkWfB2BS4F2AofcvYmZLQDOBbKCJ0jtc/fEI9Z1D5Dk7r+p2VZImbp156/0qwMhnNQasMC7QllETkLL2GiiUtvyXkZbmjTbz7+3xECjjkQ1TsOi44io17DGQzkhIaF8Oj4+nh49egDg7hQVFREXFweBY7tzCTycYReBoelSM+tD4H7Pvz/WiKWZPQ/cAEw+hc2Q46hbwQyBEL57OYzdHXhXKIvISXigXXPiIwInIb9XtIPIzWto+qNgJ9KdyPj61O/xw6O+V5UbNBx5iVZSUuAYdtOmTUlMTCy/jnrp0qUkJibSsmVLJk+ezOeff06jRo1o3bo1HTt25MCBA0RHR7Ny5Up27dpFixYtIHBzjY4EjhEnAlcC+wgMX6e7+1/Ltntkb9ndr3T3dHev/ktZpcrqXjCLiITgn1/+k0vmXcJ/vdqfllv+k2YHl1Aw8VfcPfgeIuMS2bd8EV5cSNGuzRTt2kKiGTFmGIFQTgmGbhQQERFJ06ZNiY2NBSAmJobIyEjq169Px44due2224iKiqKgoACAjIwMzj33XBo0aFAezj/+8Y+5++67OXjwIL169aJZs2a8/vrr5OTksHLlSgoLC+nSpQuTJk3iyiuvhMD9nTM5/HKmzQRO8qp4aZKEKQWziEjQP7/8J2PfG8vm/ZtxnPyCrbDmrzRLPMS531tC283r2fPeM1hUNFFJzSjc9iX73ClyJ4LAwd2dJYG7jxQDjrNv377yHjFA69atSUhIoGHDhtSrV4+hQ4eWDUGzcOFCPvvsM/bs2UNiYiJz585l/vz5PProo/Tp04fMzEy6dOnCY489Rtn5QR9//DEA+fn5NG9e/nyE4Rz+zOTdwEDgd2bW/5T9gFItFMwiIkGTP5pMQUnBYcuK44r5+ahkktp8SLuSJ+mSmkqHEY9SvGsTba+4h8xWbegaH8egBg2IrvC9qGjw0lJ6Z/YlPz+fqKgoUlJS2LlzJ9u2bSM6OpqnnnqK+fPnU1QUuHLJ3YmLi8Pdadu2LcXFxZx11lkUFxdz+eWXEx8fz7Rp0ygqKiI9PZ1zzz2X3/wmMLx+6623MmPGDAjcjrIjgXs5l3P3rQSuIX48+PhBCVMKZhGRoC37j75JVMmBEua/FHigUVzD9Zzd4iuGbv87jevFcUvhuxBVQkFaDI3rB57VbEBUtFEaGQEGq1Z+QWZmJomJiezatYtDhwI3Rrn55psZPHgwBQUFtGvXjkaNGtG9e3cmTJjAlClTWL16NW+++SafffYZy5cvp1mzZsyZM4fk5GSeeOIJPv30U5YvX84//vEPAMaPH1/2HIBV7v5A2fFjd1/s7oOC0xvcvau7f1ADP6ecJAWziEjQWfWOvg46umE0V9zUDyuJoft34vkgN48G524mMsrYf+gQRZGlbI11pu/eRWEUuEFpjBHbJpbYJnE0iGtMdnY2ZkZOTg4vvfQSpaWltGjRgiZNmlBaWsof//hHsrKy+OSTT9i6dSt33HEHPXv2ZOrUqUDgZLHdu3dz++231/RPIrWgbl3HLCISgrJjzEcOZ09f/VviGn/Ojg5/56W31zF71h62byuhR7durI5dx6FDxezN2UtMixgKNxVi0YZFGF4ISQkN2Xtwd/kx4czMTLKzs4mPj6ekpISCggLi4uJISkpi69atpKWlkZyczL59+4iKiuLll1/mJz/5CdnZ2TRs2JD58+fTp08f3J077riDN954g7Zt2+LujBw5kqFDhy5z98xa+PmkmqjHLCISNLDdQMb2GUvzes0xjAgL7CKbFKeQtKUP7d/5b+6wvzN38ExaJJ/FlYOGctsld5C8rQERERGU7nEs2vBih0LDS509B3ZRWlqKu+PubNq0iZYtW9KuXTuKi4uJjIwkNjaWrl27YmZs2bKFLl26sGbNGrZt28aGDRuYN28eycnJXHrppYwePZrt27fTu3dvZs6cSUJCAqNGjeK9996r5V9Pqkt1PF1KRKTOGNhuIAPbDQS+6UFvj8qjWXGj8jKHGsLIW3/GfjsEGIlpDbD8LRTvLaJ+QkPOapTK9v1b2Vu4h9LiQpIbNeJnI0YQExPDxIkTGTNmDA899BBRUVE0adKE/fv388EHH5CUlMT+/ft59dVXiYyM5Le//S0///nPyczMJC4ujldeeYUDBw5w5513kpqayqhRoxgwYACXXnpppc+VltOTeswiIsdQ1oN+ofVbFNg3T7PKjvqSEisldd16Gq16ge2xWwGITomm5R+asTV1A3tsHyUlxXhxMXuiYnly1t+YOHEiRUVFTJkyBXenY8eOjBo1itTUVPbv38/UqVOJiorirrvu4kc/+hG33normzZtIjk5mXHjxnH11VdTWFjIwoULefPNNxk7diyDBw9mz549FBcX19bPJNVMwSwi8i0GthvIb38+ieZXpxOZHLhRyL6IAlLXree7S5fyXJ9iSoNXDLs7ETERJKTFEtsiEqtXD8wo3b+P/TExdO/enSZNmmBmRERE0K5dO+bNm1d2xy4WL15McnIykydPpl+/fgCUlpbSrFkzIiIi+PzzzykpKaG0tJQpU6bQpUsXli1bRnZ2Nu+8806t/D5S/TSULSJSBfW6N6Ve96YAJE3KJj03l6iSEnY2iCQyIZKkC5LIezWPNb9eQ0RsBEW7iogbcBUHX5wLJcUUbN5E7rYt/O53v2P79u088sgjrFixgn79+vHVV19x++23U1hYWH5J1csvv0xGRgbNmzfn2WefZf/+/SQnJ1OvXj0uueQSvv76azp06EC3bt1o1qwZ3/ve92r5F5Lqoh6ziMgJysrKIuHAAQAa7YFmVzYj6TtJWJRR75x6uDsJnRtSf9SdWHJDIlu1Ib51KmlpaSQkJPDwww+zbNkymjRpwrJly8qHqp944glatGhB79692bVrFzfffDOzZs0iNzeX3/zmN/Tu3Zt9+/YxZcoUli1bxttvvw1Ap06deOGFF7jqqqtq82eRaqIes4jICfqypBFNEhqQcmAPaTPzeH9LHoU7i/BiZ9/yfRTvLaFwx172XzcI35NPk0f+RMnD9xFdryGPP/44Y8eO5YYbbuD999+nffv2LFmyhFatWlFcXExERATNmzdn27ZtDBs2jJ49e7Ju3TomTJjA+eefT3p6Oh07diw/I1vqHvWYRURO0MRXP+f/dRnIiqISFufu5PKGSbTsm4JFGb1/24PrJ4/BiCDhx9fSdOj1/KZ7Zw7u3sXjjz9OTk4Ot956K9HR0bz22mscPHiQESNGsHfvXs4++2yKi4v59a9/zfLly5k+fTo7d+4E4PPPP2fUqFHk5ubSoEED/vznP9fyryCnioJZROQExe75msbto3iucQp9uyRx+4OJPDIintYtYnio009Y+f9eIj4yghZvv0b3XVv4QZNkEhISuPrqq7nttts4ePAgERERvPbaa+Tl5TF79mySk5PZtWsXHTp04LLLLiM2NpZ27doxceLE8uPHnTp1AuD666/nzTff5MorryQ9PZ2ePXuyfPny2vxJpBopmEVETkBubi4XxKwnMaKQotQSirsWU9oIMIiIKOUXd/+KAQM6079/f5544gkKCgrKH2DxyiuvkJqaypQpU9ixYwfuTmpqKg8++CD9+vXjb3/7G+3atSvfVkREBN26dWP27NlHPcP5iy++ICMjg9zcXGbOnMmdd95Zw7+EnCo6xiwiUkUzZ87k/vvvL7+E6aqhexj/8G7+9d4+khpEsGdPKVgRf/nL01x44aXMmDGDkpISevXqxf79+xk/fjxDhgwhKiqKTz/9lC+//JJNmzbx2muvAbB9+/byh1yU6dSpE40bN6akpISlS5fygx/8gDlz5hAVFUVWVhYAnTt3Zt26dWzdurXGfxOpfuoxi4hUwYoVKxg/fjzXX389N998M5dddhkv/XMbWVmJRBhs3lzM/v2lHNhfyt69Jbzzzjs0btyYnTt3ctlll9GtWzfmzp3LyJEjadCgAR9//HH5fa5fffVV3n77bR566CFKgs9zPlJ0dDTPPPMM6enp5OXlMWjQIJ577jkAPvzwQ9avX8/GjRtr8ieRU0Q9ZhGRb5P7LCwaxxuvreWq1EQuaLSdDw+2IT4+npUrD/Hs3FSiooziYufqoev525w2vP1WFFu3fp+JEyeyZs0ahg0bxsUXX8yNN97IoEGDOPvssxk9ejQdOnRgzZo1vPjii0ydOrX8EY5lFi9eDMC6desAePTRR2ncuDEAe/bs4c477yQjI4Nu3brRvXt3oqK0S68L9F9RRORYcp+FBaOh6CCOY4f2cFnhPzkUcTGflHagpCSKkpJIoqJKATCDiIh4ht/wa7Iuuo+8vDyWLVtW6X2szazSTV566aVs3bqVzMxMpk2bdsyqNWjQgKeeegoI3HGsbdu2tG3bthoaLbVNQ9kiIseyaBwUHQQgq20Uz64oZtfegwyMXUpUVBQtW7bh+edaExHRhEUL95GenkznzuM5u/1P6NmzJ3feeSeDBg066sStzp0789VXX7F27VoA5syZU/7Zq6++Sk5OzmGhnJaWVn7bzjK7d++msLAQgGnTptGvXz8aNGhwSn4GqVkKZhGRY8n/5pht16aRjOkbw/emH+C7k9ayZs0aXn31VdauhTtHH2DZsm7MmvUxzc8aAsCwYcOYNWsWw4YNO2q1cXFxTJ06lYEDB3LhhRfSpk2bY1ZhypQptGrVio0bN5Kens5NN90EwGeffUbXrl3p3LkzL7/8MpMnT67mxkttsbKHd4ebzMxMz87Oru1qiMiZbNK5kP/10cuTWsPd4XndsJktc/fM2q6HnDz1mEVEjiXrQYiOP3xZdHxgucgpomAWETmW9Kvhh1MCPWQs8P7DKYHlIqeIzsoWEfk26VcriKVGqccsIiISRkIKZjNLMbPXzWx18L3hMcq9Yma7zewflX0uIiIiAaH2mO8HFrl7B2BRcL4yE4HhIW5LRCQs9e3blyVLltR2NaSOCDWYhwAzgtMzgCsqK+Tui4C9IW5LRCTsFBUVAdCrV69aronUFaEGczN33wwQfG8aepVERE4vzz///DFvsSlyoo57VraZLQTOquSjMdVdGTMbBYwCSE1Nre7Vi4hUu+jo6PIHS4hUh+MGs7sPONZnZrbVzJq7+2Yzaw5sC6Uy7j4VmAqBO3+Fsi4RkVMm+MQp8jdCUqvADUd0SZVUk1CHsl8ERgSnRwDzQ1yfiEh4Cz5x6vI/f86mvSWBW3YuGB1YLlINQg3mCcDFZrYauDg4j5llmln5o1HM7B1gLpBlZhvN7NIQtysiUuM2b5lPwUu3QtFBXrougRb1g7vQooOBHrRINQjpzl/uvhPIqmR5NnBThfm+oWxHRKS2bd4yn1WrxtC/oKjyAhWeRCUSCt35S0SkCr5c+yilpQcpiD3GbjOpVc1WSOosBbOISBUUHNoMwNq0BEoi4PLZB9i0tzTwoZ44JdVID7EQEamCuNjmFBzaxNZmcQD8fWQEcYdKKYiLJu5yPXFKqo+CWUSkCtq1v4dVq8ZQWnqQrc3i2NosjoiIeDp3Hk/zs4bUdvWkDlEwi4hUQVn4frn2UQoObSYutjnt2t+jUJZqp2AWEami5mcNURDLKaeTv0RERMKIgllERCSMKJhFRETCiIJZREQkjCiYRUREwoiCWUREJIwomEVERMKIgllERCSMKJhFRETCiIJZREQkjCiYRUREwoiCWUREJIwomEVERMKIgllERCSMKJhFRETCiIJZREQkjCiYRUREwoiCWUREJIwomEVERMKIgllERCSMKJhFRETCiIJZREQkjCiYRUREwkhIwWxmKWb2upmtDr43rKRMhpktMbMVZpZrZsNC2aaIiEhdFmqP+X5gkbt3ABYF5490ALjB3bsClwF/NLPkELcrIiJSJ4UazEOAGcHpGcAVRxZw9y/cfXVwehOwDWgS4nZFRETqpFCDuZm7bwYIvjf9tsJm1hOIAdaGuF0REZE6Kep4BcxsIXBWJR+NOZENmVlz4P+AEe5eeowyo4BRAKmpqSeyehERkTrhuMHs7gOO9ZmZbTWz5u6+ORi8245RrgHwT+DX7v7+t2xrKjAVIDMz049XNxERkbom1KHsF4ERwekRwPwjC5hZDPA8MNPd54a4PRERkTot1GCeAFxsZquBi4PzmFmmmU0Llrka6AfcaGY5wVdGiNsVERGpk8w9PEeMMzMzPTs7u7arISJyWjGzZe6eWdv1kJOnO3+JiIiEEQWziIhIGFEwi4iIhBEFs4iISBhRMIuIiIQRBbOIiEgYUTCLiIiEEQWziIhIGFEwi4iIhBEFs4iISBhRMIuIiIQRBbOIiEgYUTCLiIiEEQWziIhIGFEwi4iIhBEFs4iISBhRMIuIiIQRBbOIiEgYUTCLiIiEEQWziIhIGFEwi4iIhJGo2q6AiMiZ7IsPtrBk/lr25R0iMSWW3kPa0/H8s2q7WlKL1GMWEaklX3ywhTdnr2Jf3iEA9uUdYvBVl/POgo9ruWZSmxTMIiK1ZMn8tRQXlpbPl3op23b/m1Vv76rFWkltUzCLiNSSsp5ymS271pPRti+Fe2upQhIWFMwiIrUkMSX2sPkWKW35cZ9bj1ouZxYFs4hILek9pD1RMYfvhqNiIug9pH0t1UjCgc7KFhGpJWVnX+usbKlIwSwiUos6nn+WglgOo6FsERGRMBJSMJtZipm9bmarg+8NKynTxsyWmVmOma0ws5tD2aaIiEhdFmqP+X5gkbt3ABYF54+0Gejj7hnA+cD9ZtYixO2KiIjUSaEG8xBgRnB6BnDFkQXcvdDdyy7Wi62GbYqIiNRZoYZkM3ffDBB8b1pZITNrbWa5wNfA79190zHKjTKzbDPL3r59e4hVExEROf0c96xsM1sIVHbK4JiqbsTdvwbSg0PYL5jZPHffWkm5qcBUgMzMTK/q+kVEROqK4wazuw841mdmttXMmrv7ZjNrDmw7zro2mdkKoC8w74RrKyIiUseFOpT9IjAiOD0CmH9kATNrZWbxwemGwAXA5yFuV0REpE4KNZgnABeb2Wrg4uA8ZpZpZtOCZboAH5jZJ8BbwKPu/mmI2xUREamTQrrzl7vvBLIqWZ4N3BScfh1ID2U7IiIiZwpduiQiIhJGFMwiIiJhRMEsIiISRhTMIiIiYUTBLCIiEkYUzCIiImFEwSwiIhJGFMwiIiJhRMEsIiISRhTMIiIiYUTBLCNDOm0AABD/SURBVCIiEkYUzCIiImFEwSwiIhJGQnq6lIjImS43N5dFixaRn59PUlISWVlZpKfrgXpy8hTMIiInKTc3lwULFlBUVARAfn4+CxYsAFA4y0nTULaIyElatGhReSiXKSoqYtGiRbVUI6kLFMwiIicpPz//sPnZs2ezd+/eo5aLnAgFs4jISUpKSjps/rrrrqN+/fpHLRc5EQpmEZGTlJWVRXR09GHLoqOjycrKqqUaSV2gk79ERE5S2QleOitbqpOCWUQkBOnp6QpiqVYayhYREQkjCmYREZEwomAWEREJIwpmERGRMKJgFhERCSMKZhERkTCiYBYREQkjCmYREZEwElIwm1mKmb1uZquD7w2/pWwDM/u3mf0plG2KiIjUZaH2mO8HFrl7B2BRcP5Yfgu8FeL2RERE6rRQg3kIMCM4PQO4orJCZtYDaAa8FuL2RERE6rRQg7mZu28GCL43PbKAmUUA/w3ce7yVmdkoM8s2s+zt27eHWDUREZHTz3EfYmFmC4GzKvloTBW3cSvwkrt/bWbfWtDdpwJTATIzM72K6xcREakzjhvM7j7gWJ+Z2VYza+7um82sObCtkmK9gb5mdiuQCMSY2T53/7bj0SIiImekUB/7+CIwApgQfJ9/ZAF3v65s2sxuBDIVyiIiIpUL9RjzBOBiM1sNXBycx8wyzWxaqJUTERE505h7eB7KzczM9Ozs7NquhojIacXMlrl7Zm3XQ06e7vwlIiISRhTMIiIiYUTBLCIiEkYUzCIiImFEwSwiIhJGFMwiIiJhRMEsIiISRhTMIiIiYUTBLCIiEkYUzCIiImFEwSwiIhJGFMwiIiJhRMEsIlJHmdl7tV0HOXEKZhGRGlZcXFwj23H3PkcuM7PIGtm4nDQFs4hICGbOnEl6ejrnnXcew4cPZ/369WRlZZGenk5WVhYbNmwA4MYbb+QXv/gF3//+97nvvvvIy8vjiiuuID09nV69epGbmwvA2LFjGTlyJP3796ddu3ZMmTKlfFtXXHEFPXr0oGvXrkydOhWAv/zlL/zyl7+sWKVGZvYYgJntC773N7M3zexvwKdmlmZmy8u+YGb3mNnY4PRoM1tpZrlm9vSp++XkmNw9LF89evRwEZFwtnz5cu/YsaNv377d3d137tzpgwYN8unTp7u7+5NPPulDhgxxd/cRI0b4wIEDvbi42N3db7/9dh87dqy7uy9atMjPO+88d3d/6KGHvHfv3l5QUODbt2/3lJQULywsLF+/u/uBAwe8a9euvmPHDt+2bZu3b9++vE5APnBhYJJ9wff+wH6gbXA+DVjuwf0tcA8wNji9CYgNTif7Kd7X63X0K6pG/woQETnN5ebmsmjRIvLz88nNzaVfv340btwYgJSUFJYsWcJzzz0HwPDhww/rzQ4dOpTIyMBI8rvvvsvf//53AC666CJ27txJfn4+AAMHDiQ2NpbY2FiaNm3K1q1badWqFVOmTOH5558H4Ouvv2b16tX06tWLdu3a8f7779OhQweAOOBflVT9Q3f/qipNBGab2QvACyf+C0moNJQtIlJFubm5LFiwoDxADx48yOrVq8uHoStjZuXT9erVK58O9kgrLRsbG1u+LDIykuLiYhYvXszChQtZsmQJn3zyCd27d6egoACAYcOG8eyzz5YF/S6vbOWBHnOZYg7f/8dVmB4IPA70AJaZmTpwNUzBLCJSRYsWLaKoqKh8vm3btnz66ae8+OKLAOTl5dGnTx+efjpwaHb27NlceOGFla6rX79+zJ49G4DFixfTuHFjGjRocMxt5+fn07BhQxISEli1ahXvv/9++Wc/+tGPeOGFF5gzZw5AXhWashVoamaNzCwWGARgZhFAa3d/E/glkAwkVmF9Uo30l5CISBWV9ZTLNG3alL59+zJlyhTmzp1L9+7dmTJlCiNHjmTixIk0adKEp556qtJ1jR07lp/+9Kekp6eTkJDAjBkzvnXbl112GX/9619JT0+nU6dO9OrVq/yzhg0bcs4557By5UqAA8drh7sXmdk44APgK2BV8KNIYJaZJQEGTHL33cdbn1Qvq3zEo/ZlZmZ6dnZ2bVdDRKTcpEmTjgpngKSkJO6+++5aqNHRzGyZu2fWdj3k5GkoW0SkirKysoiOjj5sWXR0NFlZWbVUI6mLNJQtIlJF6enpAOVnZSclJZVfsyxSXRTMIiInID09vUpB3L9/fzZv3kx8fDwAZ599NvPmzWPs2LH84Q9/YN26dTRt2hSAxMRE9u3bBwTOwu7WrRtFRUVERUUxYsQI7rrrLiIiNMB5ptB/aRGRalJYWMi8efPo3r072dnZFBUVccsttzB06FDWrFlDRkYG48aNo7i4mG7duh12V6/zzjuPa665hvj4eHJyclixYgVdunRhzJgxtGjRgvPOO49//OMfFBYWcuWVV5KRkcHZZ59NUlISGRkZZGRk8N57ujV2XaBgFhEJ0SeffMLo0aPp2LEjt9xyCwsWLCAzM5NZs2bRv39/xowZQ05ODjk5OURFRfGrX/2K+Ph4rr/+egBKS0spLS3l7bffPuz65ri4OB599FGKioqYNGkSt956K506daJdu3bMmTOHadOm0bdv3/J19+nTBwJnVstpTMEsInIS9u/fz/jx42nZsiXnn38+ycnJvPXWW5gZjRo1AuCnP/0pw4YNIyMjg3vvvbf8u4mJiYwcOZLJkycDgYdaDB8+nEsuuYSSkpLDttOsWTNKS0tp3749O3bsIDc3ly5dunDTTTdxxx138PXXX7N/f8V7h5AavC/2dWYWh5x2FMwiIlX0wsf/pte4f9L48rtIrN+A8Y9M4Oabb2b79u2MGzeONm3aMHjwYNq0acPKlSu5/vrr+eijj8jJyWHixImHrWv06NHMmDGDPXv2UFxczLBhw7jmmmsqffKUu/Pmm29yxRVXUL9+fW666Sb+9a9/8Z//+Z9s2LCB5s2bVyz+FYF7X/cBVpjZY2Z23in8WaSa6eQvEZHjyX2WAy8/yOADWxg+YQ/1m7Yk5dLbKNrwCU/870zcnREjRtCmTRumTZvGp59+ypVXXsmsWbNYu3Yt06dPP2qVycnJXHvttTzwwAOYGW3atKFVq1aUlpaya9cuGjZsCMDdd9/N3r17uf3228vv9rV+/XqmT5/Ok08+SXx8/FHrd/dlBG6nGQf8HPjQzB5w9/85pb+TVIuQesxmlmJmr5vZ6uB7w2OUKzGznODrxVC2KSJSo3KfhQWjSTi4mQhz/n51PH0abMOX/o2IlNY0GnQPSUlJDBkyhAEDBrBu3Tq6detGq1atePzxx8sfVFGZX/ziF8yaNYvS0lLS0tJo37497l7+nYKCAlJSUhgzZgwPP/ww11xzDQMGDGDIkCEkJyfz2GOP8Z3vfIdLLrnksPWaWZSZDQbmAD8DHgRmnbLfSKpVqD3m+4FF7j7BzO4Pzt9XSbmD7p4R4rZERGreonFQdLB89pL2UVzSPopP9yXyvZwEPnv29yzo1o4XXniBdevW8eGHH5KWlgYEnsFcXFxMRkYGjRs3ZuHChYetOiUlhdLSUgDWrVsHQEREBHfffTeTJk1i8+bNDBo0iLFjx2JmTJs2jcGDBzN69GggcI/tSjQDvgDeIXBLzber8+eQUy/UY8xDgLIbvM4ArghxfSIi4SV/Y6WLu9bbTYPMIXz37mn87ne/IzIykh49ejB9+nQ6derE7t27adOmDe+88w45OTnloRwTE8M999wDwNtvv03Xrl0POxO7qKiIxMREFi5cyODBgxk8eDARERGYGf/1X//F/Pnzj1fjA0CGu49QKJ+eQrpXtpntdvfkCvO73P2o4WwzKwZyCDxqbIK7V/qMTzMbBYwCSE1N7bF+/fqTrpuISLWYdC7kf33U4o2ljbnYH+eRH3Xjiu4ta6FildO9sk9/x+0xm9lCM1teyWvICWwnNfgP5Vrgj2bWvrJC7j7V3TPdPbNJkyYnsHoRkVMk60GIjj9s0QGPYVrM9WEXylI3HPcYs7sPONZnZrbVzJq7+2Yzaw5sO8Y6NgXfvzSzxUB3YO3JVVlEpAalXx14XzQuMKyd1IqErAcZW7ZcpJqFevLXi8AIYELw/aiDH8EztQ+4+yEzawxcAPwhxO2KiNSc9Ku/CWiRUyzUk78mABeb2Wrg4uA8ZpZpZtOCZboA2Wb2CfAmgWPMK0PcroiISJ0UUo/Z3XcCRz2I1N2zgZuC0+8B3ULZjoiIyJlCt+QUEREJIwpmERGRMKJgFhERCSMKZhERkTCiYBYREQkjCmYREZEwEtK9sk8lM9sOVOVm2Y2BHae4OuHgTGjnmdBGUDvrmnBrZxt31z2NT2NhG8xVZWbZZ8IN28+Edp4JbQS1s645U9opNUdD2SIiImFEwSwiIhJG6kIwT63tCtSQM6GdZ0IbQe2sa86UdkoNOe2PMYuIiNQldaHHLCIiUmcomEVERMLIaRfMZjbUzFaYWamZHfMSBTO7zMw+N7M1ZnZ/TdaxOphZipm9bmarg+8Nj1HuD8Hf4zMzm2JmVtN1PVkn0MZUM3st2MaVZpZWszUNTVXbGSzbwMz+bWZ/qsk6VoeqtNPMMsxsSfDfbK6ZDauNup6M4+1TzCzWzJ4Jfv7B6fbvVMLHaRfMwHLgR8DbxypgZpHA48APgHOAa8zsnJqpXrW5H1jk7h2ARcH5w5hZH+ACIB04F/gu8L2arGSIjtvGoJnARHfvAvQEttVQ/apLVdsJ8FvgrRqpVfWrSjsPADe4e1fgMuCPZpZcg3U8KVXcp/wHsMvdzwYmAb+v2VpKXXHaBbO7f+bunx+nWE9gjbt/6e6FwNPAkFNfu2o1BJgRnJ4BXFFJGQfigBggFogGttZI7arHcdsY3PlFufvrAO6+z90P1FwVq0VV/ltiZj2AZsBrNVSv6nbcdrr7F+6+Oji9icAfWafDXaqqsk+p2P55QNbpNIIl4eO0C+Yqagl8XWF+Y3DZ6aSZu28GCL43PbKAuy8B3gQ2B1+vuvtnNVrL0By3jUBHYLeZPWdmH5vZxGDv5XRy3HaaWQTw38C9NVy36lSV/57lzKwngT8q19ZA3UJVlX1KeRl3LwbygUY1UjupU6JquwKVMbOFwFmVfDTG3edXZRWVLAu768K+rZ1V/P7ZQBegVXDR62bWz92POcxf00JtI4F/o32B7sAG4BngRuDJ6qhfdamGdt4KvOTuX4dzJ6sa2lm2nubA/wEj3L20Oup2ilVln3Ja7Hck/IVlMLv7gBBXsRFoXWG+FbApxHVWu29rp5ltNbPm7r45uBOr7LjqlcD77r4v+J2XgV58y/H3mlYNbdwIfOzuXwa/8wKBNoZVMFdDO3sDfc3sViARiDGzfe4eVicuVkM7MbMGwD+BX7v7+6eoqtWtKvuUsjIbzSwKSALyaqZ6UpfU1aHspUAHM2trZjHAT4AXa7lOJ+pFYERwegRQ2UjBBuB7ZhZlZtEETvw6nYayq9LGpUBDMys7DnkRsLIG6ladjttOd7/O3VPdPQ24B5gZbqFcBcdtZ/D/x+cJtG9uDdYtVFXZp1Rs/1XAG647OMnJcPfT6kWgl7gROETgRKdXg8tbEBgKLCt3OfAFgeNXY2q73ifRzkYEzmxdHXxPCS7PBKYFpyOBJwiE8Urgf2q73tXdxuD8xUAu8CkwHYip7bqfinZWKH8j8KfarvepaCdwPVAE5FR4ZdR23avYvqP2KcA4YHBwOg6YC6wBPgTa1Xad9To9X7olp4iISBipq0PZIiIipyUFs4iISBhRMIuIiIQRBbOIiEgYUTCLiIiEEQWziIhIGFEwi4iIhJH/D5n9sO7c5Xs+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = [key for key in cm.vocabulary.keys()]\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    \n",
    "    #if re.match('^co[rv]', word):\n",
    "    \n",
    "        x = df['Comp 1'][i]\n",
    "        y = df['Comp 2'][i]\n",
    "\n",
    "        plt.scatter(x, y)\n",
    "        plt.text(x, y, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set word embeddings for text classification\n",
    "ica = FastICA(n_components = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00040943,  0.00320895, -0.00100425, ..., -0.0011947 ,\n",
       "        -0.00358857,  0.00190187],\n",
       "       [ 0.0017883 , -0.00025361,  0.00123913, ...,  0.00078241,\n",
       "        -0.00010934, -0.00116413],\n",
       "       [ 0.00100165, -0.00050819,  0.00058576, ..., -0.00066567,\n",
       "        -0.00156485, -0.01133304],\n",
       "       ...,\n",
       "       [-0.01030683,  0.00069333,  0.00929786, ..., -0.00231218,\n",
       "        -0.00943478, -0.00639206],\n",
       "       [ 0.01097108, -0.01089988, -0.00360372, ..., -0.00626713,\n",
       "         0.00048057,  0.01020577],\n",
       "       [-0.0030422 ,  0.00015958,  0.00041317, ..., -0.00436634,\n",
       "         0.00046856, -0.00431221]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = ica.fit_transform(X_std)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out target\n",
    "y = tweets['Is_Unreliable']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derive text vectors from word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_vectors(word_embeddings,\n",
    "                     word_index_dict,\n",
    "                     text_list,\n",
    "                     remove_stopwords = True,\n",
    "                     lowercase = True,\n",
    "                     lemmatize = True,\n",
    "                     add_start_end_tokens = True):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    for k in range(len(text_list)):\n",
    "        text = text_list[k]\n",
    "        text = re.sub(r'[_~`@$%^&*[\\]+=\\|}{\\\"\\'<>/]+', '', text)\n",
    "        text_vec = np.zeros(word_embeddings.shape[1])\n",
    "        words = word_tokenize(text)\n",
    "        tracker = 0 # to track whether we've encountered a word for which we have an embedding (in each tweet)\n",
    "        \n",
    "        if remove_stopwords:\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                if word.lower() not in set(stopwords.words('english')):\n",
    "                    clean_words.append(word)\n",
    "            words = clean_words\n",
    "\n",
    "        if lowercase:\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                clean_words.append(word.lower())\n",
    "\n",
    "            words = clean_words\n",
    "\n",
    "        if lemmatize:\n",
    "            clean_words = []\n",
    "            for word in words:\n",
    "                PoS_tag = pos_tag([word])[0][1]\n",
    "\n",
    "                # to change contractions to full word form\n",
    "                if word in contractions:\n",
    "                    word = contractions[word]\n",
    "\n",
    "                if PoS_tag[0].upper() in 'JNVR':\n",
    "                    word = lemmatizer.lemmatize(word, convert_pos_wordnet(PoS_tag))\n",
    "                else:\n",
    "                    word = lemmatizer.lemmatize(word)\n",
    "\n",
    "                clean_words.append(word)\n",
    "\n",
    "            words = clean_words\n",
    "\n",
    "        if add_start_end_tokens:\n",
    "            words = ['<START>'] + words + ['<END>']\n",
    "        \n",
    "        for i in range(len(words)):\n",
    "            word = words[i]\n",
    "            if word in word_index_dict:\n",
    "                word_embed_vec = word_embeddings[word_index_dict[word],:]\n",
    "                if tracker == 0:\n",
    "                    text_matrix = word_embed_vec\n",
    "                else:\n",
    "                    text_matrix = np.vstack((text_matrix, word_embed_vec))\n",
    "                    \n",
    "                # only increment if we have come across a word in the embeddings dictionary\n",
    "                tracker += 1\n",
    "                    \n",
    "        for j in range(len(text_vec)):\n",
    "            text_vec[j] = text_matrix[:,j].mean()\n",
    "            \n",
    "        if k == 0:\n",
    "            full_matrix = text_vec\n",
    "        else:\n",
    "            full_matrix = np.vstack((full_matrix, text_vec))\n",
    "            \n",
    "    return full_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_text_vectors(embeddings, cm.vocabulary, tweets['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(560, 150)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.93372359e-03, -1.80410889e-03, -7.89333694e-03, ...,\n",
       "        -7.78511485e-02, -5.51917769e-03,  5.68501377e-03],\n",
       "       [-1.99961024e-03, -1.00337028e-03, -1.25929061e-02, ...,\n",
       "        -5.02590365e-04, -6.71244939e-04,  6.56141472e-03],\n",
       "       [ 1.02805200e-02, -1.84484563e-03,  1.51978808e-02, ...,\n",
       "         1.00118478e-03, -1.64431536e-03,  2.72763729e-03],\n",
       "       ...,\n",
       "       [-2.03835879e-04,  9.18837482e-02,  1.91431158e-03, ...,\n",
       "        -9.12727768e-02, -1.02781406e-04, -8.67995415e-02],\n",
       "       [-2.90806691e-02, -2.02319324e-03, -9.73410082e-04, ...,\n",
       "         3.57672220e-04, -1.62040881e-02, -2.30458486e-03],\n",
       "       [-6.64082710e-03, -2.41950471e-04,  5.60391395e-04, ...,\n",
       "         1.81069467e-05,  1.57306582e-03, -3.54285092e-02]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification - nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, KFold, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "pipe = Pipeline([\n",
    "    ('classify', SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC hyperparams to optimize\n",
    "kernel = ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "C = [0.001, 0.01, 0.1, 1, 10]\n",
    "# set up parameter grid\n",
    "params = {\n",
    "    'classify__kernel': kernel,\n",
    "    'classify__C': C\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CV scheme for inner and outer loops\n",
    "inner_cv = KFold(n_splits = 3, shuffle = True, random_state = 1)\n",
    "outer_cv = KFold(n_splits = 5, shuffle = True, random_state = 1)\n",
    "\n",
    "# Set up GridSearch for inner loop\n",
    "grid_SVC = GridSearchCV(pipe, params, cv = inner_cv)\n",
    "#grid_SVC.fit(X, y)\n",
    "\n",
    "# Nested CV scores\n",
    "scores = cross_validate(grid_SVC,\n",
    "                        X = X,\n",
    "                        y = y,\n",
    "                        cv = outer_cv,\n",
    "                        scoring = ['roc_auc', 'accuracy', 'f1', 'precision', 'recall'],\n",
    "                        return_estimator = True)\n",
    "auc = scores['test_roc_auc']\n",
    "accuracy = scores['test_accuracy']\n",
    "f1 = scores['test_f1']\n",
    "precision = scores['test_precision']\n",
    "recall = scores['test_recall']\n",
    "estimators = scores['estimator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9179744386774544"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8446428571428571"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8495250980360602"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8207145044281138"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8831321948940147"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classify__C': 10, 'classify__kernel': 'rbf'}\n",
      "\n",
      "\n",
      "{'classify__C': 1, 'classify__kernel': 'rbf'}\n",
      "\n",
      "\n",
      "{'classify__C': 1, 'classify__kernel': 'rbf'}\n",
      "\n",
      "\n",
      "{'classify__C': 1, 'classify__kernel': 'rbf'}\n",
      "\n",
      "\n",
      "{'classify__C': 1, 'classify__kernel': 'rbf'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in estimators:\n",
    "    print(i.best_params_)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
